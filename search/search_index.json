{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is Brainlife? Brainlife promotes engagement and education in reproducible neuroscience. We do this by providing an online platform where users can publish code (Apps), Data, and make it \"alive\" by integragrate various HPC and cloud computing resources to run those Apps. Brainlife also provide mechanisms to publish all research assets associated with a scientific project (data and analyses) embedded in a cloud computing environment and referenced by a single digital-object-identifier (DOI). The platform is unique because of its focus on supporting scientific reproducibility beyond open code and open data, by providing fundamental smart mechanisms for what we refer to as \u201cOpen Services.\u201d Brainlife Apps Brainlife uses Apps to analyze data. Apps are small programs, small modules or compute units, that can process data individually or be made part of a larger series of steps in a full data analysis workflow. Brainlife Apps are meant to do small but meaningful steps in a longer analysis pipeline. Apps are modules and the brainlife.io platform. The platform allows users to develop, use, combine, and reuse Apps to build complex pipelines for customized brain data analyses. Most Apps indeed do only one thing, they process data in a specific way and are meant to perform a small set of operations and handle small sets of data; they do one thing, they do it well. Apps can be developed and published on the Brainlife platform by anyone. App developers can be computational neuroscientists, cognitive neuroscientists, but also computer scientists or engineers. Apps are snippets of code implementing algorithms or analyses. By following a few easy steps developers can publish their code for brain data analysis on the Brainlife platform as an App. Publishing code as Apps allows scientists to use the code in combination with the data and computational resources available through Brainlife. Apps published on Brainlife can be used privately or shared publicly with the platform users community. Brainlife Datatypes Brainlife Apps communicate via Brainlife \u201cDatatypes.\u201d A Datatype defines the expected list of filenames or the directory structure that an Apps can use as input or generate as output. The same Datatype is ideally used by multiple Apps, this allows Apps to communicate by their input and output data sets and reuse the data to generate more data derivatives, useful for other Apps. Datatypes, in addition to allowing the various Apps developed by independent developers to communicate on the brainlife.io platform, also allow Apps to be joined together to form a pipeline or workflows. The Brainlife Datatypes are maintained by individual developers participating in a specific Datatype, and discussed and maintained at https://github.com/brain-life/datatypes, conveniently using the full versioning and management features of github.com. Brainlife Clouds (Compute Resource) The fundamental architecture of Brainlife allows orchestration of data and computing across mix systems of Clouds and high-performance computing clusters (HPC). We refer to both HCP and Clouds as Brainlife Clouds, this is not meant to be a technical definition but a simple way to communicate the fact that compute resources can be registered on the platform. Brainlife orchestration allows platform users and compute resource providers to register a compute resource and make it available publicly to the full Brainlife users community or privately to a subset of users. Brainlife has smart mechanisms that allow Apps to run on different resources, privately or publicly. Whereas with the more traditional approach of running an entire workflow on a single resource or on a small set of resources, does not allow optimization of the workflow (or parts of it) on every resources. Brainlife approach instead allows App Developers to identify the best resources available for the App the develop. This mechanisms is provided by scoring the compute resources available on the Brainlife platform depending on how well they work with the App being develop. Brainlife automatically keeps track of statistics, such as success rate a time to compute so that the users can quickly glance how often and efficiently each App has been processing data and make an informed choice when choosing among similar Apps to process a dataset. Brainlife Viz (Cloud Visualization) Brainlife provides mechanisms to perform data visualization on the Cloud side \u2013 without moving data to the web-browser of the users in location distal from the data. Data visualization is meant to provide users with an agile way to get feedback on the quality of the results generated by Apps and pipelines. Visualization is implemented with smart cloud-side methods, so that data are not moved from the Cloud to the users computer. This increases security and improves data management. Brainlife Viz allows running major software for data visualization familiar to the neuroscience community (e.g., FreeView, FSLview, MRview). We also developed an innovative method to run GPU rendered visualization on the cloud via Docker and VNC. Brainlife Viz as well as the Apps can be openly and conveniently contributed by developers to the Brainlife platform.","title":"Home"},{"location":"#what-is-brainlife","text":"Brainlife promotes engagement and education in reproducible neuroscience. We do this by providing an online platform where users can publish code (Apps), Data, and make it \"alive\" by integragrate various HPC and cloud computing resources to run those Apps. Brainlife also provide mechanisms to publish all research assets associated with a scientific project (data and analyses) embedded in a cloud computing environment and referenced by a single digital-object-identifier (DOI). The platform is unique because of its focus on supporting scientific reproducibility beyond open code and open data, by providing fundamental smart mechanisms for what we refer to as \u201cOpen Services.\u201d","title":"What is Brainlife?"},{"location":"#brainlife-apps","text":"Brainlife uses Apps to analyze data. Apps are small programs, small modules or compute units, that can process data individually or be made part of a larger series of steps in a full data analysis workflow. Brainlife Apps are meant to do small but meaningful steps in a longer analysis pipeline. Apps are modules and the brainlife.io platform. The platform allows users to develop, use, combine, and reuse Apps to build complex pipelines for customized brain data analyses. Most Apps indeed do only one thing, they process data in a specific way and are meant to perform a small set of operations and handle small sets of data; they do one thing, they do it well. Apps can be developed and published on the Brainlife platform by anyone. App developers can be computational neuroscientists, cognitive neuroscientists, but also computer scientists or engineers. Apps are snippets of code implementing algorithms or analyses. By following a few easy steps developers can publish their code for brain data analysis on the Brainlife platform as an App. Publishing code as Apps allows scientists to use the code in combination with the data and computational resources available through Brainlife. Apps published on Brainlife can be used privately or shared publicly with the platform users community.","title":"Brainlife Apps"},{"location":"#brainlife-datatypes","text":"Brainlife Apps communicate via Brainlife \u201cDatatypes.\u201d A Datatype defines the expected list of filenames or the directory structure that an Apps can use as input or generate as output. The same Datatype is ideally used by multiple Apps, this allows Apps to communicate by their input and output data sets and reuse the data to generate more data derivatives, useful for other Apps. Datatypes, in addition to allowing the various Apps developed by independent developers to communicate on the brainlife.io platform, also allow Apps to be joined together to form a pipeline or workflows. The Brainlife Datatypes are maintained by individual developers participating in a specific Datatype, and discussed and maintained at https://github.com/brain-life/datatypes, conveniently using the full versioning and management features of github.com.","title":"Brainlife Datatypes"},{"location":"#brainlife-clouds-compute-resource","text":"The fundamental architecture of Brainlife allows orchestration of data and computing across mix systems of Clouds and high-performance computing clusters (HPC). We refer to both HCP and Clouds as Brainlife Clouds, this is not meant to be a technical definition but a simple way to communicate the fact that compute resources can be registered on the platform. Brainlife orchestration allows platform users and compute resource providers to register a compute resource and make it available publicly to the full Brainlife users community or privately to a subset of users. Brainlife has smart mechanisms that allow Apps to run on different resources, privately or publicly. Whereas with the more traditional approach of running an entire workflow on a single resource or on a small set of resources, does not allow optimization of the workflow (or parts of it) on every resources. Brainlife approach instead allows App Developers to identify the best resources available for the App the develop. This mechanisms is provided by scoring the compute resources available on the Brainlife platform depending on how well they work with the App being develop. Brainlife automatically keeps track of statistics, such as success rate a time to compute so that the users can quickly glance how often and efficiently each App has been processing data and make an informed choice when choosing among similar Apps to process a dataset.","title":"Brainlife Clouds (Compute Resource)"},{"location":"#brainlife-viz-cloud-visualization","text":"Brainlife provides mechanisms to perform data visualization on the Cloud side \u2013 without moving data to the web-browser of the users in location distal from the data. Data visualization is meant to provide users with an agile way to get feedback on the quality of the results generated by Apps and pipelines. Visualization is implemented with smart cloud-side methods, so that data are not moved from the Cloud to the users computer. This increases security and improves data management. Brainlife Viz allows running major software for data visualization familiar to the neuroscience community (e.g., FreeView, FSLview, MRview). We also developed an innovative method to run GPU rendered visualization on the cloud via Docker and VNC. Brainlife Viz as well as the Apps can be openly and conveniently contributed by developers to the Brainlife platform.","title":"Brainlife Viz (Cloud Visualization)"},{"location":"contact/","text":"Contact Email Branlife brlife@iu.edu Franco Pestilli frakkopesto@gmail.com Newsletter Subscription Brainlife Users Newsletter Brainlife App Developers Newsletter Slack Join brainlife.slack.com Bug Report / Feature Request Brainlife UI Issues Brainlife Authentication Issues Note For Brainlife App specific issue, please contact individual developers for each App via the github issues.","title":"Contact"},{"location":"contact/#contact","text":"","title":"Contact"},{"location":"contact/#email","text":"Branlife brlife@iu.edu Franco Pestilli frakkopesto@gmail.com","title":"Email"},{"location":"contact/#newsletter-subscription","text":"Brainlife Users Newsletter Brainlife App Developers Newsletter","title":"Newsletter Subscription"},{"location":"contact/#slack","text":"Join brainlife.slack.com","title":"Slack"},{"location":"contact/#bug-report-feature-request","text":"Brainlife UI Issues Brainlife Authentication Issues Note For Brainlife App specific issue, please contact individual developers for each App via the github issues.","title":"Bug Report / Feature Request"},{"location":"privacy/","text":"Effective: 2018-05-01 Applicability This privacy notice applies only to the brainlife.io (https://brainlife.io) and explains our practices concerning the collection, use, and disclosure of visitor information. Visitor information collected by brainlife.io will be used only as outlined in this privacy notice. Other units at the university may collect and use visitor information in different ways. Therefore, visitors to other university websites should review the privacy notices for the particular sites they visit. brainlife.io is not responsible for the content of other websites or for the privacy practices of websites outside the scope of this notice. Changes Because Internet technologies continue to evolve rapidly, brainlife.io may make appropriate changes to this notice in the future. Any such changes will be consistent with our commitment to respecting visitor privacy, and will be clearly posted in a revised privacy notice. Collection and Use Passive/Automatic Collection When you view pages on our site, the web server automatically collects certain technical information from your computer and about your connection including: your IP address the domain name from which you visit our site user-specific information on which pages are visited aggregate information on pages visited the referring website the date and time of visit the duration of visit your browser type your screen resolution Asset accessed on the platform Continued use of our website indicates consent to the collection, use, and disclosure of this information as described in this notice. This technical information is retained in detail for up to 180 days days. Active/Manual/Voluntary Collection Other than automatically collected technical information about your visit (described above, or cookies, described below), we may ask you to provide information voluntarily, such as through forms or other manual input\u2014in order to make products and services available to you, to maintain and manage our relationship with you, including providing associated services or to better understand and serve your needs. This information is generally retained as long as you continue to maintain a relationship with us. Your providing this information is wholly voluntary. However, not providing the requested information (or subsequently asking that the data be removed) may affect our ability to deliver the products or service for which the information is needed. Providing the requested information indicates your consent to the collection, use, and disclosure of this information as described in this notice. Information we may actively collect could include: the email addresses of those who communicate with us via email the email addresses of those who make postings to our chat areas name Information Usage This information is: used to customize the content of our site used to notify visitors about updates to our site Additional Information Related to Information Collection n/a Information Used For Contact If you supply us with your postal/mailing address: You will only receive the information for which you provided us your address. Information Sharing We do not share any aggregate, non-personally identifiable information with other entities or organizations. We do not share any personally identifiable information with other entities or organizations, except when legally required to do so, at the request of governmental authorities conducting an investigation, to verify or enforce compliance with the policies governing our website and applicable laws, or to protect against misuse or unauthorized use of our website. Except as described above, we will not share any information with any party for any reason. Except as provided in the Disclosure of Information section below, we do not attempt to use the technical information discussed in this section to identify individual visitors. Additional Information Related to Information Use n/a Cookies A cookie is a small data file that is written to your hard drive that contains information about your visit to a web page. If you prefer not to receive cookies, you may configure your browser not to accept them at all, or to notify and require approval before accepting new cookies. Some web pages/sites may not function properly if the cookies are turned off, or you may have to provide the same information each time you visit those pages. Our site does not use cookies to store information about your actions or choices on pages associated with our site. Children This site is not directed to children under 13 years of age, does not sell products or services intended for purchase by children, and does not knowingly collect or store any personal information, even in aggregate, about children under the age of 13. We encourage parents and teachers to be involved in children\u2019s Internet explorations. It is particularly important for parents to guide their children when they are asked to provide personal information online. Use of Third Party Services This website uses Google Analytics, a web analytics service provided by Google, Inc. (\"Google\"). Google Analytics uses cookies (described above) to help the website analyze how users use the site. The information generated by the cookie about your use of the website (including possibly your IP address) will be transmitted to and stored by Google. For more information, please visit Google\u2019s Privacy Policy . Additional Information Related to the Use of Third Party Services https://disqus.comrn Updating Inaccurate Information In some cases, we will grant visitors the ability to update or correct inaccuracies in the information that we maintain. Visitors may correct inaccuracies in: contact information that we have on file Visitors can have this information corrected by: sending us email at the listed address visiting us at the following URL: brainlife.io brlife@iu.edu Disclosure of Information Other than sharing your information with other appropriate university personnel and units to ensure the quality, functionality, and security of our website, or manage your relationship with us, we will not disclose personally identifiable information about your use of the site except under the following circumstances: With your prior written (including email) consent When we have given you clear notice that we will disclose information that you voluntarily provide With appropriate external parties, such as law enforcement agencies, in order to investigate and respond to suspected violations of law or university policy. Any such disclosures shall comply with all applicable laws and university policies. Security Due to the rapidly evolving nature of information technologies, no transmission of information over the Internet can be guaranteed to be completely secure. While Indiana University is committed to protecting user privacy, IU cannot guarantee the security of any information users transmit to university sites, and users do so at their own risk. We have appropriate security measures in place in our physical facilities to protect against the loss, misuse, or alteration of information that we have collected from you at our site. Once we receive user information, we will use reasonable safeguards consistent with prevailing industry standards and commensurate with the sensitivity of the data being stored to maintain the security of that information on our systems. We will comply with all applicable federal, state and local laws regarding the privacy and security of user information. Links to non-university sites Indiana University is not responsible for the availability, content, or privacy practices of non-university sites. Non-university sites are not bound by this site privacy notice policy and may or may not have their own privacy policies. Privacy Notice Changes From time to time, we may use visitor information for new, unanticipated uses not previously disclosed in our privacy notice. We will post the policy changes to our website to notify you of these changes and provide you with the ability to opt out of these new uses. If you are concerned about how your information is used, you should check back at our website periodically. Visitors may prevent their information from being used for purposes other than those for which it was originally collected by: sending us an email at the listed address brlife@iu.edu Supplemental Information n/a Contact Information If you have questions or concerns about this policy, please contact us. brainlife.io ATTN: Franco Pestilli 1101 East 10th Street, Department of Psychological and Brain Department of Psychological and Brain Sciences Bloomington, Indiana 47405 brlife@iu.edu 8128569967 If you feel as though this site\u2019s privacy practices differ from the information stated, you may contact us at the listed address or phone number. If you feel that this site is not following its stated policy and communicating with the owner of this site does not resolve the matter, or if you have general questions or concerns about privacy or information technology policy at Indiana University, please contact the chief privacy officer through the University Information Policy Office, 812-855-UIPO, privacy@iu.edu .","title":"Privacy Policy"},{"location":"releases/","text":"Upcoming First Production Release September 2018? TBD.. Beta 2 August 2018? TBD... Beta Release April 8th, 2018 (warehouse v1.1.2) Our first official release ! We now have most of our features functional and ready to be used by users outside our labs. We've developed an initial set of documentations and defined or identified standard operating protocols needed for sustainable operation of this platform. Past Releases","title":"Releases"},{"location":"releases/#upcoming","text":"","title":"Upcoming"},{"location":"releases/#first-production-release","text":"","title":"First Production Release"},{"location":"releases/#september-2018","text":"TBD..","title":"September 2018?"},{"location":"releases/#beta-2","text":"","title":"Beta 2"},{"location":"releases/#august-2018","text":"TBD...","title":"August 2018?"},{"location":"releases/#beta-release","text":"","title":"Beta Release"},{"location":"releases/#april-8th-2018","text":"(warehouse v1.1.2) Our first official release ! We now have most of our features functional and ready to be used by users outside our labs. We've developed an initial set of documentations and defined or identified standard operating protocols needed for sustainable operation of this platform.","title":"April 8th, 2018"},{"location":"releases/#past-releases","text":"","title":"Past Releases"},{"location":"terms/","text":"Terms of use As of April 9th, 2018 Danger This document is currently under development. Brainlife License Agreement WE RESERVE THE RIGHT TO CHANGE THIS AGREEMENT AT ANY TIME, BUT IF WE DO, WE WILL BRING IT TO YOUR ATTENTION BY PLACING A NOTICE ON BRAINLIFE.IO WEBSITE, BY SENDING YOU AN EMAIL, AND/OR BY SOME OTHER MEANS. License Grants Subject to full compliance with the terms of this Agreement and the Guidelines, Brainlife.io hereby grants you a limited, personal, non-sublicensable, non-transferable, nonexclusive license to use (a) our application programming interface, (b) Brainlife web user interface, and (c) related information and documentation that, in each case, you access (this will not be a download) from https://brainlife.io for the sole purpose of allowing you to host your datasets, perform data analysis and publish processing results through our platform. Further, subject to full compliance with the terms of this Agreement, we hereby grant you a limited, personal, non-sublicensable, non-transferable, nonexclusive right to access the features, services, and apps located at brainlife.io. The Service shall include, but not be limited to, any services Brainlife performs for you. Brainlife may also impose limits on certain features or restrict your access to parts or all of the Service. No rights or licenses are granted except as expressly and unambiguously set forth herein. TODO.. TODO: Describe the type of datasets that users are allowed to upload. TODO: Describe User's and Brainlife's responsibility for detecting, and removal of inappropriate upload. TODO: Describe who will be held responsible in case of uploading datasets and the publication of inappropriate datasets TODO: Describe the right of App developers. TODO: Describe who will be held responsible in case of App / platform creating invalid data derivatives. TODO: Describe who will be held responsible in case of malicious Apps registered and ran on compute resources. Restrictions TODO.. Ownership TODO: Describe who owns the datasets uploaded to Brainlife. TODO: Describe who owns the IP for registered Apps and how Brainlife may use it. Service Availability TODO: Describe expected service level for Brainlife.io platform TODO: Describe expected service level for Brainlife slack channel, mailing list, and other communication mechanisms. TODO: Describe expected service level for resource owned by 3rd party users. TODO: Describe expected service level for Apps written by users Feedbacks If, in the course of performing under this Agreement, Licensee provides Brainlife.io with any written comments, suggestions, or feedback regarding the Service (\u201cFeedback\u201d), you hereby grant Brainlife.io a non-exclusive, worldwide, royalty-free license to use and disclose the Feedback in any manner Brainlife.io chooses and, directly or indirectly through third parties, to display, perform, copy, have copied, make, have made, use, sell, offer to sell, and otherwise dispose of Brainlife.io's products (including any improvements or modifications thereof) embodying the Feedback in any manner and via any media the Brainlife.io chooses, but without reference to you as the source of the Feedback. LIMITATION OF LIABILITY UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, INCLUDING, BUT NOT LIMITED TO, TORT, CONTRACT, NEGLIGENCE, STRICT LIABILITY, OR OTHERWISE, SHALL BRAINLIFE.IO OR ITS LICENSORS, SUPPLIERS OR RESELLERS BE LIABLE TO YOU OR ANY OTHER PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOST PROFITS, LOSS OF GOODWILL, OR DAMAGES RESULTING FROM LICENSEE\u2019S USE OF ANY SUBJECT MATTER COVERED BY THIS AGREEMENT. BRAINLIFE.IO'S LIABILITY FOR DAMAGES OF ANY KIND WHATSOEVER ARISING OUT OF THIS AGREEMENT SHALL BE LIMITED IN THE AGGREGATE TO AMOUNTS PAID (PLUS AMOUNTS PAYABLE, IN THE EVENT OF YOUR BREACH ONLY) TO BRAINLIFE.IO DURING THE TWELVE (12) MONTH PERIOD ENDING ON THE DATE THAT A CLAIM OR DEMAND IS FIRST ASSERTED. THE FOREGOING WILL NOT APPLY TO DAMAGES FOR BODILY INJURY THAT, UNDER APPLICABLE LAW, CANNOT BE SO LIMITED. THE FOREGOING LIMITATIONS SHALL APPLY EVEN IF YOU HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. SOME STATES DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THE ABOVE LIMITATION AND EXCLUSION MAY NOT APPLY TO YOU.","title":"Terms of use"},{"location":"terms/#terms-of-use","text":"As of April 9th, 2018 Danger This document is currently under development.","title":"Terms of use"},{"location":"terms/#brainlife-license-agreement","text":"WE RESERVE THE RIGHT TO CHANGE THIS AGREEMENT AT ANY TIME, BUT IF WE DO, WE WILL BRING IT TO YOUR ATTENTION BY PLACING A NOTICE ON BRAINLIFE.IO WEBSITE, BY SENDING YOU AN EMAIL, AND/OR BY SOME OTHER MEANS.","title":"Brainlife License Agreement"},{"location":"terms/#license-grants","text":"Subject to full compliance with the terms of this Agreement and the Guidelines, Brainlife.io hereby grants you a limited, personal, non-sublicensable, non-transferable, nonexclusive license to use (a) our application programming interface, (b) Brainlife web user interface, and (c) related information and documentation that, in each case, you access (this will not be a download) from https://brainlife.io for the sole purpose of allowing you to host your datasets, perform data analysis and publish processing results through our platform. Further, subject to full compliance with the terms of this Agreement, we hereby grant you a limited, personal, non-sublicensable, non-transferable, nonexclusive right to access the features, services, and apps located at brainlife.io. The Service shall include, but not be limited to, any services Brainlife performs for you. Brainlife may also impose limits on certain features or restrict your access to parts or all of the Service. No rights or licenses are granted except as expressly and unambiguously set forth herein.","title":"License Grants"},{"location":"terms/#todo","text":"TODO: Describe the type of datasets that users are allowed to upload. TODO: Describe User's and Brainlife's responsibility for detecting, and removal of inappropriate upload. TODO: Describe who will be held responsible in case of uploading datasets and the publication of inappropriate datasets TODO: Describe the right of App developers. TODO: Describe who will be held responsible in case of App / platform creating invalid data derivatives. TODO: Describe who will be held responsible in case of malicious Apps registered and ran on compute resources.","title":"TODO.."},{"location":"terms/#restrictions","text":"TODO..","title":"Restrictions"},{"location":"terms/#ownership","text":"TODO: Describe who owns the datasets uploaded to Brainlife. TODO: Describe who owns the IP for registered Apps and how Brainlife may use it.","title":"Ownership"},{"location":"terms/#service-availability","text":"TODO: Describe expected service level for Brainlife.io platform TODO: Describe expected service level for Brainlife slack channel, mailing list, and other communication mechanisms. TODO: Describe expected service level for resource owned by 3rd party users. TODO: Describe expected service level for Apps written by users","title":"Service Availability"},{"location":"terms/#feedbacks","text":"If, in the course of performing under this Agreement, Licensee provides Brainlife.io with any written comments, suggestions, or feedback regarding the Service (\u201cFeedback\u201d), you hereby grant Brainlife.io a non-exclusive, worldwide, royalty-free license to use and disclose the Feedback in any manner Brainlife.io chooses and, directly or indirectly through third parties, to display, perform, copy, have copied, make, have made, use, sell, offer to sell, and otherwise dispose of Brainlife.io's products (including any improvements or modifications thereof) embodying the Feedback in any manner and via any media the Brainlife.io chooses, but without reference to you as the source of the Feedback.","title":"Feedbacks"},{"location":"terms/#limitation-of-liability","text":"UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, INCLUDING, BUT NOT LIMITED TO, TORT, CONTRACT, NEGLIGENCE, STRICT LIABILITY, OR OTHERWISE, SHALL BRAINLIFE.IO OR ITS LICENSORS, SUPPLIERS OR RESELLERS BE LIABLE TO YOU OR ANY OTHER PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOST PROFITS, LOSS OF GOODWILL, OR DAMAGES RESULTING FROM LICENSEE\u2019S USE OF ANY SUBJECT MATTER COVERED BY THIS AGREEMENT. BRAINLIFE.IO'S LIABILITY FOR DAMAGES OF ANY KIND WHATSOEVER ARISING OUT OF THIS AGREEMENT SHALL BE LIMITED IN THE AGGREGATE TO AMOUNTS PAID (PLUS AMOUNTS PAYABLE, IN THE EVENT OF YOUR BREACH ONLY) TO BRAINLIFE.IO DURING THE TWELVE (12) MONTH PERIOD ENDING ON THE DATE THAT A CLAIM OR DEMAND IS FIRST ASSERTED. THE FOREGOING WILL NOT APPLY TO DAMAGES FOR BODILY INJURY THAT, UNDER APPLICABLE LAW, CANNOT BE SO LIMITED. THE FOREGOING LIMITATIONS SHALL APPLY EVEN IF YOU HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. SOME STATES DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THE ABOVE LIMITATION AND EXCLUSION MAY NOT APPLY TO YOU.","title":"LIMITATION OF LIABILITY"},{"location":"todo/","text":"Coming soon..","title":"Todo"},{"location":"apps/container/","text":"Containerizing App Docker is a software containerization tool that allow you to package your App and its dependencies into a portable container that can you can run on any machine that supports Docker engine, or singularity. Although you can create a fully functional standalone Docker container for your app, for Brainlife, we recommend to containerize only the App's environment and the dependencies , and not include the main part of your App (your python or Matlab scripts that drives your algorithm) on your container. You can use singularity to run the most ideosyncratic parts of your App which can be stored and maintained inside your github repo and injected into an environment container at runtime. Like.. singularity exec -e docker://brainlife/dipy:0.13 ./app.py In above example, I am using brainlife/dipy:0.13 as an environment container, and running app.py which is stored locally (comes with the github repo for my App) and injected into my environment at runtime. You can even share the same Docker container across multiple Apps that you and your lab members maintain, but you should make sure to specify the container version (\"0.13\" in above example) so that your App won't be affected when the container gets updated. Docker Engine To build a docker container, you need to install Docker engine on your laptop or find a server that has docker engine installed that you can use. (Contact Soichi if you need a help.) We assume you already have your Brainlife app hosted on Github, and you are making changes inside a cloned git repo on a machine with Docker engine. Compiling Matlab Scripts Skip this section if you are not using Matlab Matlab code requires a matlab license to run. If you are App contains any Matlab code, it needs to be compiled to a binary format using mcc command which allows you to execute your code without Matlab license. You will still need to run it against a special Matlab compiled runtime called MCR which can be distributed freely and does not require any license. You can create a script that compiles your matlab code. compile.sh #!/bin/bash module load matlab/2017a mkdir -p compiled cat build.m END addpath(genpath( /N/u/brlife/git/vistasoft )) addpath(genpath( /N/u/brlife/git/jsonlab )) addpath(genpath( /N/soft/mason/SPM/spm8 )) mcc -m -R -nodisplay -a /N/u/brlife/git/vistasoft/mrDiffusion/templates -d compiled myapp exit END matlab -nodisplay -nosplash -r build This script generates a Matlab script called build.m and immediately executes it. build.m will load Matlab paths and run Matlab command called mcc which actually compiles of your Matlab code into an executable binary that can run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR which can be download from Matlab website. Note Brainlife team has built a Docker MCR container brainlife/mcr which can be used to execute your compiled Matlab code with singularity. For the sample build script above, you will need to adjust addpath() to include all of your Matlab dependencies that your Matlab script requires. myapp is the name of the matlab entry function that you use to execute your application. mcc command will create a binary with the same name myapp inside the ./compiled directory. A few other mcc options to note. -m ... tells the name of the main entry function (in this case it's main ) of your application (it reads your config.json and runs the whole application) -R -nodisplay sets the command line options you'd normally pass when you run MatLab on the command line. -d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized. If your app or your Matlab libraries access any non-Matlab files (mexfiles, datafiles, models, templates, etc..) you will need to include them by specifying paths with -a . mcc will include specified files/directories as part of your compiled binary and make it available to the compiled code as if it is available through local filesystem (similar to how Docker containerizes things). If you are using OpenMP, you will need to include libgomp1 library installed in your MCR container. mcc compiled application can't run certain Matlab statements; like addpath. You will need to wrap some code with if ~isdeployed type statement to prevent it from getting executed. Note If you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround is to temporarly rename your startup.m to something else while you are compiling your code.s Loading custom Matlab data structure If your Matlab code is loading (by doing something like load('data.mat') ) any custom data structures (like sparse tensor array in encode's fe structure), addpath() is not enough to include them into the compiled binary. You will need to explicitly tell Matlab compiler that you are using those data structures by including a special comment like the following. %# function sptensor This tells the Matlab compiler to include the sptensor class to the compiled binary. Creating Docker Container There are quite a few materials detailing how to write Dockerfile / containers online. If you are looking for a place to start, we recommend Docker's Getting Started Guide . As most Brainlife Apps execute Docker containers through singularity, there are a few Brainlife specific items that you might need to be aware so that your App will run properly on Brainlife. ldconfig As singularity runs containers as a normal user, we must perform some OS level initialization steps outside the singularity. When Docker installs OS packages, it won't initialize dynamicly linked library links until they are first invoked. As singularity runs as normal user with set uid disabled, it won't be able to setup these paths once container is executed through singularity. To fix this, we can run ldconfig command. This command creates the necessary links and cache to the most recent shared libraries installed in the lib directories. In your Dockerfile, please be sure to run ldconfig as the last command. Like... #make it work under singularity RUN ldconfig IU HPC paths Some OS (like RHEL6) don't allow singularity to mount a host path unless there is a corresponding directory already present inside the container (due to lack of overlayfs support by the kernel). To make your App run on RHEL6 hosts (like IU Karst), please add following somewhere inside your Dockerfile RUN mkdir -p /N/u /N/home /N/dc2 /N/soft Use Bash If you are using ubuntu, by default it changes the default /bin/sh to point to a thing called \"dash\" rather than \"bash\". \"dash\" is simpler/faster to load than \"bash\", but unfortunately it breaks a lot of programs that expects /bin/sh to point to bash. To correct this, you can add following in your Dockerfile to reset it to bash. #https://wiki.ubuntu.com/DashAsBinSh RUN rm /bin/sh ln -s /bin/bash /bin/sh Now, your container should be ready to GO! Example Dockerfile for environment containers brain-life/app-dipy-workflows brain-life/docker-mcr Examples Apps using environment containers github brain-life/app-freesurfer github brain-life/app-wmaSeg","title":"Containerizing App"},{"location":"apps/container/#containerizing-app","text":"Docker is a software containerization tool that allow you to package your App and its dependencies into a portable container that can you can run on any machine that supports Docker engine, or singularity. Although you can create a fully functional standalone Docker container for your app, for Brainlife, we recommend to containerize only the App's environment and the dependencies , and not include the main part of your App (your python or Matlab scripts that drives your algorithm) on your container. You can use singularity to run the most ideosyncratic parts of your App which can be stored and maintained inside your github repo and injected into an environment container at runtime. Like.. singularity exec -e docker://brainlife/dipy:0.13 ./app.py In above example, I am using brainlife/dipy:0.13 as an environment container, and running app.py which is stored locally (comes with the github repo for my App) and injected into my environment at runtime. You can even share the same Docker container across multiple Apps that you and your lab members maintain, but you should make sure to specify the container version (\"0.13\" in above example) so that your App won't be affected when the container gets updated.","title":"Containerizing App"},{"location":"apps/container/#docker-engine","text":"To build a docker container, you need to install Docker engine on your laptop or find a server that has docker engine installed that you can use. (Contact Soichi if you need a help.) We assume you already have your Brainlife app hosted on Github, and you are making changes inside a cloned git repo on a machine with Docker engine.","title":"Docker Engine"},{"location":"apps/container/#compiling-matlab-scripts","text":"Skip this section if you are not using Matlab Matlab code requires a matlab license to run. If you are App contains any Matlab code, it needs to be compiled to a binary format using mcc command which allows you to execute your code without Matlab license. You will still need to run it against a special Matlab compiled runtime called MCR which can be distributed freely and does not require any license. You can create a script that compiles your matlab code.","title":"Compiling Matlab Scripts"},{"location":"apps/container/#compilesh","text":"#!/bin/bash module load matlab/2017a mkdir -p compiled cat build.m END addpath(genpath( /N/u/brlife/git/vistasoft )) addpath(genpath( /N/u/brlife/git/jsonlab )) addpath(genpath( /N/soft/mason/SPM/spm8 )) mcc -m -R -nodisplay -a /N/u/brlife/git/vistasoft/mrDiffusion/templates -d compiled myapp exit END matlab -nodisplay -nosplash -r build This script generates a Matlab script called build.m and immediately executes it. build.m will load Matlab paths and run Matlab command called mcc which actually compiles of your Matlab code into an executable binary that can run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR which can be download from Matlab website. Note Brainlife team has built a Docker MCR container brainlife/mcr which can be used to execute your compiled Matlab code with singularity. For the sample build script above, you will need to adjust addpath() to include all of your Matlab dependencies that your Matlab script requires. myapp is the name of the matlab entry function that you use to execute your application. mcc command will create a binary with the same name myapp inside the ./compiled directory. A few other mcc options to note. -m ... tells the name of the main entry function (in this case it's main ) of your application (it reads your config.json and runs the whole application) -R -nodisplay sets the command line options you'd normally pass when you run MatLab on the command line. -d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized. If your app or your Matlab libraries access any non-Matlab files (mexfiles, datafiles, models, templates, etc..) you will need to include them by specifying paths with -a . mcc will include specified files/directories as part of your compiled binary and make it available to the compiled code as if it is available through local filesystem (similar to how Docker containerizes things). If you are using OpenMP, you will need to include libgomp1 library installed in your MCR container. mcc compiled application can't run certain Matlab statements; like addpath. You will need to wrap some code with if ~isdeployed type statement to prevent it from getting executed. Note If you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround is to temporarly rename your startup.m to something else while you are compiling your code.s","title":"compile.sh"},{"location":"apps/container/#loading-custom-matlab-data-structure","text":"If your Matlab code is loading (by doing something like load('data.mat') ) any custom data structures (like sparse tensor array in encode's fe structure), addpath() is not enough to include them into the compiled binary. You will need to explicitly tell Matlab compiler that you are using those data structures by including a special comment like the following. %# function sptensor This tells the Matlab compiler to include the sptensor class to the compiled binary.","title":"Loading custom Matlab data structure"},{"location":"apps/container/#creating-docker-container","text":"There are quite a few materials detailing how to write Dockerfile / containers online. If you are looking for a place to start, we recommend Docker's Getting Started Guide . As most Brainlife Apps execute Docker containers through singularity, there are a few Brainlife specific items that you might need to be aware so that your App will run properly on Brainlife.","title":"Creating Docker Container"},{"location":"apps/container/#ldconfig","text":"As singularity runs containers as a normal user, we must perform some OS level initialization steps outside the singularity. When Docker installs OS packages, it won't initialize dynamicly linked library links until they are first invoked. As singularity runs as normal user with set uid disabled, it won't be able to setup these paths once container is executed through singularity. To fix this, we can run ldconfig command. This command creates the necessary links and cache to the most recent shared libraries installed in the lib directories. In your Dockerfile, please be sure to run ldconfig as the last command. Like... #make it work under singularity RUN ldconfig","title":"ldconfig"},{"location":"apps/container/#iu-hpc-paths","text":"Some OS (like RHEL6) don't allow singularity to mount a host path unless there is a corresponding directory already present inside the container (due to lack of overlayfs support by the kernel). To make your App run on RHEL6 hosts (like IU Karst), please add following somewhere inside your Dockerfile RUN mkdir -p /N/u /N/home /N/dc2 /N/soft","title":"IU HPC paths"},{"location":"apps/container/#use-bash","text":"If you are using ubuntu, by default it changes the default /bin/sh to point to a thing called \"dash\" rather than \"bash\". \"dash\" is simpler/faster to load than \"bash\", but unfortunately it breaks a lot of programs that expects /bin/sh to point to bash. To correct this, you can add following in your Dockerfile to reset it to bash. #https://wiki.ubuntu.com/DashAsBinSh RUN rm /bin/sh ln -s /bin/bash /bin/sh Now, your container should be ready to GO!","title":"Use Bash"},{"location":"apps/container/#example-dockerfile-for-environment-containers","text":"brain-life/app-dipy-workflows brain-life/docker-mcr","title":"Example Dockerfile for environment containers"},{"location":"apps/container/#examples-apps-using-environment-containers","text":"github brain-life/app-freesurfer github brain-life/app-wmaSeg","title":"Examples Apps using environment containers"},{"location":"apps/customhooks/","text":"Life cycles hooks are the script used to start / stop / monitor your apps by Brain-Life. By default, it looks for executable installed on each resource in the PATH with named start , status , and stop . Resource owner needs to make sure these scripts are installed and accessible by your apps. For most PBS, SLURM, and vanila VM, resource owner can install ABCD default hooks . By default, start hook should look for a file named main to start your app. Therefore, the only file required to make your app runnable by Brain-Life is this main executable on the root directory of the app's git repository. Under most circumstances, app developers shouldn't have to worry about these hook scripts. However, if your app requires some special mechanism to start / stop and monitor your app, you might need to provide your own hook scripts. You can specify the paths to these hook scripts by creating a file named package.json { brainlife : { start : ./start.sh , stop : ./stop.sh , status : ./status.sh } } Then, you will need to provide those hook scripts as part of your app. Please be sure to chmod +x *.sh so that your hook scripts are executable. start.sh Following is an example for start script. It submits a file named main (should be provided by each app) through qsub. It stores jobid so that we can monitor the job status. 1 2 3 4 5 6 #!/bin/bash #return code 0 = job started successfully. #return code non-0 = job failed to start qsub -d $PWD -V -o \\$ PBS_JOBID.log -e \\$ PBS_JOBID.err main jobid stop.sh Following is an example for stop script. This scripts reads the jobid created by start script and call qdel to stop it. 1 2 #!/bin/bash qdel ` cat jobid ` status.sh status hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the jobid stored by start script to query the job status with qstat PBS command. Anything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/bin/bash #return code 0 = running #return code 1 = finished successfully #return code 2 = failed #return code 3 = unknown (retry later) if [ ! -f jobid ] ; then echo no jobid - not yet submitted? exit 1 fi jobid = ` cat jobid ` if [ -z $jobid ] ; then echo jobid is empty.. failed to submit? exit 3 fi jobstate = ` qstat -f $jobid | grep job_state | cut -b17 ` if [ -z $jobstate ] ; then echo Job removed before completing - maybe timed out? exit 2 fi case $jobstate in Q ) showstart $jobid | grep start exit 0 ;; R ) #get last line of last log touched logfile = $( ls -rt *.log | tail -1 ) tail -1 $logfile exit 0 ;; H ) echo Job held.. waiting exit 0 ;; C ) exit_status = ` qstat -f $jobid | grep exit_status | cut -d = -f2 | xargs ` if [ $exit_status -eq 0 ] ; then echo finished with code 0 exit 1 else echo finished with code $exit_status exit 2 fi ;; * ) echo unknown job status $jobstate .. will check later exit 3 ;; esac","title":"Custom Hooks"},{"location":"apps/customhooks/#startsh","text":"Following is an example for start script. It submits a file named main (should be provided by each app) through qsub. It stores jobid so that we can monitor the job status. 1 2 3 4 5 6 #!/bin/bash #return code 0 = job started successfully. #return code non-0 = job failed to start qsub -d $PWD -V -o \\$ PBS_JOBID.log -e \\$ PBS_JOBID.err main jobid","title":"start.sh"},{"location":"apps/customhooks/#stopsh","text":"Following is an example for stop script. This scripts reads the jobid created by start script and call qdel to stop it. 1 2 #!/bin/bash qdel ` cat jobid `","title":"stop.sh"},{"location":"apps/customhooks/#statussh","text":"status hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the jobid stored by start script to query the job status with qstat PBS command. Anything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #!/bin/bash #return code 0 = running #return code 1 = finished successfully #return code 2 = failed #return code 3 = unknown (retry later) if [ ! -f jobid ] ; then echo no jobid - not yet submitted? exit 1 fi jobid = ` cat jobid ` if [ -z $jobid ] ; then echo jobid is empty.. failed to submit? exit 3 fi jobstate = ` qstat -f $jobid | grep job_state | cut -b17 ` if [ -z $jobstate ] ; then echo Job removed before completing - maybe timed out? exit 2 fi case $jobstate in Q ) showstart $jobid | grep start exit 0 ;; R ) #get last line of last log touched logfile = $( ls -rt *.log | tail -1 ) tail -1 $logfile exit 0 ;; H ) echo Job held.. waiting exit 0 ;; C ) exit_status = ` qstat -f $jobid | grep exit_status | cut -d = -f2 | xargs ` if [ $exit_status -eq 0 ] ; then echo finished with code 0 exit 1 else echo finished with code $exit_status exit 2 fi ;; * ) echo unknown job status $jobstate .. will check later exit 3 ;; esac","title":"status.sh"},{"location":"apps/helloworld/","text":"Prerequisites Please read App Developers / Introduction first. Before you begin, please install a text editor such as VSCode and git client on your laptop. You will also need to install jq which will be explained later. HelloWorld Here, we will create a \"HelloWorld\" Brainlife App. We will show how to create a brand new github repository containing a Brainlife App. Please be sure to make the repo public so that the brainlife.io platform will be able to access it. You can name the repository as you prefer, the Brainlife Team has beeen naming apps starting with the prefix app- , for example take a look at these Apps . As a start we will create a HelloWorld App, i.e., app-helloworld , here is an example . Git clone your new repository on your local machine - where you will be developing/editing and testing your App. git clone git@github.com:francopestilli/app-helloworld.git or (depending on your www.github.com settings): git clone https://github.com/francopestilli/app-helloworld.git Now, cd inside the local directory of the repository and create a file called main . This file contains some information about the UNIX environment ( bash-related collands ), the procedure to submit jobs in a cluster environment ( PBS-related commands ), parsing inputs from the config.json file using jq (see here for more information about jq ). For example: touch main main After creating the file main inside your local folder for the github repository app-helloworld, we will edit the content of the file and make it executable. Use your preferred editor and edit the file. Copy the text below insde the edited main file, and save it back to disk. #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -l walltime=00:05:00 #parse config.json for input parameters (here, we are pulling t1 ) t1 = $( jq -r .t1 config.json ) ./app.py $t1 Please be sure to set the file main is executable. You can do that by running thee following command in a terminal, before pushing to the github repository. chmod +x main Finally, add the file to the git repository and commit to github.com by running thee following: git add main git commit -am \"Added main file\" git push Note jq is a command line tool used to parse a small JSON file and pull values out of it. You can install it on your machine by running something like apt-get install jq or yum install jq or brew install jq depending on your Operative System (OS) or OS distribution. Also note that thee Brainlife computational resources (Cloud) wheere that App will need to run, will need to have common binaries installed including bash , jq , and singularity . For Mac Users You will need to have the XCODE, Apple Development Tools and homebrew to install jq . Once Xcode is installed run this command /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" and then this command brew install jq in a terminal. The first few lines in our main instructs PBS or Slurm batch systems to request a certain number of nodes/processes to our App. #PBS -l nodes=1:ppn=1 #PBS -l walltime=00:05:00 Note You will receive all input parameters from Brainlife through a JSON file named config.json which is created by Brainlife when your App is executed. As an App developer, you will define what parameters needs to be entered by the user and input datasets later when you register your App on Brainlife. Following lines parses the config.json using jq and the value of t1 to the main part of the application which we will create later. #parse config.json for input parameters t1 = $( jq -r .t1 config.json ) ./app.py $t1 To be able to test your application, let's create a test config.json . config.json { t1 : ~/data/t1.nii.gz } Please update the path to wherever you have your test anat/t1w input file. If you don't have any, you can download one from an the Open Diffusion Data Derivatives publication page. Just click the Datasets tab, and select any anat/t1w data to download. Then create a directory in your home directory and move the t1w.nii.gz file in there and unpack it: cd ~ mkdir data cp -v /path/to/your/downloaded/5a050966eec2b300611abff2.tar ~/data/ tar -xvf ~/data/5a050966eec2b300611abff2.tar At this point, ~/data/ should contain a file named t1w.nii.gz. Next, you should add config.json to .gitignore as config.json is created at runtime by Brainlife, and we just need this now to test your app. Hint A good pattern might be to create a file called config.json.sample used to test your App, and create a symlink ln -s config.json config.json.sample so that you can run your app using config.json.sample without including the actual config.json as part of your repo. This allows other users to construct their own config.json if they want to run your app via command line. Note Instead of parsing config.json inside main , you are free to use other parsing library as part of your App itself, such as Python's json module, or Matlab's jsonlab module. Our main script runs a python script called app.py so let's create it and edit it by compying its content as reported below. cd ~/git/app-helloworld touch app.py app.py 1 2 3 4 5 6 7 8 9 10 #!/usr/bin/env python import sys import nibabel as nib #just dump input image header to output.txt img = nib . load ( sys . argv [ 1 ]) f = open ( output.txt , w ) f . write ( str ( img . header )) f . close () Again, be sure to make app.py also executable. chmomd +x app.py Finally, add the file to the git repository and commit to github.com by running thee following: git add main git commit -am \"Added app.py file\" git push Any output files from your app should be written to the current working directory and in a file structure that complies with whichever the datatype of your dataset is. For now, we are not going to worry about the output datatype (assuming we will use raw ) Please be sure to add any output files from your app to .gitignore so that it won't be part of your git repo. .gitignore config.json output.txt Note .gitignore is a text file that instructs git to not track certain files inside your work directory. Please see ignoring files Testing Now, you should be able to test run your app locally by executing main ./main Now, it should generate an output file called output.txt containing the dump of all nifti headers. class nibabel.nifti1.Nifti1Header object, endian= sizeof_hdr : 348 data_type : db_name : extents : 0 session_error : 0 regular : r dim_info : 0 dim : [ 3 260 311 260 1 1 1 1] ... ... ... qoffset_x : 90.0 qoffset_y : -126.0 qoffset_z : -72.0 srow_x : [ -0.69999999 0. 0. 90. ] srow_y : [ 0. 0.69999999 0. -126. ] srow_z : [ 0. 0. 0.69999999 -72. ] intent_name : magic : n+1 Pushing to Github If everything looks good, push our files to the Github. git add . git commit -m created my first BL App! git push Congratulations! We have just created our first Brainlife App. To summarize, we've done following. Created a new public Github repo. Created main which parses config.json and runs our App. Created a test config.json . Created app.py which runs our algorithm and generate output files. Tested the App, and pushed all files to Github. Info You can see more concrete examples of Brainlife apps at Brainlife hosted apps . To run your App on Brainlife, you will need to do following. Register your App on Brainlife. Enable your App on at least one Brainlife compute resource. For now, please email brlife@iu.edu to enable your App on our shared test resource.","title":"HelloWorld"},{"location":"apps/helloworld/#helloworld","text":"Here, we will create a \"HelloWorld\" Brainlife App. We will show how to create a brand new github repository containing a Brainlife App. Please be sure to make the repo public so that the brainlife.io platform will be able to access it. You can name the repository as you prefer, the Brainlife Team has beeen naming apps starting with the prefix app- , for example take a look at these Apps . As a start we will create a HelloWorld App, i.e., app-helloworld , here is an example . Git clone your new repository on your local machine - where you will be developing/editing and testing your App. git clone git@github.com:francopestilli/app-helloworld.git or (depending on your www.github.com settings): git clone https://github.com/francopestilli/app-helloworld.git Now, cd inside the local directory of the repository and create a file called main . This file contains some information about the UNIX environment ( bash-related collands ), the procedure to submit jobs in a cluster environment ( PBS-related commands ), parsing inputs from the config.json file using jq (see here for more information about jq ). For example: touch main","title":"HelloWorld"},{"location":"apps/helloworld/#main","text":"After creating the file main inside your local folder for the github repository app-helloworld, we will edit the content of the file and make it executable. Use your preferred editor and edit the file. Copy the text below insde the edited main file, and save it back to disk. #!/bin/bash #PBS -l nodes=1:ppn=1 #PBS -l walltime=00:05:00 #parse config.json for input parameters (here, we are pulling t1 ) t1 = $( jq -r .t1 config.json ) ./app.py $t1 Please be sure to set the file main is executable. You can do that by running thee following command in a terminal, before pushing to the github repository. chmod +x main Finally, add the file to the git repository and commit to github.com by running thee following: git add main git commit -am \"Added main file\" git push Note jq is a command line tool used to parse a small JSON file and pull values out of it. You can install it on your machine by running something like apt-get install jq or yum install jq or brew install jq depending on your Operative System (OS) or OS distribution. Also note that thee Brainlife computational resources (Cloud) wheere that App will need to run, will need to have common binaries installed including bash , jq , and singularity . For Mac Users You will need to have the XCODE, Apple Development Tools and homebrew to install jq . Once Xcode is installed run this command /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" and then this command brew install jq in a terminal. The first few lines in our main instructs PBS or Slurm batch systems to request a certain number of nodes/processes to our App. #PBS -l nodes=1:ppn=1 #PBS -l walltime=00:05:00 Note You will receive all input parameters from Brainlife through a JSON file named config.json which is created by Brainlife when your App is executed. As an App developer, you will define what parameters needs to be entered by the user and input datasets later when you register your App on Brainlife. Following lines parses the config.json using jq and the value of t1 to the main part of the application which we will create later. #parse config.json for input parameters t1 = $( jq -r .t1 config.json ) ./app.py $t1 To be able to test your application, let's create a test config.json .","title":"main"},{"location":"apps/helloworld/#configjson","text":"{ t1 : ~/data/t1.nii.gz } Please update the path to wherever you have your test anat/t1w input file. If you don't have any, you can download one from an the Open Diffusion Data Derivatives publication page. Just click the Datasets tab, and select any anat/t1w data to download. Then create a directory in your home directory and move the t1w.nii.gz file in there and unpack it: cd ~ mkdir data cp -v /path/to/your/downloaded/5a050966eec2b300611abff2.tar ~/data/ tar -xvf ~/data/5a050966eec2b300611abff2.tar At this point, ~/data/ should contain a file named t1w.nii.gz. Next, you should add config.json to .gitignore as config.json is created at runtime by Brainlife, and we just need this now to test your app. Hint A good pattern might be to create a file called config.json.sample used to test your App, and create a symlink ln -s config.json config.json.sample so that you can run your app using config.json.sample without including the actual config.json as part of your repo. This allows other users to construct their own config.json if they want to run your app via command line. Note Instead of parsing config.json inside main , you are free to use other parsing library as part of your App itself, such as Python's json module, or Matlab's jsonlab module. Our main script runs a python script called app.py so let's create it and edit it by compying its content as reported below. cd ~/git/app-helloworld touch app.py","title":"config.json"},{"location":"apps/helloworld/#apppy","text":"1 2 3 4 5 6 7 8 9 10 #!/usr/bin/env python import sys import nibabel as nib #just dump input image header to output.txt img = nib . load ( sys . argv [ 1 ]) f = open ( output.txt , w ) f . write ( str ( img . header )) f . close () Again, be sure to make app.py also executable. chmomd +x app.py Finally, add the file to the git repository and commit to github.com by running thee following: git add main git commit -am \"Added app.py file\" git push Any output files from your app should be written to the current working directory and in a file structure that complies with whichever the datatype of your dataset is. For now, we are not going to worry about the output datatype (assuming we will use raw ) Please be sure to add any output files from your app to .gitignore so that it won't be part of your git repo.","title":"app.py"},{"location":"apps/helloworld/#gitignore","text":"config.json output.txt Note .gitignore is a text file that instructs git to not track certain files inside your work directory. Please see ignoring files","title":".gitignore"},{"location":"apps/helloworld/#testing","text":"Now, you should be able to test run your app locally by executing main ./main Now, it should generate an output file called output.txt containing the dump of all nifti headers. class nibabel.nifti1.Nifti1Header object, endian= sizeof_hdr : 348 data_type : db_name : extents : 0 session_error : 0 regular : r dim_info : 0 dim : [ 3 260 311 260 1 1 1 1] ... ... ... qoffset_x : 90.0 qoffset_y : -126.0 qoffset_z : -72.0 srow_x : [ -0.69999999 0. 0. 90. ] srow_y : [ 0. 0.69999999 0. -126. ] srow_z : [ 0. 0. 0.69999999 -72. ] intent_name : magic : n+1","title":"Testing"},{"location":"apps/helloworld/#pushing-to-github","text":"If everything looks good, push our files to the Github. git add . git commit -m created my first BL App! git push Congratulations! We have just created our first Brainlife App. To summarize, we've done following. Created a new public Github repo. Created main which parses config.json and runs our App. Created a test config.json . Created app.py which runs our algorithm and generate output files. Tested the App, and pushed all files to Github. Info You can see more concrete examples of Brainlife apps at Brainlife hosted apps . To run your App on Brainlife, you will need to do following. Register your App on Brainlife. Enable your App on at least one Brainlife compute resource. For now, please email brlife@iu.edu to enable your App on our shared test resource.","title":"Pushing to Github"},{"location":"apps/introduction/","text":"Introduction What is App ? Brainlife Apps are snippets of code comprising a (short) series of processing steps within a larger data analysis workflow. Apps are meant to be reusable by other users and not just by the App developer. Apps usage is value added to the work of the App Developer. So, the code in each App should use general tools and clarity in code writing so to make the App undeerstandable by other users. Apps are hosted on public GitHub.com repositories. Apps can comprise any combination of MatLab, Python or other types code. Apps must have a single executable file named main in the root directory of the git repository. In the most common case, main is a UNIX bash script that calls other code in the repository to run the algorithms for data analysis. The code for data analysis can be written in any language, or can be compiled binary code. Apps must read all input parameters and data files from a config.json file. config.json is created by Brainlife.io at runtime on the current working directory ( ./ , relative path ) of the compute resource that your App will run on. But you do not have to think about this actually, just write a relative path in your code when loading files from the config.json file, no need for absolute paths . Write all output files in the current directory ( ./ ), in a structure defined in a Brainlife datatype . More information about Brainlife datatypes later. Ideally, Apps should be packaged into Docker containers . But that is not a requirement. Apps Dockerizing will allow a broader App usage, because Apps can run on multiple compute systems and will most likely increase the impact of the code you write, with higher likelyhood of increasing the impact of your work as a Brainlife App developer. More information abotu Apps Dockerization can be found here . Brainlife Apps follow a technical specification called Application for Big Computational Data analysis or ABCD App Development Timeline You would normally follow following steps to develop and register your App on Brainlife. Develop an algorithm that runs on your laptop or local cluster with your test datasets. Create a sample config.json . Create main that parses config.json and pass it to your algorithm. Publish it as public github repo. Register your App on Brainlife. During this step, you can define what parameters and input file should be made available to your App via config.json . Contact resource administrators and ask them to enable your App (more below). Enabling App on a compute resource App needs to be enabled on each compute resources to run. Each user will have a different set of resources that they have access to, but Brainlife provides default shared resources for all users. If you want anyone in the Brainlife to be able to run your App, you can contact the resource administrators of these default resources to enable your Apps. You will need to discuss with resource administrators on how to handle any dependencies/libraries that your App might require. To make things easier and reproducible, you should consider Dockerizing you App's dependencies (but not the App itself) so that you can run your App through your container using singularity from your main . Hint Most compute resources now provide singularity which increases the number of resource where you might be able to run your Apps. App Launch Sequence Brainlife executes an App in following steps. A user requests to run your App through Brainlife. Brainlife queries a list of compute resources that user has access to and currently available to run your App. Brainlife then determines the best compute resource to run your App. Brainlife stages input datasets out of Brainlife's datasets archive, or transfer any dependent task's work directory that are required to run your App. Brainlife creates a new working directory by git cloning your App on (normally) a resource's scratch disk space, and place config.json containing user specified configuration parameters and various paths the input files. Brainlife then runs start hook installed on each compute resources as part of abcd specification (App developer should have to worry about this under the most circumstances). On a PBS cluster, start hook then qsub s your main script and place it on the local batch scheduler queue. Local job scheduler runs your main on a compute node and your App will execute your algorithm, and generate output on the working directory. Brainlife periodically monitors your job and relay information back to the user. Once job is completed, user archives an output dataset if the result is valid. Datatype Different Brainlife Apps can exchange input/output datasets through Brainlife datatypes which are developer defined file/directory structure that holds specific set of data. Here are some example of currently registered datatypes. neuro/anat/t1w (t1.nii.gz) neuro/anat/t2w (t2.nii.gz) neuro/dwi (diffusion data and bvecs/bvals) neuro/freesurfer (entire freesurfer output) neuro/tract (tract.tck containing fiber track data) neuro/dtiinit (dtiinit output - dti output directory) generic/images (a list of images) raw (unstructured data often used during development) Your App should read from one or more of these datatypes and write output data in a format specified by another datatype. By identifying existing datatypes that you can interoperate you can interface with datasets generated by other Apps and your output can be used by other Apps as their input. We maintain a list of datatypes in our brain-life/datatypes repo. To create a new datatype, please open an issue, or submit a PR with a new datatype definition file (.json). We do not modify datatypes once it's published to preserve backward compatibility, but you can re-register new datatype under a different version. Hint Brainlife app should follow the Do One Thing and Do It Well principle where a complex workflow should be split into several smaller Apps (but no more than necessary nor practical) to promote code-reuse and help parallelize your workflow and run each App on the most appropriate compute resource available. Hint Before writing your apps, please browse currently registered Brainlife Apps and datatypes under Brainlife.io to make sure you are not reinventing Apps. If you find an App that is similar to what you need, please contact the developer of the App and discuss if the feature you need can be added to the App.","title":"Introduction"},{"location":"apps/introduction/#introduction","text":"","title":"Introduction"},{"location":"apps/introduction/#what-is-app","text":"Brainlife Apps are snippets of code comprising a (short) series of processing steps within a larger data analysis workflow. Apps are meant to be reusable by other users and not just by the App developer. Apps usage is value added to the work of the App Developer. So, the code in each App should use general tools and clarity in code writing so to make the App undeerstandable by other users. Apps are hosted on public GitHub.com repositories. Apps can comprise any combination of MatLab, Python or other types code. Apps must have a single executable file named main in the root directory of the git repository. In the most common case, main is a UNIX bash script that calls other code in the repository to run the algorithms for data analysis. The code for data analysis can be written in any language, or can be compiled binary code. Apps must read all input parameters and data files from a config.json file. config.json is created by Brainlife.io at runtime on the current working directory ( ./ , relative path ) of the compute resource that your App will run on. But you do not have to think about this actually, just write a relative path in your code when loading files from the config.json file, no need for absolute paths . Write all output files in the current directory ( ./ ), in a structure defined in a Brainlife datatype . More information about Brainlife datatypes later. Ideally, Apps should be packaged into Docker containers . But that is not a requirement. Apps Dockerizing will allow a broader App usage, because Apps can run on multiple compute systems and will most likely increase the impact of the code you write, with higher likelyhood of increasing the impact of your work as a Brainlife App developer. More information abotu Apps Dockerization can be found here . Brainlife Apps follow a technical specification called Application for Big Computational Data analysis or ABCD","title":"What is App?"},{"location":"apps/introduction/#app-development-timeline","text":"You would normally follow following steps to develop and register your App on Brainlife. Develop an algorithm that runs on your laptop or local cluster with your test datasets. Create a sample config.json . Create main that parses config.json and pass it to your algorithm. Publish it as public github repo. Register your App on Brainlife. During this step, you can define what parameters and input file should be made available to your App via config.json . Contact resource administrators and ask them to enable your App (more below).","title":"App Development Timeline"},{"location":"apps/introduction/#enabling-app-on-a-compute-resource","text":"App needs to be enabled on each compute resources to run. Each user will have a different set of resources that they have access to, but Brainlife provides default shared resources for all users. If you want anyone in the Brainlife to be able to run your App, you can contact the resource administrators of these default resources to enable your Apps. You will need to discuss with resource administrators on how to handle any dependencies/libraries that your App might require. To make things easier and reproducible, you should consider Dockerizing you App's dependencies (but not the App itself) so that you can run your App through your container using singularity from your main . Hint Most compute resources now provide singularity which increases the number of resource where you might be able to run your Apps.","title":"Enabling App on a compute resource"},{"location":"apps/introduction/#app-launch-sequence","text":"Brainlife executes an App in following steps. A user requests to run your App through Brainlife. Brainlife queries a list of compute resources that user has access to and currently available to run your App. Brainlife then determines the best compute resource to run your App. Brainlife stages input datasets out of Brainlife's datasets archive, or transfer any dependent task's work directory that are required to run your App. Brainlife creates a new working directory by git cloning your App on (normally) a resource's scratch disk space, and place config.json containing user specified configuration parameters and various paths the input files. Brainlife then runs start hook installed on each compute resources as part of abcd specification (App developer should have to worry about this under the most circumstances). On a PBS cluster, start hook then qsub s your main script and place it on the local batch scheduler queue. Local job scheduler runs your main on a compute node and your App will execute your algorithm, and generate output on the working directory. Brainlife periodically monitors your job and relay information back to the user. Once job is completed, user archives an output dataset if the result is valid.","title":"App Launch Sequence"},{"location":"apps/introduction/#datatype","text":"Different Brainlife Apps can exchange input/output datasets through Brainlife datatypes which are developer defined file/directory structure that holds specific set of data. Here are some example of currently registered datatypes. neuro/anat/t1w (t1.nii.gz) neuro/anat/t2w (t2.nii.gz) neuro/dwi (diffusion data and bvecs/bvals) neuro/freesurfer (entire freesurfer output) neuro/tract (tract.tck containing fiber track data) neuro/dtiinit (dtiinit output - dti output directory) generic/images (a list of images) raw (unstructured data often used during development) Your App should read from one or more of these datatypes and write output data in a format specified by another datatype. By identifying existing datatypes that you can interoperate you can interface with datasets generated by other Apps and your output can be used by other Apps as their input. We maintain a list of datatypes in our brain-life/datatypes repo. To create a new datatype, please open an issue, or submit a PR with a new datatype definition file (.json). We do not modify datatypes once it's published to preserve backward compatibility, but you can re-register new datatype under a different version. Hint Brainlife app should follow the Do One Thing and Do It Well principle where a complex workflow should be split into several smaller Apps (but no more than necessary nor practical) to promote code-reuse and help parallelize your workflow and run each App on the most appropriate compute resource available. Hint Before writing your apps, please browse currently registered Brainlife Apps and datatypes under Brainlife.io to make sure you are not reinventing Apps. If you find an App that is similar to what you need, please contact the developer of the App and discuss if the feature you need can be added to the App.","title":"Datatype"},{"location":"apps/product/","text":"product.json Your App can optionally generate a file named product.json to send information back to Brainlife. You can think of it as the opposite end of config.json . While config.json is used to send inputs and configuration from the user to your App, product.json will be loaded by Brainlife upon successful completion of your App and information will be stored in the Brainlife's internal database. You can use product.json to relay information back to the App submitter, or used to perform data aggregation quickly across multiple subjects. Warning You should not store more than a few kilobytes of information on product.json . You can use product.json for the following purposes. Display messages, statuses, graphs(plotly) on Brainlife UI. Store small amount of unstructured data that you can later use to quickly aggregate across multiple output datasets. Specify user tags (or datatype_tags) to be added to the output dataset. We will explain each use cases below. 1. Displaying messages, graphs on Brainlife UI Although product.json can store any data, we've defined a special key( \"brainlife\" ) that you could use to render graphical information back to Brainlife UI. Messages (error / warnings) You can display error / warning message on Brainlife process UI by storing them in following format. { brainlife : [ { type : error , msg : Some tracts have less than 20 streamlines. Check quality of data! }, ] } Above message will be displayed under the Output section of the process UI as well as dataset detail page, like the following. You can also display following message types. { brainlife : [ { type : error , msg : here is my error message }, { type : danger , msg : here is my error message }, { type : info , msg : here is my info message }, { type : warning , msg : here is my warning message }, { type : success , msg : here is my warning message }, ] } Graphs (plotly) You can also display plotly graph. { brainlife : [ { type : plotly , name : my plotly test , data : [ { x : 2014-06-11 , y : 10 }, { x : 2014-06-12 , y : 25 }, { x : 2014-06-13 , y : 30 }, { x : 2014-06-14 , y : 10 }, { x : 2014-06-15 , y : 15 }, { x : 2014-06-16 , y : 30 } ], layout : { start: 2014-06-10 , end: 2014-06-18 }, } ] } 2. Storing unstructured data Any unstructured data can be just stored anywhere inside the product.json . Like.. { rmse : 123 , } You might have App that could produce multi-gigabytes of output datasets. If you need to run aggregation process across many subjects using such outputs, you might end up needing to stage hundreds of gigabytes of output datasets which may or may not be possible, or desirable to do so. Instead, you could produce pre-aggregated data and store it inside product.json . You can then query all datasets and download product.json contents from each dataset without having to download the actual output datasets. Please take a look at Brainlife CLI for more information on querying datasets. 3. Specifying dataset (user) tags Your App can set \"tags\" field to specify dataset tags that you'd like to add to the dataset output. { tags : [ tags_from_app ] } You can use this feature to set important information about the processing done to the dataset and pass it down the pipeline.","title":"product.json"},{"location":"apps/product/#productjson","text":"Your App can optionally generate a file named product.json to send information back to Brainlife. You can think of it as the opposite end of config.json . While config.json is used to send inputs and configuration from the user to your App, product.json will be loaded by Brainlife upon successful completion of your App and information will be stored in the Brainlife's internal database. You can use product.json to relay information back to the App submitter, or used to perform data aggregation quickly across multiple subjects. Warning You should not store more than a few kilobytes of information on product.json . You can use product.json for the following purposes. Display messages, statuses, graphs(plotly) on Brainlife UI. Store small amount of unstructured data that you can later use to quickly aggregate across multiple output datasets. Specify user tags (or datatype_tags) to be added to the output dataset. We will explain each use cases below.","title":"product.json"},{"location":"apps/product/#1-displaying-messages-graphs-on-brainlife-ui","text":"Although product.json can store any data, we've defined a special key( \"brainlife\" ) that you could use to render graphical information back to Brainlife UI.","title":"1. Displaying messages, graphs on Brainlife UI"},{"location":"apps/product/#messages-error-warnings","text":"You can display error / warning message on Brainlife process UI by storing them in following format. { brainlife : [ { type : error , msg : Some tracts have less than 20 streamlines. Check quality of data! }, ] } Above message will be displayed under the Output section of the process UI as well as dataset detail page, like the following. You can also display following message types. { brainlife : [ { type : error , msg : here is my error message }, { type : danger , msg : here is my error message }, { type : info , msg : here is my info message }, { type : warning , msg : here is my warning message }, { type : success , msg : here is my warning message }, ] }","title":"Messages (error / warnings)"},{"location":"apps/product/#graphs-plotly","text":"You can also display plotly graph. { brainlife : [ { type : plotly , name : my plotly test , data : [ { x : 2014-06-11 , y : 10 }, { x : 2014-06-12 , y : 25 }, { x : 2014-06-13 , y : 30 }, { x : 2014-06-14 , y : 10 }, { x : 2014-06-15 , y : 15 }, { x : 2014-06-16 , y : 30 } ], layout : { start: 2014-06-10 , end: 2014-06-18 }, } ] }","title":"Graphs (plotly)"},{"location":"apps/product/#2-storing-unstructured-data","text":"Any unstructured data can be just stored anywhere inside the product.json . Like.. { rmse : 123 , } You might have App that could produce multi-gigabytes of output datasets. If you need to run aggregation process across many subjects using such outputs, you might end up needing to stage hundreds of gigabytes of output datasets which may or may not be possible, or desirable to do so. Instead, you could produce pre-aggregated data and store it inside product.json . You can then query all datasets and download product.json contents from each dataset without having to download the actual output datasets. Please take a look at Brainlife CLI for more information on querying datasets.","title":"2. Storing unstructured data"},{"location":"apps/product/#3-specifying-dataset-user-tags","text":"Your App can set \"tags\" field to specify dataset tags that you'd like to add to the dataset output. { tags : [ tags_from_app ] } You can use this feature to set important information about the processing done to the dataset and pass it down the pipeline.","title":"3. Specifying dataset (user) tags"},{"location":"apps/register/","text":"Registering App Once your App is published on github, you can now register it on Brainlife and let you and other users discover your App and execute on Brainlife. First, go to the Apps page on Brainlife, click Plus Button at the bottom right corner of the page. App registration form should appear. Let's go through each sections. Detail Enter any name for your App and Git Repository Name field which is the the organization / repository name (like yourname/app-name ) of your github repo. Please do not enter the full github URL. All other fields in this section are optional, but you could populate following fields. Avatar You can enter avatar URL if you have URL for an avatar that you'd like to use for your App. Please choose a square image with https:// URL (not http:// ). Avatar may sounds superfluous, but please keep in mind that there are many other Apps registered on Brainlife and this might be the only visual queue for users to identify and search for your App. If you don't specify Avatar URL, Brainlife will use a randomly generated (robots) Avatar. Project By default, all Apps are public meaning any user can find your App and execute your App. If you'd like to make your App only available on a specific project (and their group members), you can enter project names under Project field and only the member of that project will be able to access your App. This might be useful if you are still developing your App and wants to keep it hidden until you make a formal release , or if your App would only function on datasets stored under a specific project. Branch If you don't specify the github repo's branch name, it uses master branch by default. Most developer makes the latest code changes on master branch. If you leave the Branch field empty and let it use the master branch by default, a user won't be able to reproduce the output with exactly the same version of your code if you make any changes to it. Once you finish developing your App, you should consider creating a release branch (like 1.0 ) and specifying that branch name for your App so that Brainlife will always run the specific version of your App. Brainlife stores the branch name used to execute each task, so this allows users to reproduce the output using the same version of the code. Please see Versioning Tip for more info! Input Datasets Here you can define a list of input datasets that your App is expecting. ID This is just an ID to uniquely identify this input dataset. Please enter any ID you'd like to use. It just has to be unique among all other input datasets. Datatype/Tags The datatype/tags of this input dataset. Please enter any datatype tags that your App would require under Datatype Tags field. Brainlife will only allow users to select dataset that meets specified datatype tags. Hint If you don't know which datatype to use, please consult the #datatype slack channel on Brainlife slack team. Please read datatypes for more information. File Mapping Once you select the datatype/tags for your input dataset, you then need to configure how you want the selected dataset to be represented in the config.json . Each datatype consists of various files and directories. Here, you can map the object key in config.json to a particular file / directory within the datatype. For example, neuro/dwi datatype consists of dwi, bvecs, and bvals files. If your App somehow only uses the dwi file, and if you'd like to receive the path to the dwi as dwi inside the config.json , you can configure it as following. When a user submits your App, it will generate config.json that looks like this { dwi : ../path/to/dwi.nii.gz } Your App can parse config.json and use the value for dwi as the file path pointing to the input dwi image. Note If you want to use all 3 files from neuro/dwi datatype, you have to create 3 file mappings for each files; dwi, bvecs, and bvals. Optional Click this to make this input dataset optional. Leave it unchecked if it's a required field. If you make it optional and user doesn't provide the input, Brainlife will generate config.json without any keys defined in the File Mapping section. Multi Click this if you'd like to allow users to select multiple input datasets. Selected datasests will be placed inside a json array. If user select only 1 dataset, it will still be placed inside a json array. For example, above config.json will be generated as the following. { dwi : [ ../path/to/dwi.nii.gz ] } Note The index of the datasets listed in the array will be preserved across all File Mappings if there are more than 1 file mapping for this input. Output Datasets Similar to the input datasets, you can specify the datatypes of your output datasets here. It's up to developer to decide which datatype to use, and produce output files in the correct file structure / file names according to the specification of the datatype. Datatype Tags You can add specificities / context to the selected datatype. For example, above screenshot shows this App outputs anat/t1w datatype with a tag acpc_aligned . If there is an App that only works with ACPC aligned anat/t1w as an input dataset, it can specify the same tag as a required input datatype tag to be more specific about its input dataset. Please read datatypes page for more information on datatypes. Tag Passthrough Some App behaves as a filter ; it receives an input dataset and produces another dataset with the same datetype. In this circumstance, the App often needs to add new datatype tags rather than completely replacing them. To accomplish this, you can set this field to the ID of the input dataset that you'd like to copy all datatype tags from. For example, if the input dataset contains defaced datatype tag, the output dataset will have both acpc_aligned and defaced as the output datatype tags. Datatype File Mapping By default, Brainlife expects you to generate all output files in a file structure expected by each datatype on the root of the current working directory. However, if you have more than one output datasets with the same datatype, or have multiple datasets with colliding file/directory names as specified by each datatype, you will not be able to output them all under the root of the current directory. To solve this issue, you can output each dataset under a different filename or in a sub-directory, and configure the Datatype File Mapping field to override which file/dir should corresponds to which file/dir within each datatype. For example, following example show that the App is producing a file called output.DT_STREAM.tck which should treated as a track.tck file output for neuro/track datatype. (\"track\" is the file ID for \"track.tck\" as defined by neuro/track datatype). Or, if you'd like to store the entire raw output files in a sub directory called \"output\", you can specify the directory by.. Note The JSON key for each file mapping needs to be the file/dir ID defined for each datatype. You will need to find the datatype definition to know which file/dir ID to use. Please consult Brainlife slack team if you need a help. raw datatype Your App may generate output data that is not meant to be used by any other Apps, at least initially. You can use raw datatype to output and archive such output data. You should avoid using raw datatype as an input datatype, however. If you are trying to use other App's raw data as your input dataset, it is probably a good indication that the upstream App developer and you should discuss and define a new datatype so that both Apps can interoperate through a well defined datatype. Please contact the other developer and discuss how the data should be structured, and submit a new issue on brain-life/datatypes and/or a pull request containing the list of files/directories to be registered on Brainlife. Configuration Parameters Configuration parameters allow users to enter any number (integer/float), boolean (true/false) or string parameters as configuration parameters for your App. You can also define a enum parameter which lets users select from multiple options. Placeholder For each input parameter, you can set a placeholder (a string displayed inside the form element if no value is entered yet). For example, you could use placeholder to let user know the format of the values, or some samples. Description Some configuration parameters let you specify a description which will be displayed next to the input parameter to show detailed explanation for the input parameter. Please provide enough details for both novice and experienced users of your App. Note For Novice Users Brainlife is a platform for both novice and experienced users. For novice user, please add enough details for each configuration parameter and instruction about how to find a correct values to set for each parameters (a link to other webpage, point to README, etcs) Please make as many parameters optional as possible, and auto-detect the optimal values at runtime if user does not provide/override the default option. For Experienced Users At the same time, your App should not become a blackbox for experienced users. You should allow them to choose/override any parameters and allow them to be fully in control of how the algorithm works. Finally, click Submit . Visit the Apps page to make sure everything looks OK. README / Description / Topics Brainlife re-uses information stored in github repo. App Description / Topics Your Github repo description is used to display Brainlife App description. Please be sure to enter description that shows what the App does, and what user can do with the output. Github topics are also used to organize Brainlife's App by placing them under various categories . Please look through the existing categories already registered in Brainlife, and reuse one or more of those categories to help users find your App more easily. Note Please avoid using too many topics . Also please avoid using topics that are not yet used by any other Apps (it will create a category with a single App) README.md Brainlife displays README.md content from your github repo. You can include any images, katex equations, or any other standard markdown syntax . You should include information such as.. What your App does, and how it's implemented (tools, libraries used) What you App produces and what user can do with it Any diagrams / sample output images Details on how the algorithm works Computational cost / resources required to run your App (how long does it take to run, minimum required memory / cpu cores, etc..) How can other users contribute (Are you accepting any PR?) References to other published papers, or list of contributors. Note Brainlife is for both novice and experienced neuroscience researchers. Please try to cater for both groups of users. Enabling App on resource Once you registered your App on Brainlife, you then need to enable your App on resources where you can run it. A resource could be any VM, HPC cluster, or public / private cloud resource. Only the resource owners/administrators can enable your App to run on their resources. Please contact the resource administrator for the resource where you'd like to submit your App. Note If you are not sure who the resource administrator is, please contact Brainlife . If you have access to your own computing resources, you can register personal resources and run your App there to test. Please read registering resource page for more detail. Please keep in mind that, on personal resources, only you can run enabled Apps on those resources. To allow other users to run your App, you will need to enable it on Brainlife's shared resources. Once your App is enabled on various resources, you should be able to see them listed under computing resources section in the App details page.","title":"Registering App"},{"location":"apps/register/#registering-app","text":"Once your App is published on github, you can now register it on Brainlife and let you and other users discover your App and execute on Brainlife. First, go to the Apps page on Brainlife, click Plus Button at the bottom right corner of the page. App registration form should appear. Let's go through each sections.","title":"Registering App"},{"location":"apps/register/#detail","text":"Enter any name for your App and Git Repository Name field which is the the organization / repository name (like yourname/app-name ) of your github repo. Please do not enter the full github URL. All other fields in this section are optional, but you could populate following fields.","title":"Detail"},{"location":"apps/register/#avatar","text":"You can enter avatar URL if you have URL for an avatar that you'd like to use for your App. Please choose a square image with https:// URL (not http:// ). Avatar may sounds superfluous, but please keep in mind that there are many other Apps registered on Brainlife and this might be the only visual queue for users to identify and search for your App. If you don't specify Avatar URL, Brainlife will use a randomly generated (robots) Avatar.","title":"Avatar"},{"location":"apps/register/#project","text":"By default, all Apps are public meaning any user can find your App and execute your App. If you'd like to make your App only available on a specific project (and their group members), you can enter project names under Project field and only the member of that project will be able to access your App. This might be useful if you are still developing your App and wants to keep it hidden until you make a formal release , or if your App would only function on datasets stored under a specific project.","title":"Project"},{"location":"apps/register/#branch","text":"If you don't specify the github repo's branch name, it uses master branch by default. Most developer makes the latest code changes on master branch. If you leave the Branch field empty and let it use the master branch by default, a user won't be able to reproduce the output with exactly the same version of your code if you make any changes to it. Once you finish developing your App, you should consider creating a release branch (like 1.0 ) and specifying that branch name for your App so that Brainlife will always run the specific version of your App. Brainlife stores the branch name used to execute each task, so this allows users to reproduce the output using the same version of the code. Please see Versioning Tip for more info!","title":"Branch"},{"location":"apps/register/#input-datasets","text":"Here you can define a list of input datasets that your App is expecting.","title":"Input Datasets"},{"location":"apps/register/#id","text":"This is just an ID to uniquely identify this input dataset. Please enter any ID you'd like to use. It just has to be unique among all other input datasets.","title":"ID"},{"location":"apps/register/#datatypetags","text":"The datatype/tags of this input dataset. Please enter any datatype tags that your App would require under Datatype Tags field. Brainlife will only allow users to select dataset that meets specified datatype tags. Hint If you don't know which datatype to use, please consult the #datatype slack channel on Brainlife slack team. Please read datatypes for more information.","title":"Datatype/Tags"},{"location":"apps/register/#file-mapping","text":"Once you select the datatype/tags for your input dataset, you then need to configure how you want the selected dataset to be represented in the config.json . Each datatype consists of various files and directories. Here, you can map the object key in config.json to a particular file / directory within the datatype. For example, neuro/dwi datatype consists of dwi, bvecs, and bvals files. If your App somehow only uses the dwi file, and if you'd like to receive the path to the dwi as dwi inside the config.json , you can configure it as following. When a user submits your App, it will generate config.json that looks like this { dwi : ../path/to/dwi.nii.gz } Your App can parse config.json and use the value for dwi as the file path pointing to the input dwi image. Note If you want to use all 3 files from neuro/dwi datatype, you have to create 3 file mappings for each files; dwi, bvecs, and bvals.","title":"File Mapping"},{"location":"apps/register/#optional","text":"Click this to make this input dataset optional. Leave it unchecked if it's a required field. If you make it optional and user doesn't provide the input, Brainlife will generate config.json without any keys defined in the File Mapping section.","title":"Optional"},{"location":"apps/register/#multi","text":"Click this if you'd like to allow users to select multiple input datasets. Selected datasests will be placed inside a json array. If user select only 1 dataset, it will still be placed inside a json array. For example, above config.json will be generated as the following. { dwi : [ ../path/to/dwi.nii.gz ] } Note The index of the datasets listed in the array will be preserved across all File Mappings if there are more than 1 file mapping for this input.","title":"Multi"},{"location":"apps/register/#output-datasets","text":"Similar to the input datasets, you can specify the datatypes of your output datasets here. It's up to developer to decide which datatype to use, and produce output files in the correct file structure / file names according to the specification of the datatype.","title":"Output Datasets"},{"location":"apps/register/#datatype-tags","text":"You can add specificities / context to the selected datatype. For example, above screenshot shows this App outputs anat/t1w datatype with a tag acpc_aligned . If there is an App that only works with ACPC aligned anat/t1w as an input dataset, it can specify the same tag as a required input datatype tag to be more specific about its input dataset. Please read datatypes page for more information on datatypes.","title":"Datatype Tags"},{"location":"apps/register/#tag-passthrough","text":"Some App behaves as a filter ; it receives an input dataset and produces another dataset with the same datetype. In this circumstance, the App often needs to add new datatype tags rather than completely replacing them. To accomplish this, you can set this field to the ID of the input dataset that you'd like to copy all datatype tags from. For example, if the input dataset contains defaced datatype tag, the output dataset will have both acpc_aligned and defaced as the output datatype tags.","title":"Tag Passthrough"},{"location":"apps/register/#datatype-file-mapping","text":"By default, Brainlife expects you to generate all output files in a file structure expected by each datatype on the root of the current working directory. However, if you have more than one output datasets with the same datatype, or have multiple datasets with colliding file/directory names as specified by each datatype, you will not be able to output them all under the root of the current directory. To solve this issue, you can output each dataset under a different filename or in a sub-directory, and configure the Datatype File Mapping field to override which file/dir should corresponds to which file/dir within each datatype. For example, following example show that the App is producing a file called output.DT_STREAM.tck which should treated as a track.tck file output for neuro/track datatype. (\"track\" is the file ID for \"track.tck\" as defined by neuro/track datatype). Or, if you'd like to store the entire raw output files in a sub directory called \"output\", you can specify the directory by.. Note The JSON key for each file mapping needs to be the file/dir ID defined for each datatype. You will need to find the datatype definition to know which file/dir ID to use. Please consult Brainlife slack team if you need a help.","title":"Datatype File Mapping"},{"location":"apps/register/#raw-datatype","text":"Your App may generate output data that is not meant to be used by any other Apps, at least initially. You can use raw datatype to output and archive such output data. You should avoid using raw datatype as an input datatype, however. If you are trying to use other App's raw data as your input dataset, it is probably a good indication that the upstream App developer and you should discuss and define a new datatype so that both Apps can interoperate through a well defined datatype. Please contact the other developer and discuss how the data should be structured, and submit a new issue on brain-life/datatypes and/or a pull request containing the list of files/directories to be registered on Brainlife.","title":"raw datatype"},{"location":"apps/register/#configuration-parameters","text":"Configuration parameters allow users to enter any number (integer/float), boolean (true/false) or string parameters as configuration parameters for your App. You can also define a enum parameter which lets users select from multiple options. Placeholder For each input parameter, you can set a placeholder (a string displayed inside the form element if no value is entered yet). For example, you could use placeholder to let user know the format of the values, or some samples. Description Some configuration parameters let you specify a description which will be displayed next to the input parameter to show detailed explanation for the input parameter. Please provide enough details for both novice and experienced users of your App. Note For Novice Users Brainlife is a platform for both novice and experienced users. For novice user, please add enough details for each configuration parameter and instruction about how to find a correct values to set for each parameters (a link to other webpage, point to README, etcs) Please make as many parameters optional as possible, and auto-detect the optimal values at runtime if user does not provide/override the default option. For Experienced Users At the same time, your App should not become a blackbox for experienced users. You should allow them to choose/override any parameters and allow them to be fully in control of how the algorithm works. Finally, click Submit . Visit the Apps page to make sure everything looks OK.","title":"Configuration Parameters"},{"location":"apps/register/#readme-description-topics","text":"Brainlife re-uses information stored in github repo. App Description / Topics Your Github repo description is used to display Brainlife App description. Please be sure to enter description that shows what the App does, and what user can do with the output. Github topics are also used to organize Brainlife's App by placing them under various categories . Please look through the existing categories already registered in Brainlife, and reuse one or more of those categories to help users find your App more easily. Note Please avoid using too many topics . Also please avoid using topics that are not yet used by any other Apps (it will create a category with a single App) README.md Brainlife displays README.md content from your github repo. You can include any images, katex equations, or any other standard markdown syntax . You should include information such as.. What your App does, and how it's implemented (tools, libraries used) What you App produces and what user can do with it Any diagrams / sample output images Details on how the algorithm works Computational cost / resources required to run your App (how long does it take to run, minimum required memory / cpu cores, etc..) How can other users contribute (Are you accepting any PR?) References to other published papers, or list of contributors. Note Brainlife is for both novice and experienced neuroscience researchers. Please try to cater for both groups of users.","title":"README / Description / Topics"},{"location":"apps/register/#enabling-app-on-resource","text":"Once you registered your App on Brainlife, you then need to enable your App on resources where you can run it. A resource could be any VM, HPC cluster, or public / private cloud resource. Only the resource owners/administrators can enable your App to run on their resources. Please contact the resource administrator for the resource where you'd like to submit your App. Note If you are not sure who the resource administrator is, please contact Brainlife . If you have access to your own computing resources, you can register personal resources and run your App there to test. Please read registering resource page for more detail. Please keep in mind that, on personal resources, only you can run enabled Apps on those resources. To allow other users to run your App, you will need to enable it on Brainlife's shared resources. Once your App is enabled on various resources, you should be able to see them listed under computing resources section in the App details page.","title":"Enabling App on resource"},{"location":"apps/versioning/","text":"Although Brainlife does not require you to use any specific git branching schema and you should observe any standrad practice from your own group, here are some guidelines that we suggest. 1. git pull often Before you start editing your app on your local machine each day, be sure to pull from origin. git pull You should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master. git rebase See git rebase if you are not familiar with rebasing. It could help with keeping our commit log clean. 2. Always work on master branch When you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin. 3. Create branch for new versions When we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. You can easily create a new branch using github UI. Once you create a branch, you should update the BL app to point users to use that new branch. You could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also. We recommend creating new branches for each release. 4. Bug fix on master, then on branch. If you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I recommend using command like cherry-pick to apply specific changes on other branches. Again, we should not add any new features on branches (only bug fixes) for reproducibility. 5. Semver If you don't know what semantic versioning is, please read https://semver.org/ . For branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new Brainlife app with different major version number so that user can continue to submit your app with previous versions.","title":"Versioning Tips"},{"location":"apps/versioning/#1-git-pull-often","text":"Before you start editing your app on your local machine each day, be sure to pull from origin. git pull You should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master. git rebase See git rebase if you are not familiar with rebasing. It could help with keeping our commit log clean.","title":"1. git pull often"},{"location":"apps/versioning/#2-always-work-on-master-branch","text":"When you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin.","title":"2. Always work on master branch"},{"location":"apps/versioning/#3-create-branch-for-new-versions","text":"When we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. You can easily create a new branch using github UI. Once you create a branch, you should update the BL app to point users to use that new branch. You could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also. We recommend creating new branches for each release.","title":"3. Create branch for new versions"},{"location":"apps/versioning/#4-bug-fix-on-master-then-on-branch","text":"If you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I recommend using command like cherry-pick to apply specific changes on other branches. Again, we should not add any new features on branches (only bug fixes) for reproducibility.","title":"4. Bug fix on master, then on branch."},{"location":"apps/versioning/#5-semver","text":"If you don't know what semantic versioning is, please read https://semver.org/ . For branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new Brainlife app with different major version number so that user can continue to submit your app with previous versions.","title":"5. Semver"},{"location":"resources/register/","text":"Background Brainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled. As Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases. You have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control. You are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging. You are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide. Currently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact Brainlife Admin Note Resource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact Brainlife Admin to enable your app on Brainlife default resources. Warning Although we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security especially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information). Registering Resources To register your resource, go to Brainlife Settings page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields. Name Enter the name of rhe resource Hostname The hostname of your compute resource (usually a login/submit host) Username Username used to ssh to this resource Workdir Directory used to stage and store generated datasets by apps. You should not share the same directory with other resources . Please make sure that the specified directory exits (mkdir if not). SSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read authorized_keys for more detail. You can leave the rest of the fields empty for now. Click OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good. Configuring Resources Once you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps. ABCD Default Hooks ABCD Hooks are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting $PATH . If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following. cd ~ git clone https://github.com/brain-life/abcd-spec Then, add one of following to your ~/.bashrc For PBS cluster export PATH=~/abcd-spec/hooks/pbs:$PATH For Slurm cluster export PATH=~/abcd-spec/hooks/slurm:$PATH For direct execution - no batch submission manager export PATH=~/abcd-spec/hooks/direct:$PATH Common Binaries Brainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed. jq (command line json parser commonly used by Brainlife apps to parse config.json) git (used to clone / update apps installed) singularity (user level container execution engine) For IU HPC resource, please feel free to use following ~/bin directory which contains jq $ ~/.bashrc export PATH = $PATH :/N/u/brlife/Carbonate/bin For singularity, you can either install it on the system, or for most HPC systems you can simply add following in your ~/.modules file. module load singularity By default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc export SINGULARITY_CACHEDIR=/N/dc2/scratch/ username /singularity-cachedir Please replace with your username, and make sure specified directories exists. Other ENV parameters Depending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via FREESURFER_LICENSE . export FREESURFER_LICENSE= hayashis@iu.edu 29511 *xxxxxxxxxxx xxxxxxxxxxx Enabling Apps Once you have registered and tested your resource, you can now enable apps to run on your resource. Go back to the Brainlife's resource settings page , and click the resource you have created. Under the services section, enter the git org/repo name (such as like brain-life/app-life ) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK. You can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife. example","title":"Registering Resource"},{"location":"resources/register/#background","text":"Brainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled. As Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases. You have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control. You are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging. You are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide. Currently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact Brainlife Admin Note Resource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact Brainlife Admin to enable your app on Brainlife default resources. Warning Although we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security especially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information).","title":"Background"},{"location":"resources/register/#registering-resources","text":"To register your resource, go to Brainlife Settings page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields. Name Enter the name of rhe resource Hostname The hostname of your compute resource (usually a login/submit host) Username Username used to ssh to this resource Workdir Directory used to stage and store generated datasets by apps. You should not share the same directory with other resources . Please make sure that the specified directory exits (mkdir if not). SSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read authorized_keys for more detail. You can leave the rest of the fields empty for now. Click OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good.","title":"Registering Resources"},{"location":"resources/register/#configuring-resources","text":"Once you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps.","title":"Configuring Resources"},{"location":"resources/register/#abcd-default-hooks","text":"ABCD Hooks are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting $PATH . If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following. cd ~ git clone https://github.com/brain-life/abcd-spec Then, add one of following to your ~/.bashrc","title":"ABCD Default Hooks"},{"location":"resources/register/#for-pbs-cluster","text":"export PATH=~/abcd-spec/hooks/pbs:$PATH","title":"For PBS cluster"},{"location":"resources/register/#for-slurm-cluster","text":"export PATH=~/abcd-spec/hooks/slurm:$PATH","title":"For Slurm cluster"},{"location":"resources/register/#for-direct-execution-no-batch-submission-manager","text":"export PATH=~/abcd-spec/hooks/direct:$PATH","title":"For direct execution - no batch submission manager"},{"location":"resources/register/#common-binaries","text":"Brainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed. jq (command line json parser commonly used by Brainlife apps to parse config.json) git (used to clone / update apps installed) singularity (user level container execution engine) For IU HPC resource, please feel free to use following ~/bin directory which contains jq $ ~/.bashrc export PATH = $PATH :/N/u/brlife/Carbonate/bin For singularity, you can either install it on the system, or for most HPC systems you can simply add following in your ~/.modules file. module load singularity By default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc export SINGULARITY_CACHEDIR=/N/dc2/scratch/ username /singularity-cachedir Please replace with your username, and make sure specified directories exists.","title":"Common Binaries"},{"location":"resources/register/#other-env-parameters","text":"Depending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via FREESURFER_LICENSE . export FREESURFER_LICENSE= hayashis@iu.edu 29511 *xxxxxxxxxxx xxxxxxxxxxx","title":"Other ENV parameters"},{"location":"resources/register/#enabling-apps","text":"Once you have registered and tested your resource, you can now enable apps to run on your resource. Go back to the Brainlife's resource settings page , and click the resource you have created. Under the services section, enter the git org/repo name (such as like brain-life/app-life ) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK. You can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife. example","title":"Enabling Apps"},{"location":"technical/api/","text":"Brainlife API This document describes some of Brainlife's microservices in case you might be interested in directly interfacing with them through APIs. Warehouse Warehouse is the main application responsible for bulk of Brainlife platform UI. Warehouse Github Warehouse API Doc Note Brainlife CLI interacts with Warehouse API to import / export datasets, query task status, and among other things. At the moment, we have a very limited CLI support, but please try using our CLI tool if you just want to interface with Brainlife Warehouse via commandline CLI Github Amaretti Amaretti is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information. Amaretti Doc AMaretti Github Amaretti API Doc Authentication Service Auth Github Auth API Doc Event Service Event Github Event API Doc Profile Service Profile Github Profile API Doc","title":"APIs"},{"location":"technical/api/#brainlife-api","text":"This document describes some of Brainlife's microservices in case you might be interested in directly interfacing with them through APIs.","title":"Brainlife API"},{"location":"technical/api/#warehouse","text":"Warehouse is the main application responsible for bulk of Brainlife platform UI. Warehouse Github Warehouse API Doc Note Brainlife CLI interacts with Warehouse API to import / export datasets, query task status, and among other things. At the moment, we have a very limited CLI support, but please try using our CLI tool if you just want to interface with Brainlife Warehouse via commandline CLI Github","title":"Warehouse"},{"location":"technical/api/#amaretti","text":"Amaretti is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information. Amaretti Doc AMaretti Github Amaretti API Doc","title":"Amaretti"},{"location":"technical/api/#authentication-service","text":"Auth Github Auth API Doc","title":"Authentication Service"},{"location":"technical/api/#event-service","text":"Event Github Event API Doc","title":"Event Service"},{"location":"technical/api/#profile-service","text":"Profile Github Profile API Doc","title":"Profile Service"},{"location":"technical/arthitecture/","text":"Brainlife Architecture TODO..","title":"Architecture"},{"location":"technical/arthitecture/#brainlife-architecture","text":"TODO..","title":"Brainlife Architecture"},{"location":"user/datatypes/","text":"Datatypes Please read Tutorial / Datatypes first. Brainlife Apps exchange data through datatypes . Each datatype consists of name , description , and a list of files (or directories) that define the overall structure of the datatype. For example, the following is a datatype definition for neuro/life datatype. { name : neuro/life , desc : LiFE Output (fe structure) , files : [ { id : fe , filename : output_fe.mat , desc : FE structure , ext : .mat , required : true }, { id : life_results , filename : life_results.json , required : true }, { id : tracts , dirname : tracts , required : true } ] } For this example datatype, brain-life/app-life App generates a dataset with this datatype, and other Apps that want to use neuro/life output can request to have those files made available to their Apps by registering them on Brainlife App registration form. (Please read Registering App page for more info). The actual content/semantics of each file are up to developers exchanging the dataset to decide, and it should be well documented as part of the datatype registration process. Please see other datatypes defined in brain-life/datatypes . Brainlife datatype might sound similar to BIDS specification, but it differs in following areas. Brainlife datatypes mainly concern data derivatives generated by Apps and used only by Apps exchanging those datasets. They are only used within Brainlife platform and not meant to become standards for that particular data format. Brainlife datatypes are defined by App developers involved in exchanging input/output datasets, not by Brainlife platform developers. App developers should discuss and agree on the structure of the datatype and what each file means. They can submit an issue on brain-life/datatypes and/or a pull request containing the list of files/directories to be registered on Brainlife. Once Brainlife team incorporate the PR, you will be able to use the new datatype for your App. Datatype Tags Sometimes you want to be more specific about the type of dataset for a particular datatype. For example, neuro/anat/t1w could be ACPC aligned or not, neuro/dwi could be single-shell or multi-shell, etc. Brainlife allows you to adds specificity to each datatype through datatype tags . Warning Please don't confuse Datatype tag with Dataset tag . \"Dataset tag\" is a tag that user can freely edit under dataset dialog to allow for easier searching or bulk processing of datasets with specific tags. \"Datatype tag\", on the other hand, can only be set by App developer and it is a part of datatype and can not be modified once dataset is created. It is important to note that, datatype tag should always be used to add specificity to datasets, but not to generalize it. For example, we don't have \"multi-shell\" datatype tags because neuro/dwi is by default a \"multi-shell\" data; it is perfectly valid to have different b-values in dwi.bvals file. \"single-shell\" is a special case for neuro/dwi datatype where b-values happens to be all same number. Therefore, we introduce \"single-shell\" tag to describe such dwi dataset. By always using datatype tags to add specificity, Brainlife can correctly identify which datasets can be used for which Apps by examining dataset's datatype tags and App's input dataset tags. Hint Please consult the #datatype slack channel on Brainlife slack team for any datatype related questions.","title":"Datatypes"},{"location":"user/datatypes/#datatypes","text":"Please read Tutorial / Datatypes first. Brainlife Apps exchange data through datatypes . Each datatype consists of name , description , and a list of files (or directories) that define the overall structure of the datatype. For example, the following is a datatype definition for neuro/life datatype. { name : neuro/life , desc : LiFE Output (fe structure) , files : [ { id : fe , filename : output_fe.mat , desc : FE structure , ext : .mat , required : true }, { id : life_results , filename : life_results.json , required : true }, { id : tracts , dirname : tracts , required : true } ] } For this example datatype, brain-life/app-life App generates a dataset with this datatype, and other Apps that want to use neuro/life output can request to have those files made available to their Apps by registering them on Brainlife App registration form. (Please read Registering App page for more info). The actual content/semantics of each file are up to developers exchanging the dataset to decide, and it should be well documented as part of the datatype registration process. Please see other datatypes defined in brain-life/datatypes . Brainlife datatype might sound similar to BIDS specification, but it differs in following areas. Brainlife datatypes mainly concern data derivatives generated by Apps and used only by Apps exchanging those datasets. They are only used within Brainlife platform and not meant to become standards for that particular data format. Brainlife datatypes are defined by App developers involved in exchanging input/output datasets, not by Brainlife platform developers. App developers should discuss and agree on the structure of the datatype and what each file means. They can submit an issue on brain-life/datatypes and/or a pull request containing the list of files/directories to be registered on Brainlife. Once Brainlife team incorporate the PR, you will be able to use the new datatype for your App.","title":"Datatypes"},{"location":"user/datatypes/#datatype-tags","text":"Sometimes you want to be more specific about the type of dataset for a particular datatype. For example, neuro/anat/t1w could be ACPC aligned or not, neuro/dwi could be single-shell or multi-shell, etc. Brainlife allows you to adds specificity to each datatype through datatype tags . Warning Please don't confuse Datatype tag with Dataset tag . \"Dataset tag\" is a tag that user can freely edit under dataset dialog to allow for easier searching or bulk processing of datasets with specific tags. \"Datatype tag\", on the other hand, can only be set by App developer and it is a part of datatype and can not be modified once dataset is created. It is important to note that, datatype tag should always be used to add specificity to datasets, but not to generalize it. For example, we don't have \"multi-shell\" datatype tags because neuro/dwi is by default a \"multi-shell\" data; it is perfectly valid to have different b-values in dwi.bvals file. \"single-shell\" is a special case for neuro/dwi datatype where b-values happens to be all same number. Therefore, we introduce \"single-shell\" tag to describe such dwi dataset. By always using datatype tags to add specificity, Brainlife can correctly identify which datasets can be used for which Apps by examining dataset's datatype tags and App's input dataset tags. Hint Please consult the #datatype slack channel on Brainlife slack team for any datatype related questions.","title":"Datatype Tags"},{"location":"user/pipeline/","text":"Pipelines The Processes tab allows you to submit tasks one at a time. This is great if you are exploring different Apps or experimenting with different configurations that can best process your datasets. However, once you find the optimal set of Apps, you would probably want to run it across many subjects and submitting them one by one is simply not a good option. Brainlife allows you to setup a series of submission rules called pipeline rules . Instead of describing the entire workflow that you submit once (or re-submit if something fails), you will define a set of individual rules which will be continuously evaluated until you deactivate them. It is similar to how a factory assembly line produces products. When a subject fails to produce an output dataset for a specific rule, you can examine and handle it manually. Once you are able to produce a valid output, the rest of the pipeline rules will pick it up as if it came from the original rule. Setting up Pipeline Rule To setup a new pipeline rule, go to Project Pipelines tab and click a plus button at the bottom right corner of the page. Each rule will be responsible for submitting a specific App with a specific set of configuration. Enter Name field, and search for the App that you'd like to submit. Once you select an App, you will be able to set its configuration parameters. All Brainlife Apps have a defined list of input datatypes that Apps needs to run. Using this information, Brainlife will look for any subject that provides all input datatypes required by the App, and submit new process for each subject found to run your App. If you have more than one dataset that matches the required datatype for a subject, you can specify which datasets to use by specifying a dataset tags (not datatype tags). By default, it will use the latest dataset available for a given datatype. When you are submitting your first rule, you probably don't have any dataset archived inside your project. If you'd like to use datasets from other project, you can specify the Project field to look for the input datasets there. Above rule will submit processes for each subject found on ABIDE2 project that provides dwi datatype with a dataset tag of \"ABIDEII-BNI_1\". Brainlife will only submit new process if it hasn't submitted a new process for each subject. Brainlife also won't submit new process if your project already has an output datasets (maybe generated by other rules, or generated manually). To be more specific about which datasets are generated by which rule, you can specify output dataset tags under outputs section. You can leave this default if you know you there won't be any other App generating the same output datatype. We recommend to always set output dataset tags just in case. Lastly, you can set a Subject Filtering which limits the subjects that gets processed. Above example will make this rule to only submit for subjects with names that start with \"100\" or \"200\". When you are setting up your first rule, it's always good to limit number of subjects to make sure your rule is setup correctly. Hint There are number of regular expression tutorials available online. Also, please feel free to send us your question via Brainlife slack team. Monitoring Pipeline Rules Once you submit your pipeline rule, it should start submitting processes and you can monitor them under the processes tab. You can treat these processes as you normally do with any processes that you normally submit manually; examine outputs, stop, restart, etc.. The output datasets will be automatically archived once each task have completed successfully. Note If you remove a process or task, Brainlife will resubmit another process to handle that subject if the subject has all required input datasets and has not produce the output from the requested app yet. If you don't want them to be resubmitted, please remove or deactivate your rule. Troubleshooting Pipeline Rules Once you submit your pipeline rule, you can monitor the status of the pipeline under the Log section Information here should help you troubleshoot what Brainlife is doing with your rule, and most importantly, why it's not submitting proceses.","title":"Pipelines"},{"location":"user/pipeline/#pipelines","text":"The Processes tab allows you to submit tasks one at a time. This is great if you are exploring different Apps or experimenting with different configurations that can best process your datasets. However, once you find the optimal set of Apps, you would probably want to run it across many subjects and submitting them one by one is simply not a good option. Brainlife allows you to setup a series of submission rules called pipeline rules . Instead of describing the entire workflow that you submit once (or re-submit if something fails), you will define a set of individual rules which will be continuously evaluated until you deactivate them. It is similar to how a factory assembly line produces products. When a subject fails to produce an output dataset for a specific rule, you can examine and handle it manually. Once you are able to produce a valid output, the rest of the pipeline rules will pick it up as if it came from the original rule.","title":"Pipelines"},{"location":"user/pipeline/#setting-up-pipeline-rule","text":"To setup a new pipeline rule, go to Project Pipelines tab and click a plus button at the bottom right corner of the page. Each rule will be responsible for submitting a specific App with a specific set of configuration. Enter Name field, and search for the App that you'd like to submit. Once you select an App, you will be able to set its configuration parameters. All Brainlife Apps have a defined list of input datatypes that Apps needs to run. Using this information, Brainlife will look for any subject that provides all input datatypes required by the App, and submit new process for each subject found to run your App. If you have more than one dataset that matches the required datatype for a subject, you can specify which datasets to use by specifying a dataset tags (not datatype tags). By default, it will use the latest dataset available for a given datatype. When you are submitting your first rule, you probably don't have any dataset archived inside your project. If you'd like to use datasets from other project, you can specify the Project field to look for the input datasets there. Above rule will submit processes for each subject found on ABIDE2 project that provides dwi datatype with a dataset tag of \"ABIDEII-BNI_1\". Brainlife will only submit new process if it hasn't submitted a new process for each subject. Brainlife also won't submit new process if your project already has an output datasets (maybe generated by other rules, or generated manually). To be more specific about which datasets are generated by which rule, you can specify output dataset tags under outputs section. You can leave this default if you know you there won't be any other App generating the same output datatype. We recommend to always set output dataset tags just in case. Lastly, you can set a Subject Filtering which limits the subjects that gets processed. Above example will make this rule to only submit for subjects with names that start with \"100\" or \"200\". When you are setting up your first rule, it's always good to limit number of subjects to make sure your rule is setup correctly. Hint There are number of regular expression tutorials available online. Also, please feel free to send us your question via Brainlife slack team.","title":"Setting up Pipeline Rule"},{"location":"user/pipeline/#monitoring-pipeline-rules","text":"Once you submit your pipeline rule, it should start submitting processes and you can monitor them under the processes tab. You can treat these processes as you normally do with any processes that you normally submit manually; examine outputs, stop, restart, etc.. The output datasets will be automatically archived once each task have completed successfully. Note If you remove a process or task, Brainlife will resubmit another process to handle that subject if the subject has all required input datasets and has not produce the output from the requested app yet. If you don't want them to be resubmitted, please remove or deactivate your rule.","title":"Monitoring Pipeline Rules"},{"location":"user/pipeline/#troubleshooting-pipeline-rules","text":"Once you submit your pipeline rule, you can monitor the status of the pipeline under the Log section Information here should help you troubleshoot what Brainlife is doing with your rule, and most importantly, why it's not submitting proceses.","title":"Troubleshooting Pipeline Rules"},{"location":"user/process/","text":"Processes Please read Tutorial / Data Processing first. Under Project page, Processes tab is where you can perform data analysis on Brainlife. Each process is a logical grouping of various data analysis/processing tasks that share input and output datasets. Each task you submit will be assigned to various computing resources that you have access to and are currently available. Brainlife's task orchestration engine ( Amaretti ) takes care of data transfer and monitoring of your tasks. To begin processing of your data, first you need to stage the initial datasets. You can stage from any project that you have read access to and for any subject. However, it is best to create a separate Process for each subject, as it would make it easier to submit Apps by allowing Brainlife to auto-populate various input datasets. Monitoring Tasks Brainlife monitors task status on remote resources and relays the most recent log entries back to the UI. You can also see the entire content of the log by opening the Raw Output section of the task and selecting any log files you'd like to examine. Note Raw Output section will not be available for tasks that are not yet assigned to any resource. If you'd like to download files, instead of opening directly via the browser, you can click the download ( ) button to download individual files, or the entire directories. Task Status Brainlife task can have one of the following task statuses. Requested When you first submit your task, Brainlife will place them under the Requested state and wait for Brainlife to assign a resource to run on. If there are many other tasks being processed, it might take a while for it to be picked up, but it should not take more than a few minutes. Once the resource is assigned, Brainlife will transfer any dependent input datasets and setup a work directory on the resource. Running Once the task is ready to be executed, Brainlife will make a request to the local resource to start your task. Most resources have their own local batch scheduling systems, and your task will be placed in a queue where it waits for it to be actually executed on the system. Brainlife status might show that it's running even though it is actually just waiting in the remote queue. Finished The task has completed successfully. You can visualize output datasets using Brainlife's built in visualization tool by clicking buttons next to each output dataset. If you have requested to auto-archive the output datasets (at submit time), those datasets will be copied to the Project's datasets archive. Any dependent task will be placed in Requested status. Failed If the App terminates with non-0 exit code, the task will be marked as Failed. Please examine the output and determine the cause of the failure. Please help improving the App by contacting the App developer and/or submitting github issues. Removed Most resources use what is called a scratch space to stage the task's work directory. Normally scratch space has a time limit on how long the data files can be left on those systems (typically 30-90 days). When Brainlife detects that the task directory no longer exists on remote systems, it will mark those tasks as Removed . Note Brainlife will try to clean up old task directories in 25 days after the successful completion of the task to provide consistent behavior across various resources. This also reduces the disk space usage on various resources. If you have any output dataset that you'd like to keep, please archive ig or submit your task with the auto-archiving flag checked. If you archive your output, you will see a list of datasets archived from this output. Submitting Apps You can submit Apps in a couple of different ways. One way is to use the Submit New App button under each process, as you have done already. Brainlife selects Apps that you can currently submit based on available datasets within the process and required input datasets for each App. The more datasets you stage or generate, the more Apps you can submit. If you don't find an App that you are looking for, please head over to the App page and find the app you are trying to submit. You can either go back to the Process page and generate or stage required datasets, or you can execute the App directly from the App page by selecting the Execute tab under the App, which is our second way to submit an App. When you submit App via the Execute tab, Brainlife will create a new process under the selected project and stage all input files you have selected and submit your app in a single step. Tip If you are looking for just any sample dataset, you can try O3D project which contains a lot of common data derivatives.","title":"Processes"},{"location":"user/process/#processes","text":"Please read Tutorial / Data Processing first. Under Project page, Processes tab is where you can perform data analysis on Brainlife. Each process is a logical grouping of various data analysis/processing tasks that share input and output datasets. Each task you submit will be assigned to various computing resources that you have access to and are currently available. Brainlife's task orchestration engine ( Amaretti ) takes care of data transfer and monitoring of your tasks. To begin processing of your data, first you need to stage the initial datasets. You can stage from any project that you have read access to and for any subject. However, it is best to create a separate Process for each subject, as it would make it easier to submit Apps by allowing Brainlife to auto-populate various input datasets.","title":"Processes"},{"location":"user/process/#monitoring-tasks","text":"Brainlife monitors task status on remote resources and relays the most recent log entries back to the UI. You can also see the entire content of the log by opening the Raw Output section of the task and selecting any log files you'd like to examine. Note Raw Output section will not be available for tasks that are not yet assigned to any resource. If you'd like to download files, instead of opening directly via the browser, you can click the download ( ) button to download individual files, or the entire directories.","title":"Monitoring Tasks"},{"location":"user/process/#task-status","text":"Brainlife task can have one of the following task statuses. Requested When you first submit your task, Brainlife will place them under the Requested state and wait for Brainlife to assign a resource to run on. If there are many other tasks being processed, it might take a while for it to be picked up, but it should not take more than a few minutes. Once the resource is assigned, Brainlife will transfer any dependent input datasets and setup a work directory on the resource. Running Once the task is ready to be executed, Brainlife will make a request to the local resource to start your task. Most resources have their own local batch scheduling systems, and your task will be placed in a queue where it waits for it to be actually executed on the system. Brainlife status might show that it's running even though it is actually just waiting in the remote queue. Finished The task has completed successfully. You can visualize output datasets using Brainlife's built in visualization tool by clicking buttons next to each output dataset. If you have requested to auto-archive the output datasets (at submit time), those datasets will be copied to the Project's datasets archive. Any dependent task will be placed in Requested status. Failed If the App terminates with non-0 exit code, the task will be marked as Failed. Please examine the output and determine the cause of the failure. Please help improving the App by contacting the App developer and/or submitting github issues. Removed Most resources use what is called a scratch space to stage the task's work directory. Normally scratch space has a time limit on how long the data files can be left on those systems (typically 30-90 days). When Brainlife detects that the task directory no longer exists on remote systems, it will mark those tasks as Removed . Note Brainlife will try to clean up old task directories in 25 days after the successful completion of the task to provide consistent behavior across various resources. This also reduces the disk space usage on various resources. If you have any output dataset that you'd like to keep, please archive ig or submit your task with the auto-archiving flag checked. If you archive your output, you will see a list of datasets archived from this output.","title":"Task Status"},{"location":"user/process/#submitting-apps","text":"You can submit Apps in a couple of different ways. One way is to use the Submit New App button under each process, as you have done already. Brainlife selects Apps that you can currently submit based on available datasets within the process and required input datasets for each App. The more datasets you stage or generate, the more Apps you can submit. If you don't find an App that you are looking for, please head over to the App page and find the app you are trying to submit. You can either go back to the Process page and generate or stage required datasets, or you can execute the App directly from the App page by selecting the Execute tab under the App, which is our second way to submit an App. When you submit App via the Execute tab, Brainlife will create a new process under the selected project and stage all input files you have selected and submit your app in a single step. Tip If you are looking for just any sample dataset, you can try O3D project which contains a lot of common data derivatives.","title":"Submitting Apps"},{"location":"user/project/","text":"Project Project is where you can organize your datasets, do data processing, and share them with your project members. Each project can have a specific set of users for admins , members , or guests groups. Admin can update the project access policy and edit various groups for the project. Members have read/write access to the datasets but cannot make changes to the group members. Guests have read access to datasets and processes, but cannot modify them. Brainlife currently supports the following project access policies. Public Public project allows anyone registered on Brainlife (not just a member of the project) to download and use archived datasets to process data. All users can view all public projects on Brainlife under project list. Private Private project allows only the project members to download or use archived datasets to run Apps. Only the admins and project members can find the project under project list. Note List project summary for all users If you'd like to keep your project private while allowing other users to know its existance through the project list, please check this check box. You can solicit other users to join the project for full access or become guest users to have read access to the datasets.","title":"Projects"},{"location":"user/project/#project","text":"Project is where you can organize your datasets, do data processing, and share them with your project members. Each project can have a specific set of users for admins , members , or guests groups. Admin can update the project access policy and edit various groups for the project. Members have read/write access to the datasets but cannot make changes to the group members. Guests have read access to datasets and processes, but cannot modify them. Brainlife currently supports the following project access policies. Public Public project allows anyone registered on Brainlife (not just a member of the project) to download and use archived datasets to process data. All users can view all public projects on Brainlife under project list. Private Private project allows only the project members to download or use archived datasets to run Apps. Only the admins and project members can find the project under project list. Note List project summary for all users If you'd like to keep your project private while allowing other users to know its existance through the project list, please check this check box. You can solicit other users to join the project for full access or become guest users to have read access to the datasets.","title":"Project"},{"location":"user/publication/","text":"Publications Once you finish processing your datasets and produced all data derivatives, you can publish them by creating a current snapshots of your datasets and create a new publication page. A publication page can be accessed and datasets to be downloaded by anyone on the internet without having to login to Brainlife. Each publication page may include following information. A permanent-URL for the publication page. Unique DOI that redirects to the permanent-URL. Publication details such as authors, description, funders, data access license, etc.. Citation template that visitors can use to cite your publication. A list of datasets that you'd like to make it part of your publication. For each dataset listed, visitors can view dataset detail including provenance, App used to generate the dataset and a link to download the dataset. Danger Once you publish your datasets, those datasets will become \"world-readable\" even for a private project. Also, currently, you cannot change the list of published dataset. Please be careful which datasets to publish. Creating Publication Page To create a new publication page, go to a project where you want to publish your datasets, then open to the Publication tab. Click the Plus button at the bottom right corner of the screen. You should see a list of datatypes and the total files sizes like below. Select datatypes that you'd like to include in your publication. Click Next . Danger Brainlife currently does not allow you to make modification to the list of datasets included in your publication once published. Please examine carefully as you will not be able to change/un-publish datasets once they are made public. Next page allows you to enter title, description and various other metadata. You can edit these information after you publish. When you are done, click Submit . The creation of publication page might take a few minute depending on the number of datasets. Brainlife will issue a new DOI from Datacite at this point. Once you've successfully published your new publication, you should see it listed under the Publications tab under your project as well as the global Publication page which lists all currently registered publications from all projects.","title":"Publications"},{"location":"user/publication/#publications","text":"Once you finish processing your datasets and produced all data derivatives, you can publish them by creating a current snapshots of your datasets and create a new publication page. A publication page can be accessed and datasets to be downloaded by anyone on the internet without having to login to Brainlife. Each publication page may include following information. A permanent-URL for the publication page. Unique DOI that redirects to the permanent-URL. Publication details such as authors, description, funders, data access license, etc.. Citation template that visitors can use to cite your publication. A list of datasets that you'd like to make it part of your publication. For each dataset listed, visitors can view dataset detail including provenance, App used to generate the dataset and a link to download the dataset. Danger Once you publish your datasets, those datasets will become \"world-readable\" even for a private project. Also, currently, you cannot change the list of published dataset. Please be careful which datasets to publish.","title":"Publications"},{"location":"user/publication/#creating-publication-page","text":"To create a new publication page, go to a project where you want to publish your datasets, then open to the Publication tab. Click the Plus button at the bottom right corner of the screen. You should see a list of datatypes and the total files sizes like below. Select datatypes that you'd like to include in your publication. Click Next . Danger Brainlife currently does not allow you to make modification to the list of datasets included in your publication once published. Please examine carefully as you will not be able to change/un-publish datasets once they are made public. Next page allows you to enter title, description and various other metadata. You can edit these information after you publish. When you are done, click Submit . The creation of publication page might take a few minute depending on the number of datasets. Brainlife will issue a new DOI from Datacite at this point. Once you've successfully published your new publication, you should see it listed under the Publications tab under your project as well as the global Publication page which lists all currently registered publications from all projects.","title":"Creating Publication Page"},{"location":"user/tutorial/","text":"Tutorial This tutorial will guide you through the following functionality of Brainlife. Signing up Creating new projects and uploading datasets Launching visualizers to visualize your data Running processes on datasets and archiving results. Sign Up If you have not registered on Brainlife.io yet, please do so by visiting The Authentication Page and clicking on a preferred authentication method: Google, ORCID, Github, or through your institution. Warning If you register through a 3rd party authenticator, please use the same authenticator each time you login, or you will end up creating multiple Brainlife accounts. If you would like to setup a dedicated username/password for Brainlife, please click the \"Sign Up\" link. You will be asked to confirm your email address once you register. Note You can associate multiple authenticators to your account once you register by going to Settings / Account, Connected Accounts, and click \"Connect\" next to various 3rd party authenticators. Create Project Once you login, you will land on the Brainlife Apps page. Before we can start using Brainlife, you will need to create a new project. Click on Project button on the left hand side menu, then click a plus side button at the bottom of the project list. Note Project is where you can organize your datasets, do data processing, and share datasets with your project members. For more information about project, please read project page Enter any name and description , and leave everything else default. Click Submit . Congratulations! You just created your first private project! What is Dataset? Note Dataset (noun) a collection of related sets of information that is composed of separate elements but can be manipulated as a unit by a computer. dataset is a set of files/directories for a specific subject and modaility. It is the smallest set of data that you can interact with in Brainlife. For example, neuro/dwi dataset consists of files such as dwi.nii.gz (a file containing the actual 4D diffusion brain image data), dwi.bvecs , and dwi.bvals (which describe how the image was acquired by the MRI scanner). Or freesurfer output for a subject is considered to be a single \"dataset\" containing many directories and files. Brainlife processes data at each subject level, and for each dataset. Brainlife Datasets are immutable; you cannot modify the content of the dataset once you create it, although you can modify its metadata (description, tags, sidecard). Upload Dataset Now, let's upload some test datasets. Open the Datasets tab. Brainlife has 2 kinds of data storage. Datasets Archive The datasets tab you are seeing now shows the current content of your dataset archive. Datasets under this tab are stored in our object storage permanently (and some are backed up to our tape archives also). Process Scratch Space You cannot directly use archived datasets to run Apps. To run Apps, datasets will be automatically staged out of your archive and transferred to Brainlife's scratch space and on various compute resources where Apps are executed. Datasets on process scratch space will be automatically removed within 25 days or sooner. If you have any output datasets that you'd like to keep permanently, you will need to archive them back to the Datasets Archive. Now, click plus button at the bottom of the screen to open the dataset upload dialog. Select Datatype that you'd like to upload (currently limited to t1/t2 and dwi) and upload your dataset. Note If you don't have any data to upload, you can use datasets from Brainlife's various public projects. Please open any public project on Brainlife and go to \"Visualize Dataset\" section below. The Upload form will run a server side validation and data normalization service. You can check the results from this step. If everything looks good, click Archive . Once uploaded, you should see a new dialog showing details about your new datasets. All archived datasets are immutable (read-only), but you can copy them between projects or make changes to the metadata if necessary (description, tags, etc..). Note Stored in field shows where your dataset is archived. For a large dataset, it might take a while for it to be archived. Please give it a few minutes. Visualize Dataset To launch a visualization program, click a dataset record (not the check box which \"selects\" dataset), which opens a dataset detail modal. Then click on the visualizer icon ( ) at the top of dataset modal. Any datasets stored in Brainlife can be visualized using Web-based and/or Native (via Web-VNC ) visualization Apps registered for each datatype. For example, neuro/anat/t1 datasets can be visualized by the following set of Visualization Apps. Note Similar to Apps , developers can develop and contribute new visualization Apps to run on Brainlife. If you are developing visualization Apps, or have Apps that you'd like us to add, please contact us at brlife@iu.edu . Click any of the visualization Apps that you'd like to launch to visualize your data. Downloading BIDS You can search/select and bulk download datasets. On the dataset table, select the datasets you'd like to download by clicking on the check box, then click Download (BIDS) button. Brainlife will stage selected datasets, organize them into a BIDS structure , and let you download the whole structure as a single tar ball. Once it's ready, click Download . Note At the moment, all Brainlife datasets will simply be stored under /derivatives directory regardless of the datatype. Apps Before we proceed to Process tab, let's take a quick detour and visit the Apps page. The Apps page shows all Brainlife Apps that are publicly available that you can execute on resources and datasets that you have access to. Please take a look and see what type of Apps are currently available. You can click on each tile to see more details. On Brainlife, Apps are normally small programs that perform a specific data processing. Although we have a few Apps that behave more like a typical pipeline or workflow (including pre/post processing, data analysis, reporting, etc..), most Brainlife Apps should only do one thing, and one thing well. Datatypes Brainlife Apps exchange data through datatypes . Developers involved with interoperating input/output datasets should discuss and agree on the set of files/directory structure and their semantics, and register a new datatype by submitting an issue on datatypes github repo . Various colored boxes show the input and output datatypes. For example, the above image shows that this app will take dwi input dataset, and generate another dwi dataset with a datatype tag of \"masked\", and also output another dataset of a datatype mask . For more information on datatype, please visit datatypes page Data Processing Now that we know what Apps are, we can go back to your private project, to practice data processing. Open the Processes tab. You should see an empty page as you do not have any processes yet. On Brainlife, Process is where you can submit a group of tasks/Apps that can share input/output datasets. We will create a new process by clicking the + button at the right bottom corner of the page. Enter any name you would like for your process. You should see a screen that looks like this now. To process data, you first need to stage any dataset from our archive to your process. Each process can only process data that is either staged or generated by other Apps. Click Stage New Dataset button. On the Select Datasets dialog, please select NKI (Rockland Sample) project, and select any anat/t1w dataset. Click OK to stage. You should see a box showing \"Staging Datasets\" with selected datasets. While it is staging your data, please submit your first App. Click Submit New App button. A dialog should show up with a list of Apps that you can submit using your anat/t1w dataset. Brainlife allows you to select only the Apps where you have all required input datasets. Please run the ACPC alignment with ART App on your data. ACPC alignment is a common alignment tool used to re-orient/re-position the Brain image in common orientation suited for further image analysis. Find and click the App, then make sure that Brainlife has automatically selected your staged data as input. Leave other options default. Click Submit . Brainlife should now find the most appropriate resource to run this App, and transfer data to the resource and submit it to the local batch scheduler. Once started, a task should take a few minutes to run. Once completed, you should see a screen that looks like this. You can browse/download any output files as they are generated under Raw Output section. Once completed successfully, you can launch various visualization tools by clicking the button next to the Output section. Open Volume Viewer on both the original input data and the ACPC alignment output data and see how this algorithm has re-oriented your data. Below is the before/after view. Can you see that bottom one is better aligned/re-positioned at ACPC line? For more info on ACPC alignment see here Now that you have finished running ACPC alignment, you will be able to submit a few new Apps under Submit New App dialog that you couldn't submit before. Please feel free to submit other Apps or stage more datasets. Hint If you are not sure which datasets to stage, please see Apps page and find which datatype each App requires to run. Archiving So far, you have staged datasets, submitted an App that generated data derivatives, and visualized them. Now, it is important to note that all processes are meant to be temporary and Brainlife will remove processes within 25 days of data generation. If you would like to permanently keep the output datasets you just generated, you will need to archive them by clicking on button next to the Output section. You can edit any metadata and description, and click the Archive button to archive it. After you archive your data, open the Datasets tab and make sure that your dataset is listed there. You can click on the dataset record to see more details. Datasets As you submit more Apps and generate datasets from them, it becomes harder to keep up with how a given dataset was generated. Brainlife keeps track of a record of how a given dataset was generated all the way from the original input dataset (called data provenance ). Under Datasets tab, select any dataset you have generated and look under Provenance tab. The green boxes are the input datasets (uploaded to Brainlife from outside) and the white boxes are the Apps run to generate the data derivatives. You can pan/zoom the diagram, or re-layout some items by dragging/dropping. What's Next You should now be familiar with basic functionalities of Brainlife. Please take a look at other pages for more information. For example, if you'd like to write your app and register on Brainlife, please take a look at App Developer Guide . If you'd like to learn how to bulk process multiple subjects, please take a look at Pipeline . Please let us know how we can improve this tutorial, or send us pull requests with your edits.","title":"Tutorial"},{"location":"user/tutorial/#tutorial","text":"This tutorial will guide you through the following functionality of Brainlife. Signing up Creating new projects and uploading datasets Launching visualizers to visualize your data Running processes on datasets and archiving results.","title":"Tutorial"},{"location":"user/tutorial/#sign-up","text":"If you have not registered on Brainlife.io yet, please do so by visiting The Authentication Page and clicking on a preferred authentication method: Google, ORCID, Github, or through your institution. Warning If you register through a 3rd party authenticator, please use the same authenticator each time you login, or you will end up creating multiple Brainlife accounts. If you would like to setup a dedicated username/password for Brainlife, please click the \"Sign Up\" link. You will be asked to confirm your email address once you register. Note You can associate multiple authenticators to your account once you register by going to Settings / Account, Connected Accounts, and click \"Connect\" next to various 3rd party authenticators.","title":"Sign Up"},{"location":"user/tutorial/#create-project","text":"Once you login, you will land on the Brainlife Apps page. Before we can start using Brainlife, you will need to create a new project. Click on Project button on the left hand side menu, then click a plus side button at the bottom of the project list. Note Project is where you can organize your datasets, do data processing, and share datasets with your project members. For more information about project, please read project page Enter any name and description , and leave everything else default. Click Submit . Congratulations! You just created your first private project!","title":"Create Project"},{"location":"user/tutorial/#what-is-dataset","text":"Note Dataset (noun) a collection of related sets of information that is composed of separate elements but can be manipulated as a unit by a computer. dataset is a set of files/directories for a specific subject and modaility. It is the smallest set of data that you can interact with in Brainlife. For example, neuro/dwi dataset consists of files such as dwi.nii.gz (a file containing the actual 4D diffusion brain image data), dwi.bvecs , and dwi.bvals (which describe how the image was acquired by the MRI scanner). Or freesurfer output for a subject is considered to be a single \"dataset\" containing many directories and files. Brainlife processes data at each subject level, and for each dataset. Brainlife Datasets are immutable; you cannot modify the content of the dataset once you create it, although you can modify its metadata (description, tags, sidecard).","title":"What is Dataset?"},{"location":"user/tutorial/#upload-dataset","text":"Now, let's upload some test datasets. Open the Datasets tab. Brainlife has 2 kinds of data storage. Datasets Archive The datasets tab you are seeing now shows the current content of your dataset archive. Datasets under this tab are stored in our object storage permanently (and some are backed up to our tape archives also). Process Scratch Space You cannot directly use archived datasets to run Apps. To run Apps, datasets will be automatically staged out of your archive and transferred to Brainlife's scratch space and on various compute resources where Apps are executed. Datasets on process scratch space will be automatically removed within 25 days or sooner. If you have any output datasets that you'd like to keep permanently, you will need to archive them back to the Datasets Archive. Now, click plus button at the bottom of the screen to open the dataset upload dialog. Select Datatype that you'd like to upload (currently limited to t1/t2 and dwi) and upload your dataset. Note If you don't have any data to upload, you can use datasets from Brainlife's various public projects. Please open any public project on Brainlife and go to \"Visualize Dataset\" section below. The Upload form will run a server side validation and data normalization service. You can check the results from this step. If everything looks good, click Archive . Once uploaded, you should see a new dialog showing details about your new datasets. All archived datasets are immutable (read-only), but you can copy them between projects or make changes to the metadata if necessary (description, tags, etc..). Note Stored in field shows where your dataset is archived. For a large dataset, it might take a while for it to be archived. Please give it a few minutes.","title":"Upload Dataset"},{"location":"user/tutorial/#visualize-dataset","text":"To launch a visualization program, click a dataset record (not the check box which \"selects\" dataset), which opens a dataset detail modal. Then click on the visualizer icon ( ) at the top of dataset modal. Any datasets stored in Brainlife can be visualized using Web-based and/or Native (via Web-VNC ) visualization Apps registered for each datatype. For example, neuro/anat/t1 datasets can be visualized by the following set of Visualization Apps. Note Similar to Apps , developers can develop and contribute new visualization Apps to run on Brainlife. If you are developing visualization Apps, or have Apps that you'd like us to add, please contact us at brlife@iu.edu . Click any of the visualization Apps that you'd like to launch to visualize your data.","title":"Visualize Dataset"},{"location":"user/tutorial/#downloading-bids","text":"You can search/select and bulk download datasets. On the dataset table, select the datasets you'd like to download by clicking on the check box, then click Download (BIDS) button. Brainlife will stage selected datasets, organize them into a BIDS structure , and let you download the whole structure as a single tar ball. Once it's ready, click Download . Note At the moment, all Brainlife datasets will simply be stored under /derivatives directory regardless of the datatype.","title":"Downloading BIDS"},{"location":"user/tutorial/#apps","text":"Before we proceed to Process tab, let's take a quick detour and visit the Apps page. The Apps page shows all Brainlife Apps that are publicly available that you can execute on resources and datasets that you have access to. Please take a look and see what type of Apps are currently available. You can click on each tile to see more details. On Brainlife, Apps are normally small programs that perform a specific data processing. Although we have a few Apps that behave more like a typical pipeline or workflow (including pre/post processing, data analysis, reporting, etc..), most Brainlife Apps should only do one thing, and one thing well.","title":"Apps"},{"location":"user/tutorial/#datatypes","text":"Brainlife Apps exchange data through datatypes . Developers involved with interoperating input/output datasets should discuss and agree on the set of files/directory structure and their semantics, and register a new datatype by submitting an issue on datatypes github repo . Various colored boxes show the input and output datatypes. For example, the above image shows that this app will take dwi input dataset, and generate another dwi dataset with a datatype tag of \"masked\", and also output another dataset of a datatype mask . For more information on datatype, please visit datatypes page","title":"Datatypes"},{"location":"user/tutorial/#data-processing","text":"Now that we know what Apps are, we can go back to your private project, to practice data processing. Open the Processes tab. You should see an empty page as you do not have any processes yet. On Brainlife, Process is where you can submit a group of tasks/Apps that can share input/output datasets. We will create a new process by clicking the + button at the right bottom corner of the page. Enter any name you would like for your process. You should see a screen that looks like this now. To process data, you first need to stage any dataset from our archive to your process. Each process can only process data that is either staged or generated by other Apps. Click Stage New Dataset button. On the Select Datasets dialog, please select NKI (Rockland Sample) project, and select any anat/t1w dataset. Click OK to stage. You should see a box showing \"Staging Datasets\" with selected datasets. While it is staging your data, please submit your first App. Click Submit New App button. A dialog should show up with a list of Apps that you can submit using your anat/t1w dataset. Brainlife allows you to select only the Apps where you have all required input datasets. Please run the ACPC alignment with ART App on your data. ACPC alignment is a common alignment tool used to re-orient/re-position the Brain image in common orientation suited for further image analysis. Find and click the App, then make sure that Brainlife has automatically selected your staged data as input. Leave other options default. Click Submit . Brainlife should now find the most appropriate resource to run this App, and transfer data to the resource and submit it to the local batch scheduler. Once started, a task should take a few minutes to run. Once completed, you should see a screen that looks like this. You can browse/download any output files as they are generated under Raw Output section. Once completed successfully, you can launch various visualization tools by clicking the button next to the Output section. Open Volume Viewer on both the original input data and the ACPC alignment output data and see how this algorithm has re-oriented your data. Below is the before/after view. Can you see that bottom one is better aligned/re-positioned at ACPC line? For more info on ACPC alignment see here Now that you have finished running ACPC alignment, you will be able to submit a few new Apps under Submit New App dialog that you couldn't submit before. Please feel free to submit other Apps or stage more datasets. Hint If you are not sure which datasets to stage, please see Apps page and find which datatype each App requires to run.","title":"Data Processing"},{"location":"user/tutorial/#archiving","text":"So far, you have staged datasets, submitted an App that generated data derivatives, and visualized them. Now, it is important to note that all processes are meant to be temporary and Brainlife will remove processes within 25 days of data generation. If you would like to permanently keep the output datasets you just generated, you will need to archive them by clicking on button next to the Output section. You can edit any metadata and description, and click the Archive button to archive it. After you archive your data, open the Datasets tab and make sure that your dataset is listed there. You can click on the dataset record to see more details.","title":"Archiving"},{"location":"user/tutorial/#datasets","text":"As you submit more Apps and generate datasets from them, it becomes harder to keep up with how a given dataset was generated. Brainlife keeps track of a record of how a given dataset was generated all the way from the original input dataset (called data provenance ). Under Datasets tab, select any dataset you have generated and look under Provenance tab. The green boxes are the input datasets (uploaded to Brainlife from outside) and the white boxes are the Apps run to generate the data derivatives. You can pan/zoom the diagram, or re-layout some items by dragging/dropping.","title":"Datasets"},{"location":"user/tutorial/#whats-next","text":"You should now be familiar with basic functionalities of Brainlife. Please take a look at other pages for more information. For example, if you'd like to write your app and register on Brainlife, please take a look at App Developer Guide . If you'd like to learn how to bulk process multiple subjects, please take a look at Pipeline . Please let us know how we can improve this tutorial, or send us pull requests with your edits.","title":"What's Next"}]}