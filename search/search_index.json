{
    "docs": [
        {
            "location": "/", 
            "text": "Brainlife Documentation\n\n\nWelcome to the Brainlife documentation portal! This site contains up-to-date information about the Brainlife platform. The information is organized based in sections focusing on our different target communities. We envision three primary groups of Brainlife users:\n\n\nEnd Users.\n\n\nNeuroscientists, Psychologists and other investigators interested in using Brainlife to run Apps on public or private data to generate scientific insights and publish their results.\n\n\nApp Developers.\n\n\nComputational neuroscientists, computer scientists and statisticians interested in using Brainlife to share their analysis methods and algorithms by developing Apps and making them available on the platform.\n\n\nResource Providers.\n\n\nClusters managers, and cyberinfrastructure managers interested in providing a resource where Brainlife Apps and users can run.", 
            "title": "Home"
        }, 
        {
            "location": "/#brainlife-documentation", 
            "text": "Welcome to the Brainlife documentation portal! This site contains up-to-date information about the Brainlife platform. The information is organized based in sections focusing on our different target communities. We envision three primary groups of Brainlife users:", 
            "title": "Brainlife Documentation"
        }, 
        {
            "location": "/#end-users", 
            "text": "Neuroscientists, Psychologists and other investigators interested in using Brainlife to run Apps on public or private data to generate scientific insights and publish their results.", 
            "title": "End Users."
        }, 
        {
            "location": "/#app-developers", 
            "text": "Computational neuroscientists, computer scientists and statisticians interested in using Brainlife to share their analysis methods and algorithms by developing Apps and making them available on the platform.", 
            "title": "App Developers."
        }, 
        {
            "location": "/#resource-providers", 
            "text": "Clusters managers, and cyberinfrastructure managers interested in providing a resource where Brainlife Apps and users can run.", 
            "title": "Resource Providers."
        }, 
        {
            "location": "/about/", 
            "text": "What is Brainlife?\n\n\nA modern platform that uses both cloud and high-performance computing systems to support reproducible analyses, data management and visualization. Brainlife also provide unique mechanisms to publish all research assets associated with a scientific project (data and analyses) embedded in a cloud computing environment and referenced by a single digital-object-identifier (DOI). The platform is unique because of its focus on supporting scientific reproducibility beyond open code and open data, by providing fundamental smart mechanisms for what we refer to as \u201cOpen Services.\u201d \n\n\nBrainlife Apps\n\n\nBrainlife uses Apps to analyze data. \nApps\n are small programs, small modules or compute units, that can b made part of a larger series of steps in a full data analysis workflow for a publication. Brainlife Apps are meant to do a small but meaningful step in a longer analysis pipeline. Apps are modules and the platform allows users to develop, use, combine, and reuse Apps to simply build complex pipelines for customized brain data analyses. Most Apps indeed do only one thing, they process data in a specific way and are meant to perform a small set of operations and handle small sets of data; they do one thing, they do it well.\n\n\nApps can be developed and published on the Brainlife platform by anyone. App developers can be computational neuroscientists, cognitive neuroscientists, but also computer scientists or engineers. Apps are snippets of code implementing algorithms or analyses. By follow a few easy steps code to publish the code on the Brainlife platform as an App. Publishing code as an App allows scientists to use the data and computational resources available through Brainlife. Apps published on Brainlife can be used privately or shared publicly with the platform users community. \n\n\nBrainlife Datatypes\n\n\nBrainlife Apps communicate via Brainlife \u201cDatatypes.\u201d A Datatype defines expected list of file names / directory structure that Apps that uses that datatype to generate as output data. It can also be used as input dataset by other App that expects the same datatype. Datatypes allow multiple apps developed by independent developers to be joined together to form a \nworkflows\n. The Brainlife Datatypes are similar to BIDS in concept, except they are maintained by individual developers participating in a specific datatype, and they mainly concerns \ndata derivatives\n. Brainlife allows user to download datasets in BIDS structure.\n\n\nBrainlife Clouds (Compute Resource)\n\n\nBrainlife allows orchestration of data and computing across mix systems of Clouds and high-performance computing clusters (HPC). We refer to both HCP and Clouds as Brainlife Clouds, technically not precise terminology, but simple. Less is more. Brainlife orchestration allows users and compute resource providers to register a compute resource to make it available publicly to the full Brainlife users community or privately to a subset of users. Brainlife has smart mechanisms that allow Apps to run on different resources, privately or publicly. With a traditional approach of running an entire workflow on a small set of resources, some part of the workflow might not be optimal to run on a given resource, Brainlife allows App Developers to identify resources that best fit their Apps and score the compute resources available on the Brainlife platform depending on how well they work with the App they develop.\n\n\nBrainlife Viz (Cloud Visualization)\n\n\nBrainlife has mechanisms to run brain data visualization on Brainlife Clouds. Data visualization is meant to provide users with an agile way to get feedback on the quality of the results generatd by Apps and pipelines. Visualization is impleelemnted with smart cloud-side methods, so that data are not moved from the Clould to the users computer. THis increases seecurity and improves management. Brainlife Viz allows running major software for data visualization familiar to the neuroscience community (e.g., FreeView, FSLview, MRview), and running GPU rendered visualizations. Visualization Apps can be openly contributed by developers tot hee Brainlife platform.", 
            "title": "About"
        }, 
        {
            "location": "/about/#what-is-brainlife", 
            "text": "A modern platform that uses both cloud and high-performance computing systems to support reproducible analyses, data management and visualization. Brainlife also provide unique mechanisms to publish all research assets associated with a scientific project (data and analyses) embedded in a cloud computing environment and referenced by a single digital-object-identifier (DOI). The platform is unique because of its focus on supporting scientific reproducibility beyond open code and open data, by providing fundamental smart mechanisms for what we refer to as \u201cOpen Services.\u201d", 
            "title": "What is Brainlife?"
        }, 
        {
            "location": "/about/#brainlife-apps", 
            "text": "Brainlife uses Apps to analyze data.  Apps  are small programs, small modules or compute units, that can b made part of a larger series of steps in a full data analysis workflow for a publication. Brainlife Apps are meant to do a small but meaningful step in a longer analysis pipeline. Apps are modules and the platform allows users to develop, use, combine, and reuse Apps to simply build complex pipelines for customized brain data analyses. Most Apps indeed do only one thing, they process data in a specific way and are meant to perform a small set of operations and handle small sets of data; they do one thing, they do it well.  Apps can be developed and published on the Brainlife platform by anyone. App developers can be computational neuroscientists, cognitive neuroscientists, but also computer scientists or engineers. Apps are snippets of code implementing algorithms or analyses. By follow a few easy steps code to publish the code on the Brainlife platform as an App. Publishing code as an App allows scientists to use the data and computational resources available through Brainlife. Apps published on Brainlife can be used privately or shared publicly with the platform users community.", 
            "title": "Brainlife Apps"
        }, 
        {
            "location": "/about/#brainlife-datatypes", 
            "text": "Brainlife Apps communicate via Brainlife \u201cDatatypes.\u201d A Datatype defines expected list of file names / directory structure that Apps that uses that datatype to generate as output data. It can also be used as input dataset by other App that expects the same datatype. Datatypes allow multiple apps developed by independent developers to be joined together to form a  workflows . The Brainlife Datatypes are similar to BIDS in concept, except they are maintained by individual developers participating in a specific datatype, and they mainly concerns  data derivatives . Brainlife allows user to download datasets in BIDS structure.", 
            "title": "Brainlife Datatypes"
        }, 
        {
            "location": "/about/#brainlife-clouds-compute-resource", 
            "text": "Brainlife allows orchestration of data and computing across mix systems of Clouds and high-performance computing clusters (HPC). We refer to both HCP and Clouds as Brainlife Clouds, technically not precise terminology, but simple. Less is more. Brainlife orchestration allows users and compute resource providers to register a compute resource to make it available publicly to the full Brainlife users community or privately to a subset of users. Brainlife has smart mechanisms that allow Apps to run on different resources, privately or publicly. With a traditional approach of running an entire workflow on a small set of resources, some part of the workflow might not be optimal to run on a given resource, Brainlife allows App Developers to identify resources that best fit their Apps and score the compute resources available on the Brainlife platform depending on how well they work with the App they develop.", 
            "title": "Brainlife Clouds (Compute Resource)"
        }, 
        {
            "location": "/about/#brainlife-viz-cloud-visualization", 
            "text": "Brainlife has mechanisms to run brain data visualization on Brainlife Clouds. Data visualization is meant to provide users with an agile way to get feedback on the quality of the results generatd by Apps and pipelines. Visualization is impleelemnted with smart cloud-side methods, so that data are not moved from the Clould to the users computer. THis increases seecurity and improves management. Brainlife Viz allows running major software for data visualization familiar to the neuroscience community (e.g., FreeView, FSLview, MRview), and running GPU rendered visualizations. Visualization Apps can be openly contributed by developers tot hee Brainlife platform.", 
            "title": "Brainlife Viz (Cloud Visualization)"
        }, 
        {
            "location": "/user/tutorial/", 
            "text": "Tutorial\n\n\nThis tutorial will guide you through the following functionalities of Brainlife. \n\n\n\n\nSigning up\n\n\nCreating new projects and uploading datasets\n\n\nLaunching visualizers to visualize your data\n\n\nRunning processes on datasets and archiving results.\n\n\n\n\nFor a more high-level overview of Brainlife, see \nAbout Page\n.\n\n\nSign Up\n\n\nIf you have not registered on Brainlife.io yet, please do so by visiting The \nAuthentication Page\n and clicking on a preferred 3rd party authentication method: Google, ORCID, Github, or through your institution.\n\n\n\n\nWarning\n\n\nIf you register through the 3rd party authenticator, please use the same authenticator each time you login, or you will end up creating multiple Brainlife accounts. \n\n\n\n\nIf you'd like to setup a dedicated username/password for Brainlife, please click the \"Sign Up\" link. You will be asked to confirm your email address once you register.\n\n\n\n\nNote\n\n\nYou can associate multiple authenticators to your account once you register by going to Settings / Account, Connected Accounts, and click \"Connect\" next to various 3rd party authenticators. \n\n\n\n\nCreate Project\n\n\nOnce you login, you should land on Brainlife Apps page. \n\n\nBefore we can start using Brainlife, we need to create a new project. \n\n\nClick on \nProject\n button on the left hand side menu, then click a plus side button at the bottom of the project list.\n\n\n\n\n\n\nNote\n\n\nProject is where you can organize your datasets, do data processing, and share datasets with your project members. For more information about project, please read \nproject page\n\n\n\n\nEnter any \nname\n and \ndescription\n, and leave everything else default. Click \nSubmit\n. \n\n\nCongratulations! You just created your first private project!\n\n\nUpload Dataset\n\n\nNow, let's upload some test datasets. Open the \nDatasets\n tab.\n\n\n\n\nNote\n\n\nIn Brainlife, datasets are sets of files/directories for specific modality or data derivatives for a specific subject. All data processing is done at the subject level. Datasets are immutable; you can only modify the metadata, but not the data files once you create them.\n\n\n\n\nBrainlife has 2 kinds of data storage. \n\n\n\n\n\n\nDatasets Archive\n\n\nThe datasets tab you are seeing now shows the current content of your dataset archive. Datasets under this tab are stored in our object storage permanently (and some are backed up to our tape archives also).\n\n\n\n\n\n\nProcess Scratch Space\n\n\nYou cannot directly use archived datasets to run Apps. To run Apps, datasets will be automatically staged out of your archive and transferred to Brainlife's scratch space and on various compute resources where Apps are executed.\n\n\nDatasets on process scratch space will be automatically removed within 25 days or sooner. If you have any output datasets that you'd like to keep permanently, you will need to archive them back to the Datasets Archive. \n\n\n\n\n\n\nNow, click \nplus button\n at the bottom of the screen to open the dataset upload dialog.\n\n\n\n\nSelect Datatype that you'd like to upload (currently limited to t1/t2 and dwi) and upload your dataset. \n\n\n\n\nNote\n\n\nIf you don't have any data to upload, you can skip this step and you can use pre-uploaded datasets from various public projects.\n\n\n\n\nUpload form will run the server side validation/data normalization service. You can check the results from this step. If everything looks good, click \nArchive\n.\n\n\n\n\nOnce uploaded, you should see a new dialog showing details about your new datasets. All archived datasets are immutable (read-only), but you can make changes to the metadata if necessary (description, tags, etc..).\n\n\n\n\n\n\nNote\n\n\nStored in\n field shows where your dataset is archived. For a large dataset, it might take a while for it to be archived. Please give it a few minutes.\n\n\n\n\nVisualize Dataset\n\n\nAny datasets stored in Brainlife can be visualized using Web-based and Native (via Web-VNC) visualization Apps registered for specific datatypes. To launch a visualizer, click on the visualizer icon (\n) at the top of dataset dialog.\n\n\nFor example, \nneuro/anat/t1\n datasets can be visualized by the following set of Visualization Apps.\n\n\n\n\n\n\nNote\n\n\nSimilar to \nApps\n, developers can develop and contribute new visualization Apps to run on Brainlife. If you are developing visualization Apps, or have Apps that you'd like us to add, please contact us at \nbrlife@iu.edu\n.\n\n\n\n\nClick any of the visualization Apps that you'd like to launch to visualize your data. \n\n\nDownloading BIDS\n\n\nYou can search/select and bulk download datasets. On the dataset table, select the datasets you'd like to download by clicking on the check box, then click \nDownload (BIDS)\n button.\n\n\n\n\nBrainlife will stage selected datasets, organize them into a BIDS structure, and let you download the whole structure as a single tar ball. Once it's ready, click \nDownload\n.\n\n\n\n\nNote\n\n\nAt the moment, all Brainlife datasets will simply be stored under \n/derivatives\n directory regardless of the datatype.\n\n\n\n\nApps\n\n\nBefore we proceed to \nProcess\n tab, let's take a quick detour and visit the \nApps\n page.\n\n\n\n\nThe \nApps\n page shows all Brainlife Apps that are publicly available that you can execute on resources and datasets that you have access to. Please take a look and see what type of Apps are currently available. You can click on each tile to see more details.\n\n\nOn Brainlife, Apps are normally small programs that perform a specific data processing. Although we have a few Apps that behave more like a typical \npipeline\n or \nworkflow\n (including pre/post processing, data analysis, reporting, etc..), most Brainlife Apps should only do one thing, and one thing well. Please see \nabout\n for more details.\n\n\nDatatypes\n\n\nBrainlife Apps exchange data through \ndatatypes\n. Developers involved with interoperating input/output datasets should discuss and agree on the set of files/directory structure and their semantics, and register a new datatype by submitting an issue on \ndatatypes github repo\n.\n\n\n\n\nVarious colored boxes show the input and output datatypes. For example, the above image shows that this app will take \ndwi\n input dataset, and generate another \ndwi\n dataset with a datatype tag of \"masked\", and also output another dataset of a datatype \nmask\n. \n\n\nFor more information on datatype, please visit \ndatatypes page\n\n\n\n\nData Processing\n\n\nNow that we know what \nApps\n are, let's go back to your private project, then open \nProcesses\n tab. You should see an empty page as you don't have any processes yet. On Brainlife, \nProcess\n is where you can submit a group of tasks/Apps that can share input/output datasets. \n\n\nLet's create a new process by clicking the plus button at the right bottom corner of the page. Enter any name you'd like for your process. You should see a screen that looks like this now.\n\n\n\n\nTo process data, you first need to stage any dataset from our archive to your process. Each process can only process data that is either staged or generated by other Apps. Click \nStage New Dataset\n button. On the Select Datasets dialog, let's select \nNKI (Rockland Sample)\n project, and select any \nanat/t1w\n dataset.\n\n\n\n\nClick \nOK\n to stage. You should see a box showing \"Staging Datasets\" with selected datasets. While it's staging your data, let's submit our first App. Click \nSubmit New App\n button. A dialog should show up with a list of Apps that you can submit using your \nanat/t1w\n dataset. Brainlife allows you to select only the Apps where you have all required input datasets.\n\n\nLet's run \nACPC alignment with ART\n App on your data. \nACPC alignment\n is a common alignment tool used to re-orient/re-position the Brain image in common orientation suited for further image analysis. \n\n\nFind and click the App, then make sure that Brainlife has automatically selected your staged data as input. Leave other options default. Click \nSubmit\n. Brainlife should now find the most appropriate resource to run this App, and transfer data to the resource and submit it to the local batch scheduler.\n\n\nOnce started, a task should take a few minutes to run. Once completed, you should see a screen that looks like this.\n\n\n\n\nYou can browse/download any output files as they are generated under \nRaw Output\n section. Once completed successfully, you can launch various visualization tools by clicking the \n button next to the Output section. Open \nVolume Viewer\n on both the original input data and the ACPC alignment output data and see how this algorithm has re-oriented your data. \n\n\nBelow is the before/after view. Can you see that bottom one is better aligned/re-positioned at ACPC line? For more info on ACPC alignment see \nhere\n\n\n\n\nNow that you have finished running ACPC alignment, you will be able to submit a few new Apps under \nSubmit New App\n dialog that you couldn't submit before. Please feel free to submit other Apps or stage more datasets. \n\n\n\n\nHint\n\n\nIf you are not sure which datasets to stage, please see \nApps\n page and find which datatype each App requires to run.\n\n\n\n\nArchiving\n\n\nSo far, you have staged datasets, submitted an App that generated data derivatives, and visualized them.\n\n\nNow, it's important to note that all processes are meant to be temporary and Brainlife will remove processes within 25 days of data generation. If you'd like to permanently keep the output datasets you just generated, you will need to \narchive\n them by clicking on \n button next to the Output section. You can edit any metadata and description, and click the \nArchive\n button to archive it.\n\n\nAfter you archive your data, open the \nDatasets\n tab and make sure that your dataset is listed there. You can click on the dataset record to see more details.\n\n\nDatasets\n\n\nAs you submit more Apps and generate datasets from them, it becomes harder to keep up with how a given dataset was generated. Brainlife keeps track of a record of how a given dataset was generated all the way from the original input dataset (called \ndata provenance\n).\n\n\nUnder \nDatasets\n tab, select any dataset you have generated and look under \nProvenance\n tab.\n\n\n\n\nThe green boxes are the input datasets (uploaded to Brainlife from outside) and the white boxes are the Apps run to generate the data derivatives. You can pan/zoom the diagram, or re-layout some items by dragging/dropping.\n\n\nWhat's Next\n\n\nYou should now be familiar with basic functionalities of Brainlife. Please take a look at other pages for more information. For example, if you'd like to write your app and register on Brainlife, please take a look at \nApp Developer Guide\n. If you'd like to learn how to bulk process multiple subjects, please take a look at \nPipeline\n.\n\n\nPlease let us know how we can improve this tutorial, or send us pull requests with your edits.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/user/tutorial/#tutorial", 
            "text": "This tutorial will guide you through the following functionalities of Brainlife.    Signing up  Creating new projects and uploading datasets  Launching visualizers to visualize your data  Running processes on datasets and archiving results.   For a more high-level overview of Brainlife, see  About Page .", 
            "title": "Tutorial"
        }, 
        {
            "location": "/user/tutorial/#sign-up", 
            "text": "If you have not registered on Brainlife.io yet, please do so by visiting The  Authentication Page  and clicking on a preferred 3rd party authentication method: Google, ORCID, Github, or through your institution.   Warning  If you register through the 3rd party authenticator, please use the same authenticator each time you login, or you will end up creating multiple Brainlife accounts.    If you'd like to setup a dedicated username/password for Brainlife, please click the \"Sign Up\" link. You will be asked to confirm your email address once you register.   Note  You can associate multiple authenticators to your account once you register by going to Settings / Account, Connected Accounts, and click \"Connect\" next to various 3rd party authenticators.", 
            "title": "Sign Up"
        }, 
        {
            "location": "/user/tutorial/#create-project", 
            "text": "Once you login, you should land on Brainlife Apps page.   Before we can start using Brainlife, we need to create a new project.   Click on  Project  button on the left hand side menu, then click a plus side button at the bottom of the project list.    Note  Project is where you can organize your datasets, do data processing, and share datasets with your project members. For more information about project, please read  project page   Enter any  name  and  description , and leave everything else default. Click  Submit .   Congratulations! You just created your first private project!", 
            "title": "Create Project"
        }, 
        {
            "location": "/user/tutorial/#upload-dataset", 
            "text": "Now, let's upload some test datasets. Open the  Datasets  tab.   Note  In Brainlife, datasets are sets of files/directories for specific modality or data derivatives for a specific subject. All data processing is done at the subject level. Datasets are immutable; you can only modify the metadata, but not the data files once you create them.   Brainlife has 2 kinds of data storage.     Datasets Archive  The datasets tab you are seeing now shows the current content of your dataset archive. Datasets under this tab are stored in our object storage permanently (and some are backed up to our tape archives also).    Process Scratch Space  You cannot directly use archived datasets to run Apps. To run Apps, datasets will be automatically staged out of your archive and transferred to Brainlife's scratch space and on various compute resources where Apps are executed.  Datasets on process scratch space will be automatically removed within 25 days or sooner. If you have any output datasets that you'd like to keep permanently, you will need to archive them back to the Datasets Archive.     Now, click  plus button  at the bottom of the screen to open the dataset upload dialog.   Select Datatype that you'd like to upload (currently limited to t1/t2 and dwi) and upload your dataset.    Note  If you don't have any data to upload, you can skip this step and you can use pre-uploaded datasets from various public projects.   Upload form will run the server side validation/data normalization service. You can check the results from this step. If everything looks good, click  Archive .   Once uploaded, you should see a new dialog showing details about your new datasets. All archived datasets are immutable (read-only), but you can make changes to the metadata if necessary (description, tags, etc..).    Note  Stored in  field shows where your dataset is archived. For a large dataset, it might take a while for it to be archived. Please give it a few minutes.", 
            "title": "Upload Dataset"
        }, 
        {
            "location": "/user/tutorial/#visualize-dataset", 
            "text": "Any datasets stored in Brainlife can be visualized using Web-based and Native (via Web-VNC) visualization Apps registered for specific datatypes. To launch a visualizer, click on the visualizer icon ( ) at the top of dataset dialog.  For example,  neuro/anat/t1  datasets can be visualized by the following set of Visualization Apps.    Note  Similar to  Apps , developers can develop and contribute new visualization Apps to run on Brainlife. If you are developing visualization Apps, or have Apps that you'd like us to add, please contact us at  brlife@iu.edu .   Click any of the visualization Apps that you'd like to launch to visualize your data.", 
            "title": "Visualize Dataset"
        }, 
        {
            "location": "/user/tutorial/#downloading-bids", 
            "text": "You can search/select and bulk download datasets. On the dataset table, select the datasets you'd like to download by clicking on the check box, then click  Download (BIDS)  button.   Brainlife will stage selected datasets, organize them into a BIDS structure, and let you download the whole structure as a single tar ball. Once it's ready, click  Download .   Note  At the moment, all Brainlife datasets will simply be stored under  /derivatives  directory regardless of the datatype.", 
            "title": "Downloading BIDS"
        }, 
        {
            "location": "/user/tutorial/#apps", 
            "text": "Before we proceed to  Process  tab, let's take a quick detour and visit the  Apps  page.   The  Apps  page shows all Brainlife Apps that are publicly available that you can execute on resources and datasets that you have access to. Please take a look and see what type of Apps are currently available. You can click on each tile to see more details.  On Brainlife, Apps are normally small programs that perform a specific data processing. Although we have a few Apps that behave more like a typical  pipeline  or  workflow  (including pre/post processing, data analysis, reporting, etc..), most Brainlife Apps should only do one thing, and one thing well. Please see  about  for more details.", 
            "title": "Apps"
        }, 
        {
            "location": "/user/tutorial/#datatypes", 
            "text": "Brainlife Apps exchange data through  datatypes . Developers involved with interoperating input/output datasets should discuss and agree on the set of files/directory structure and their semantics, and register a new datatype by submitting an issue on  datatypes github repo .   Various colored boxes show the input and output datatypes. For example, the above image shows that this app will take  dwi  input dataset, and generate another  dwi  dataset with a datatype tag of \"masked\", and also output another dataset of a datatype  mask .   For more information on datatype, please visit  datatypes page", 
            "title": "Datatypes"
        }, 
        {
            "location": "/user/tutorial/#data-processing", 
            "text": "Now that we know what  Apps  are, let's go back to your private project, then open  Processes  tab. You should see an empty page as you don't have any processes yet. On Brainlife,  Process  is where you can submit a group of tasks/Apps that can share input/output datasets.   Let's create a new process by clicking the plus button at the right bottom corner of the page. Enter any name you'd like for your process. You should see a screen that looks like this now.   To process data, you first need to stage any dataset from our archive to your process. Each process can only process data that is either staged or generated by other Apps. Click  Stage New Dataset  button. On the Select Datasets dialog, let's select  NKI (Rockland Sample)  project, and select any  anat/t1w  dataset.   Click  OK  to stage. You should see a box showing \"Staging Datasets\" with selected datasets. While it's staging your data, let's submit our first App. Click  Submit New App  button. A dialog should show up with a list of Apps that you can submit using your  anat/t1w  dataset. Brainlife allows you to select only the Apps where you have all required input datasets.  Let's run  ACPC alignment with ART  App on your data.  ACPC alignment  is a common alignment tool used to re-orient/re-position the Brain image in common orientation suited for further image analysis.   Find and click the App, then make sure that Brainlife has automatically selected your staged data as input. Leave other options default. Click  Submit . Brainlife should now find the most appropriate resource to run this App, and transfer data to the resource and submit it to the local batch scheduler.  Once started, a task should take a few minutes to run. Once completed, you should see a screen that looks like this.   You can browse/download any output files as they are generated under  Raw Output  section. Once completed successfully, you can launch various visualization tools by clicking the   button next to the Output section. Open  Volume Viewer  on both the original input data and the ACPC alignment output data and see how this algorithm has re-oriented your data.   Below is the before/after view. Can you see that bottom one is better aligned/re-positioned at ACPC line? For more info on ACPC alignment see  here   Now that you have finished running ACPC alignment, you will be able to submit a few new Apps under  Submit New App  dialog that you couldn't submit before. Please feel free to submit other Apps or stage more datasets.    Hint  If you are not sure which datasets to stage, please see  Apps  page and find which datatype each App requires to run.", 
            "title": "Data Processing"
        }, 
        {
            "location": "/user/tutorial/#archiving", 
            "text": "So far, you have staged datasets, submitted an App that generated data derivatives, and visualized them.  Now, it's important to note that all processes are meant to be temporary and Brainlife will remove processes within 25 days of data generation. If you'd like to permanently keep the output datasets you just generated, you will need to  archive  them by clicking on   button next to the Output section. You can edit any metadata and description, and click the  Archive  button to archive it.  After you archive your data, open the  Datasets  tab and make sure that your dataset is listed there. You can click on the dataset record to see more details.", 
            "title": "Archiving"
        }, 
        {
            "location": "/user/tutorial/#datasets", 
            "text": "As you submit more Apps and generate datasets from them, it becomes harder to keep up with how a given dataset was generated. Brainlife keeps track of a record of how a given dataset was generated all the way from the original input dataset (called  data provenance ).  Under  Datasets  tab, select any dataset you have generated and look under  Provenance  tab.   The green boxes are the input datasets (uploaded to Brainlife from outside) and the white boxes are the Apps run to generate the data derivatives. You can pan/zoom the diagram, or re-layout some items by dragging/dropping.", 
            "title": "Datasets"
        }, 
        {
            "location": "/user/tutorial/#whats-next", 
            "text": "You should now be familiar with basic functionalities of Brainlife. Please take a look at other pages for more information. For example, if you'd like to write your app and register on Brainlife, please take a look at  App Developer Guide . If you'd like to learn how to bulk process multiple subjects, please take a look at  Pipeline .  Please let us know how we can improve this tutorial, or send us pull requests with your edits.", 
            "title": "What's Next"
        }, 
        {
            "location": "/user/project/", 
            "text": "Project\n\n\nProject\n is where you can organize your datasets, do data processing, and share them with your project members.\n\n\nEach project can have a specific set of users for \nadmins\n, \nmembers\n, or \nguests\n groups. \nAdmin\n can update the project access policy and edit various groups for the project. \nMembers\n have read/write access to the datasets but cannot make changes to the group members. \nGuests\n have read access to datasets and processes, but cannot modify them.\n\n\nBrainlife currently supports the following project access policies.\n\n\n\n\n\n\nPublic\n\n\nPublic project allows anyone registered on Brainlife (not just a member of the project) to download and use archived datasets to process data. All users can view all public projects on Brainlife under project list.\n\n\n\n\n\n\nPrivate\n\n\nPrivate project allows only the project members to download or use archived datasets to run Apps. Only the admins and project members can find the project under project list.\n\n\nList project summary for all users\n \n\n\nIf you'd like to keep your project private while allowing other users to know its existance through the project menu, please check this check box. You can solicit other users to join the project and/or become guest users to have read access to its datasets.\n\n\n\n\n\n\nThe following table shows who can perform which actions under a project.\n\n\n\n\n\n\n\n\nAction\n\n\nPublic Project\n\n\nPrivate Project\n\n\n\n\n\n\n\n\n\n\nUpdate project detail\n\n\nAdmin\n\n\nAdmin\n\n\n\n\n\n\nSee project info\n\n\nAdmin / Members\n\n\nAdmin / Members (If \nlisted\n, all users)\n\n\n\n\n\n\nList datasets\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nDownload datasets\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nUpdate dataset detail\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nCreate publication record\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nUpdate publication record\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nUpload datasets\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nList processes\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nSubmit new process\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nAccess process output\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nList published datasets\n\n\nAdmin / Members / Guest\n\n\nAdmin / Members / Guest\n\n\n\n\n\n\nList publication records\n\n\nAdmin / Members / Guest\n\n\nAdmin / Members / Guest\n\n\n\n\n\n\nDownload published datasets\n\n\nAdmin / Members / Guest\n\n\nAdmin / Members / Guest", 
            "title": "Projects"
        }, 
        {
            "location": "/user/project/#project", 
            "text": "Project  is where you can organize your datasets, do data processing, and share them with your project members.  Each project can have a specific set of users for  admins ,  members , or  guests  groups.  Admin  can update the project access policy and edit various groups for the project.  Members  have read/write access to the datasets but cannot make changes to the group members.  Guests  have read access to datasets and processes, but cannot modify them.  Brainlife currently supports the following project access policies.    Public  Public project allows anyone registered on Brainlife (not just a member of the project) to download and use archived datasets to process data. All users can view all public projects on Brainlife under project list.    Private  Private project allows only the project members to download or use archived datasets to run Apps. Only the admins and project members can find the project under project list.  List project summary for all users    If you'd like to keep your project private while allowing other users to know its existance through the project menu, please check this check box. You can solicit other users to join the project and/or become guest users to have read access to its datasets.    The following table shows who can perform which actions under a project.     Action  Public Project  Private Project      Update project detail  Admin  Admin    See project info  Admin / Members  Admin / Members (If  listed , all users)    List datasets  Admin / Members  Admin / Members    Download datasets  Admin / Members  Admin / Members    Update dataset detail  Admin / Members  Admin / Members    Create publication record  Admin / Members  Admin / Members    Update publication record  Admin / Members  Admin / Members    Upload datasets  Admin / Members  Admin / Members    List processes  Admin / Members  Admin / Members    Submit new process  Admin / Members  Admin / Members    Access process output  Admin / Members  Admin / Members    List published datasets  Admin / Members / Guest  Admin / Members / Guest    List publication records  Admin / Members / Guest  Admin / Members / Guest    Download published datasets  Admin / Members / Guest  Admin / Members / Guest", 
            "title": "Project"
        }, 
        {
            "location": "/user/datatypes/", 
            "text": "Datatypes\n\n\n\n\nPlease read \nTutorial / Datatypes\n first.\n\n\n\n\nBrainlife Apps exchange data through \ndatatypes\n.\n\n\n\n\nEach datatype consists of \nname\n, \ndescription\n, and a list of files (or directories) that define the overall structure of the datatype. \n\n\nFor example, the following is a datatype definition for \nneuro/life\n datatype.\n\n\n{\n\n    \nname\n \n:\n \nneuro/life\n,\n\n    \ndesc\n \n:\n \nLiFE Output (fe structure)\n,\n\n    \nfiles\n \n:\n \n[\n \n        \n{\n\n            \nid\n \n:\n \nfe\n,\n\n            \nfilename\n \n:\n \noutput_fe.mat\n,\n\n            \ndesc\n \n:\n \nFE structure\n,\n\n            \next\n \n:\n \n.mat\n,\n\n            \nrequired\n \n:\n \ntrue\n\n        \n},\n \n        \n{\n\n            \nid\n \n:\n \nlife_results\n,\n\n            \nfilename\n \n:\n \nlife_results.json\n,\n\n            \nrequired\n \n:\n \ntrue\n\n        \n},\n \n        \n{\n\n            \nid\n \n:\n \ntracts\n,\n\n            \ndirname\n \n:\n \ntracts\n,\n\n            \nrequired\n \n:\n \ntrue\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\nFor this example datatype, \nbrain-life/app-life\n App generates a dataset with this datatype, and other Apps that want to use \nneuro/life\n output can request to have those files made available to their Apps by registering them on Brainlife App registration form. (Please read \nRegistering App\n page for more info). The actual content/semantics of each file are up to developers exchanging the dataset to decide, and it should be well documented as part of the datatype registration process.\n\n\nPlease see other datatypes defined in \nbrain-life/datatypes\n.\n\n\nBrainlife datatype might sound similar to BIDS specification, but it differs in following areas.\n\n\n\n\n\n\nBrainlife datatypes mainly concern data derivatives generated by Apps and used only by Apps exchanging those datasets. They are only used within Brainlife platform and not meant to become standards for that particular data format.\n\n\n\n\n\n\nBrainlife datatypes are defined by App developers involved in exchanging input/output datasets, not by Brainlife platform developers. App developers should discuss and agree on the structure of the datatype and what each file means. They can submit an issue on \nbrain-life/datatypes\n and/or submit a pull request containing the list of files/directories to be registered on Brainlife.", 
            "title": "Datatypes"
        }, 
        {
            "location": "/user/datatypes/#datatypes", 
            "text": "Please read  Tutorial / Datatypes  first.   Brainlife Apps exchange data through  datatypes .   Each datatype consists of  name ,  description , and a list of files (or directories) that define the overall structure of the datatype.   For example, the following is a datatype definition for  neuro/life  datatype.  { \n     name   :   neuro/life , \n     desc   :   LiFE Output (fe structure) , \n     files   :   [  \n         { \n             id   :   fe , \n             filename   :   output_fe.mat , \n             desc   :   FE structure , \n             ext   :   .mat , \n             required   :   true \n         },  \n         { \n             id   :   life_results , \n             filename   :   life_results.json , \n             required   :   true \n         },  \n         { \n             id   :   tracts , \n             dirname   :   tracts , \n             required   :   true \n         } \n     ]  }   For this example datatype,  brain-life/app-life  App generates a dataset with this datatype, and other Apps that want to use  neuro/life  output can request to have those files made available to their Apps by registering them on Brainlife App registration form. (Please read  Registering App  page for more info). The actual content/semantics of each file are up to developers exchanging the dataset to decide, and it should be well documented as part of the datatype registration process.  Please see other datatypes defined in  brain-life/datatypes .  Brainlife datatype might sound similar to BIDS specification, but it differs in following areas.    Brainlife datatypes mainly concern data derivatives generated by Apps and used only by Apps exchanging those datasets. They are only used within Brainlife platform and not meant to become standards for that particular data format.    Brainlife datatypes are defined by App developers involved in exchanging input/output datasets, not by Brainlife platform developers. App developers should discuss and agree on the structure of the datatype and what each file means. They can submit an issue on  brain-life/datatypes  and/or submit a pull request containing the list of files/directories to be registered on Brainlife.", 
            "title": "Datatypes"
        }, 
        {
            "location": "/user/process/", 
            "text": "Processes\n\n\n\n\nPlease read \nTutorial / Data Processing\n first.\n\n\n\n\nUnder Project page, Processes tab is where you can perform data analysis on Brainlife.\n\n\n\n\nEach process is a logical grouping of various data analysis/processing tasks that share input and output datasets. Each task you submit will be assigned to various computing resources that you have access to and are currently available. Brainlife's task orchestration engine (\nAmaretti\n) takes care of data transfer and monitoring of your tasks.\n\n\nTo begin processing of your data, first you need to stage the initial datasets. You can stage from any project that you have read access to and for any subject. However, it is best to create a separate Process for each subject, as it would make it easier to submit Apps by allowing Brainlife to auto-populate various input datasets.\n\n\nMonitoring Tasks\n\n\nBrainlife monitors task status on remote resources and relays the most recent log entries back to the UI.\n\n\n\n\nYou can also see the entire content of the log by opening the \nRaw Output\n section of the task and selecting any log files you'd like to examine.\n\n\n\n\n\n\nNote\n\n\nRaw Output\n section will not be available for tasks that are not yet assigned to any resource.\n\n\n\n\nIf you'd like to download files, instead of opening directly via the browser, you can click the download (\n) button to download individual files, or the entire directories.\n\n\nTask Status\n\n\nBrainlife task can have one of the following task statuses.\n\n\n\n\n\n\nRequested\n\n\nWhen you first submit your task, Brainlife will place them under the Requested state and wait for Brainlife to assign a resource to run on. If there are many other tasks being processed, it might take a while for it to be picked up, but it should not take more than a few minutes. Once the resource is assigned, Brainlife will transfer any dependent input datasets and setup a \nwork directory\n on the resource.\n\n\n\n\n\n\nRunning\n\n\nOnce the task is ready to be executed, Brainlife will make a request to the local resource to start your task. Most resources have their own local batch scheduling systems, and your task will be placed in a queue where it waits for it to be actually executed on the system. Brainlife status might show that it's \nrunning\n even though it is actually just waiting in the remote queue.\n\n\n\n\n\n\nFinished\n\n\nThe task has completed successfully. You can visualize output datasets using Brainlife's built in visualization tool by clicking \n buttons next to each output dataset. If you have requested to auto-archive the output datasets (at submit time), those datasets will be copied to the Project's datasets archive. Any dependent task will be placed in Requested status.\n\n\n\n\n\n\nFailed\n\n\nIf the App terminates with non-0 exit code, the task will be marked as Failed. Please examine the output and determine the cause of the failure. Please help improving the App by contacting the App developer and/or submitting github issues.\n\n\n\n\n\n\nRemoved\n\n\nMost resources use what is called a \nscratch space\n to stage the task's work directory. Normally scratch space has a time limit on how long the data files can be left on those systems (typically 30-90 days). When Brainlife detects that the task directory no longer exists on remote systems, it will mark those tasks as \nRemoved\n.\n\n\n\n\nNote\n\n\nBrainlife will try to clean up old task directories in 25 days after the successful completion of the task to provide consistent behavior across various resources. This also reduces the disk space usage on various resources. If you have any output dataset that you'd like to keep, please archive ig or submit your task with the auto-archiving flag checked.\n\n\nIf you archive your output, you will see a list of datasets archived from this output.\n\n\n\n\n\n\n\n\n\n\nSubmitting Apps\n\n\nYou can submit Apps in a couple of different ways.\n\n\nOne way is to use the \nSubmit New App\n button under each process, as you have done already. Brainlife selects Apps that you can currently submit based on available datasets within the process and required input datasets for each App.\n\n\n\n\nThe more datasets you stage or generate, the more Apps you can submit. If you don't find an App that you are looking for, please head over to the App page and find the app you are trying to submit. You can either go back to the Process page and generate or stage required datasets, or you can execute the App directly from the App page by selecting the \nExecute\n tab under the App, which is our second way to submit an App.\n\n\n\n\nWhen you submit App via the Execute tab, Brainlife will create a new process under the selected project and stage all input files you have selected and submit your app in a single step. \n\n\n\n\nTip\n\n\nIf you are looking for just any sample dataset, you can try O3D project which contains a lot of common data derivatives.", 
            "title": "Processes"
        }, 
        {
            "location": "/user/process/#processes", 
            "text": "Please read  Tutorial / Data Processing  first.   Under Project page, Processes tab is where you can perform data analysis on Brainlife.   Each process is a logical grouping of various data analysis/processing tasks that share input and output datasets. Each task you submit will be assigned to various computing resources that you have access to and are currently available. Brainlife's task orchestration engine ( Amaretti ) takes care of data transfer and monitoring of your tasks.  To begin processing of your data, first you need to stage the initial datasets. You can stage from any project that you have read access to and for any subject. However, it is best to create a separate Process for each subject, as it would make it easier to submit Apps by allowing Brainlife to auto-populate various input datasets.", 
            "title": "Processes"
        }, 
        {
            "location": "/user/process/#monitoring-tasks", 
            "text": "Brainlife monitors task status on remote resources and relays the most recent log entries back to the UI.   You can also see the entire content of the log by opening the  Raw Output  section of the task and selecting any log files you'd like to examine.    Note  Raw Output  section will not be available for tasks that are not yet assigned to any resource.   If you'd like to download files, instead of opening directly via the browser, you can click the download ( ) button to download individual files, or the entire directories.", 
            "title": "Monitoring Tasks"
        }, 
        {
            "location": "/user/process/#task-status", 
            "text": "Brainlife task can have one of the following task statuses.    Requested  When you first submit your task, Brainlife will place them under the Requested state and wait for Brainlife to assign a resource to run on. If there are many other tasks being processed, it might take a while for it to be picked up, but it should not take more than a few minutes. Once the resource is assigned, Brainlife will transfer any dependent input datasets and setup a  work directory  on the resource.    Running  Once the task is ready to be executed, Brainlife will make a request to the local resource to start your task. Most resources have their own local batch scheduling systems, and your task will be placed in a queue where it waits for it to be actually executed on the system. Brainlife status might show that it's  running  even though it is actually just waiting in the remote queue.    Finished  The task has completed successfully. You can visualize output datasets using Brainlife's built in visualization tool by clicking   buttons next to each output dataset. If you have requested to auto-archive the output datasets (at submit time), those datasets will be copied to the Project's datasets archive. Any dependent task will be placed in Requested status.    Failed  If the App terminates with non-0 exit code, the task will be marked as Failed. Please examine the output and determine the cause of the failure. Please help improving the App by contacting the App developer and/or submitting github issues.    Removed  Most resources use what is called a  scratch space  to stage the task's work directory. Normally scratch space has a time limit on how long the data files can be left on those systems (typically 30-90 days). When Brainlife detects that the task directory no longer exists on remote systems, it will mark those tasks as  Removed .   Note  Brainlife will try to clean up old task directories in 25 days after the successful completion of the task to provide consistent behavior across various resources. This also reduces the disk space usage on various resources. If you have any output dataset that you'd like to keep, please archive ig or submit your task with the auto-archiving flag checked.  If you archive your output, you will see a list of datasets archived from this output.", 
            "title": "Task Status"
        }, 
        {
            "location": "/user/process/#submitting-apps", 
            "text": "You can submit Apps in a couple of different ways.  One way is to use the  Submit New App  button under each process, as you have done already. Brainlife selects Apps that you can currently submit based on available datasets within the process and required input datasets for each App.   The more datasets you stage or generate, the more Apps you can submit. If you don't find an App that you are looking for, please head over to the App page and find the app you are trying to submit. You can either go back to the Process page and generate or stage required datasets, or you can execute the App directly from the App page by selecting the  Execute  tab under the App, which is our second way to submit an App.   When you submit App via the Execute tab, Brainlife will create a new process under the selected project and stage all input files you have selected and submit your app in a single step.    Tip  If you are looking for just any sample dataset, you can try O3D project which contains a lot of common data derivatives.", 
            "title": "Submitting Apps"
        }, 
        {
            "location": "/user/pipeline/", 
            "text": "Pipelines\n\n\nThe \nProcesses\n tab allows you to submit tasks one at a time. This is great if you are exploring different Apps or experimenting with different configurations that can best process your datasets. However, once you find the optimal set of Apps, you would probably want to run it across many subjects and submitting them one by one is simply not a good option.\n\n\n\n\n\nBrainlife allows you to setup a series of submission rules called \npipeline rules\n. Instead of describing the entire workflow that you submit \nonce\n (or re-submit if something fails), you will define a set of individual rules which will be continuously evaluated until you deactivate them. It is similar to how a factory assembly line produces products. When a subject fails to produce an output dataset for a specific rule, you can examine and handle it manually. Once you are able to produce a valid output, the rest of the pipeline rules will pick it up as if it came from the original rule. \n\n\n\n\n\nSetting up Pipeline Rule\n\n\nTo setup a new pipeline rule, go to Project \n \nPipelines\n tab and click a plus button at the bottom right corner of the page.\n\n\nEach rule will be responsible for submitting a specific App with a specific set of configuration. Enter \nName\n field, and search for the App that you'd like to submit. Once you select an App, you will be able to set its configuration parameters.\n\n\n\n\nAll Brainlife Apps have a defined list of input datatypes that Apps needs to run. Using this information, Brainlife will look for any subject that provides all input datatypes required by the App, and submit new process for each subject found to run your App. If you have more than one dataset that matches the required datatype for a subject, you can specify which datasets to use by specifying a dataset tags (not datatype tags). By default, it will use the latest dataset available for a given datatype.\n\n\nWhen you are submitting your first rule, you probably don't have any dataset archived inside your project. If you'd like to use datasets from other project, you can specify the \nProject\n field to look for the input datasets there.\n\n\n\n\nAbove rule will submit processes for each subject found on ABIDE2 project that provides \ndwi\n datatype with a dataset tag of \"ABIDEII-BNI_1\".\n\n\nBrainlife will only submit new process if it hasn't submitted a new process for each subject. Brainlife also won't submit new process if your project already has an output datasets (maybe generated by other rules, or generated manually). To be more specific about which datasets are generated by which rule, you can specify output dataset tags under outputs section.\n\n\n\n\nYou can leave this default if you know you there won't be any other App generating the same output datatype. We recommend to always set output dataset tags just in case.\n\n\nLastly, you can set a \nSubject Filtering\n which limits the subjects that gets processed.\n\n\n\n\nAbove example will make this rule to only submit for subjects with names that start with \"100\" or \"200\". When you are setting up your first rule, it's always good to limit number of subjects to make sure your rule is setup correctly.\n\n\n\n\nHint\n\n\nThere are number of regular expression tutorials available online. Also, please feel free to send us your question via Brainlife slack team.\n\n\n\n\nMonitoring Pipeline Rules\n\n\nOnce you submit your pipeline rule, it might take 10 - 20 minutes before you start seeing new processes submitted by Brainlife's pipeline handler. \n\n\n\n\nNode\n\n\nBrainlife also limit number of running processes at around 50 processes for each rule so that any given rule won't consume all available computing resources.\n\n\n\n\n\n\nYou can treat these processes as you normally do with any other processes; examine outputs, stop, restart, etc.. The output datasets will be automatically archived once each task have completed successfully.\n\n\n\n\nNote\n\n\nIf you remove a process or task, Brainlife will resubmit another process to handle that subject if the subject has all required input datasets and has not produce the output from the requested app yet.", 
            "title": "Pipelines"
        }, 
        {
            "location": "/user/pipeline/#pipelines", 
            "text": "The  Processes  tab allows you to submit tasks one at a time. This is great if you are exploring different Apps or experimenting with different configurations that can best process your datasets. However, once you find the optimal set of Apps, you would probably want to run it across many subjects and submitting them one by one is simply not a good option.   Brainlife allows you to setup a series of submission rules called  pipeline rules . Instead of describing the entire workflow that you submit  once  (or re-submit if something fails), you will define a set of individual rules which will be continuously evaluated until you deactivate them. It is similar to how a factory assembly line produces products. When a subject fails to produce an output dataset for a specific rule, you can examine and handle it manually. Once you are able to produce a valid output, the rest of the pipeline rules will pick it up as if it came from the original rule.", 
            "title": "Pipelines"
        }, 
        {
            "location": "/user/pipeline/#setting-up-pipeline-rule", 
            "text": "To setup a new pipeline rule, go to Project    Pipelines  tab and click a plus button at the bottom right corner of the page.  Each rule will be responsible for submitting a specific App with a specific set of configuration. Enter  Name  field, and search for the App that you'd like to submit. Once you select an App, you will be able to set its configuration parameters.   All Brainlife Apps have a defined list of input datatypes that Apps needs to run. Using this information, Brainlife will look for any subject that provides all input datatypes required by the App, and submit new process for each subject found to run your App. If you have more than one dataset that matches the required datatype for a subject, you can specify which datasets to use by specifying a dataset tags (not datatype tags). By default, it will use the latest dataset available for a given datatype.  When you are submitting your first rule, you probably don't have any dataset archived inside your project. If you'd like to use datasets from other project, you can specify the  Project  field to look for the input datasets there.   Above rule will submit processes for each subject found on ABIDE2 project that provides  dwi  datatype with a dataset tag of \"ABIDEII-BNI_1\".  Brainlife will only submit new process if it hasn't submitted a new process for each subject. Brainlife also won't submit new process if your project already has an output datasets (maybe generated by other rules, or generated manually). To be more specific about which datasets are generated by which rule, you can specify output dataset tags under outputs section.   You can leave this default if you know you there won't be any other App generating the same output datatype. We recommend to always set output dataset tags just in case.  Lastly, you can set a  Subject Filtering  which limits the subjects that gets processed.   Above example will make this rule to only submit for subjects with names that start with \"100\" or \"200\". When you are setting up your first rule, it's always good to limit number of subjects to make sure your rule is setup correctly.   Hint  There are number of regular expression tutorials available online. Also, please feel free to send us your question via Brainlife slack team.", 
            "title": "Setting up Pipeline Rule"
        }, 
        {
            "location": "/user/pipeline/#monitoring-pipeline-rules", 
            "text": "Once you submit your pipeline rule, it might take 10 - 20 minutes before you start seeing new processes submitted by Brainlife's pipeline handler.    Node  Brainlife also limit number of running processes at around 50 processes for each rule so that any given rule won't consume all available computing resources.    You can treat these processes as you normally do with any other processes; examine outputs, stop, restart, etc.. The output datasets will be automatically archived once each task have completed successfully.   Note  If you remove a process or task, Brainlife will resubmit another process to handle that subject if the subject has all required input datasets and has not produce the output from the requested app yet.", 
            "title": "Monitoring Pipeline Rules"
        }, 
        {
            "location": "/user/publication/", 
            "text": "Publications\n\n\nOnce you finish processing your datasets and produced all data derivatives, you can publish them by creating a current snapshots of your datasets and create a new publication page.\n\n\n\n\nA publication page can be accessed and datasets to be downloaded by anyone on the internet without having to login to Brainlife.\n\n\nEach publication page may include following information.\n\n\n\n\nA permanent-URL for the publication page.\n\n\nUnique DOI that redirects to the permanent-URL.\n\n\nPublication details such as authors, description, funders, data access license, etc..\n\n\nCitation template that visitors can use to cite your publication.\n\n\nA list of datasets that you'd like to make it part of your publication. For each dataset listed, visitors can view dataset detail including provenance, App used to generate the dataset and a link to download the dataset.\n\n\n\n\n\n\nDanger\n\n\nOnce you publish your datasets, those datasets will become \"world-readable\" even for a private project. Also, currently, you cannot change the list of published dataset. Please be careful which datasets to publish.\n\n\n\n\nCreating Publication Page\n\n\nTo create a new publication page, go to a project where you want to publish your datasets, then open to the \nPublication\n tab. Click the \nPlus\n button at the bottom right corner of the screen. You should see a list of datatypes and the total files sizes like below.\n\n\n\n\nSelect datatypes that you'd like to include in your publication. Click \nNext\n.\n\n\n\n\nDanger\n\n\nBrainlife currently does not allow you to make modification to the list of datasets included in your publication once published. Please examine carefully as you will not be able to change/un-publish datasets once they are made public.\n\n\n\n\nNext page allows you to enter title, description and various other metadata. You can edit these information after you publish. When you are done, click \nSubmit\n. The creation of publication page might take a few minute depending on the number of datasets. Brainlife will issue a new DOI from Datacite at this point.\n\n\nOnce you've successfully published your new publication, you should see it listed under the Publications tab under your project as well as the global Publication page which lists all currently registered publications from all projects.", 
            "title": "Publication"
        }, 
        {
            "location": "/user/publication/#publications", 
            "text": "Once you finish processing your datasets and produced all data derivatives, you can publish them by creating a current snapshots of your datasets and create a new publication page.   A publication page can be accessed and datasets to be downloaded by anyone on the internet without having to login to Brainlife.  Each publication page may include following information.   A permanent-URL for the publication page.  Unique DOI that redirects to the permanent-URL.  Publication details such as authors, description, funders, data access license, etc..  Citation template that visitors can use to cite your publication.  A list of datasets that you'd like to make it part of your publication. For each dataset listed, visitors can view dataset detail including provenance, App used to generate the dataset and a link to download the dataset.    Danger  Once you publish your datasets, those datasets will become \"world-readable\" even for a private project. Also, currently, you cannot change the list of published dataset. Please be careful which datasets to publish.", 
            "title": "Publications"
        }, 
        {
            "location": "/user/publication/#creating-publication-page", 
            "text": "To create a new publication page, go to a project where you want to publish your datasets, then open to the  Publication  tab. Click the  Plus  button at the bottom right corner of the screen. You should see a list of datatypes and the total files sizes like below.   Select datatypes that you'd like to include in your publication. Click  Next .   Danger  Brainlife currently does not allow you to make modification to the list of datasets included in your publication once published. Please examine carefully as you will not be able to change/un-publish datasets once they are made public.   Next page allows you to enter title, description and various other metadata. You can edit these information after you publish. When you are done, click  Submit . The creation of publication page might take a few minute depending on the number of datasets. Brainlife will issue a new DOI from Datacite at this point.  Once you've successfully published your new publication, you should see it listed under the Publications tab under your project as well as the global Publication page which lists all currently registered publications from all projects.", 
            "title": "Creating Publication Page"
        }, 
        {
            "location": "/apps/introduction/", 
            "text": "Introduction\n\n\nWhat is \nApp\n?\n\n\nBrainlife Apps are ..\n\n\n\n\nHosted on a public github repo.\n\n\nHave an executable file with a filename of \nmain\n at the root of the git repo which. It should normally be a bash script that runs your algorithms. It could be written in any language, or compiled binaries, but you should package your app as a Docker container so you can execute it with \nsingularity exec\n) \n\n\nReceive all input parameters and paths for input files through \nconfig.json\n which is created by Brainlife at runtime on the current working directory of the compute resource that your App will run on. \n\n\nWrite all output files in the current directory in a structure expected by various Brainlife \ndatatype\n (more later).\n\n\n\n\nFor more technical specification, please read \nABCD specification\n\n\nApp Development Timeline\n\n\nYou would normally follow following steps to develop and register your App on Brainlife.\n\n\n\n\nDevelop an algorithm that runs on your laptop or local cluster with your test datasets.\n\n\nCreate a sample \nconfig.json\n but add it to \n.gitignore\n so that it won't be part of your repo. (You can include \nconfig.json.sample\n and symlink it to config.json)\n\n\nCreate \nmain\n that parses \nconfig.json\n and pass it to your algorithm.\n\n\nPublish it as public github repo.\n\n\nRegister your App on Brainlife. During this step, you can define what parameters and input file should be made available to your App via \nconfig.json\n.\n\n\nContact resource administrators and ask them to enable your App (more below). \n\n\n\n\nEnabling App on a compute resource\n\n\nApp needs to be enabled on each compute resources to run. Each user will have a different set of resources that they have access to, but Brainlife provides default \nshared\n resources for all users. If you want anyone in the Brainlife to be able to run your App, you can \ncontact the resource administrators\n of these default resources to enable your Apps.\n\n\nYou will need to discuss with resource administrators on how to handle any dependencies/libraries that your App might require. To make things easier and reproducible, you should consider Dockerizing you App's \ndependencies\n (but not the App itself) so that you can run your App through your container using \nsingularity\n from your \nmain\n. \n\n\n\n\nHint\n\n\nMost compute resources now provide singularity which increases the number of resource where you might be able to run your Apps.\n\n\n\n\nApp Launch Sequence\n\n\nBrainlife executes an App in following steps.\n\n\n\n\nA user requests to run your App through Brainlife.\n\n\nBrainlife queries a list of compute resources that user has access to and currently available to run your App. Brainlife then determines the best compute resource to run your App.\n\n\nBrainlife stages input datasets out of Brainlife's datasets archive, or transfer any dependent task's work directory that are required to run your App.\n\n\nBrainlife creates a new working directory by git cloning your App on (normally) a resource's scratch disk space, and place \nconfig.json\n containing user specified configuration parameters and various paths the input files.\n\n\nBrainlife then runs \nstart\n hook installed on each compute resources as part of \nabcd specification\n (App developer should have to worry about this under the most circumstances).\n\n\nOn a PBS cluster, \nstart\n hook then \nqsub\ns your \nmain\n script and place it on the local batch scheduler queue.\n\n\nLocal job scheduler runs your \nmain\n on a compute node and your App will execute your algorithm, and generate output on the working directory.\n\n\nBrainlife periodically monitors your job and relay information back to the user.\n\n\nOnce job is completed, user archives an output dataset if the result is valid.\n\n\n\n\nDatatype\n\n\nDifferent Brainlife Apps can exchange input/output datasets through Brainlife \ndatatypes\n which are developer defined file/directory structure that holds specific set of data.\n\n\nHere are some example of currently registered datatypes.\n\n\n\n\nneuro/anat/t1w (t1.nii.gz)\n\n\nneuro/anat/t2w (t2.nii.gz) \n\n\nneuro/dwi (diffusion data and bvecs/bvals)\n\n\nneuro/freesurfer (entire freesurfer output)\n\n\nneuro/tract (tract.tck containing fiber track data)\n\n\nneuro/dtiinit (dtiinit output - dti output directory)\n\n\ngeneric/images (a list of images)\n\n\nraw (unstructured data often used during development)\n\n\n\n\nYour App should read from one or more of these datatypes and write output data in a format specified by another datatype. By identifying existing datatypes that you can interoperate you can interface with datasets generated by other Apps and your output can be used by other Apps as their input.\n\n\nWe maintain a list of \ndatatypes\n in our \nbrain-life/datatypes\n repo. To create a new datatype, please open an issue, or submit a PR with a new datatype definition file (.json). We do not modify datatypes once it's published to preserve backward compatibility, but you can re-register new datatype under a different version.\n\n\n\n\nHint\n\n\nBrainlife app should follow the \nDo One Thing and Do It Well\n principle where a complex workflow should be split into several smaller Apps (but no more than necessary nor practical) to promote code-reuse and help parallelize your workflow and run each App on the most appropriate compute resource available.\n\n\n\n\n\n\nHint\n\n\nBefore writing your apps, please browse \ncurrently registered Brainlife Apps\n and datatypes under Brainlife.io to make sure you are not reinventing Apps. If you find an App that is similar to what you need, please contact the developer of the App and discuss if the feature you need can be added to the App.", 
            "title": "Introduction"
        }, 
        {
            "location": "/apps/introduction/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/apps/introduction/#what-is-app", 
            "text": "Brainlife Apps are ..   Hosted on a public github repo.  Have an executable file with a filename of  main  at the root of the git repo which. It should normally be a bash script that runs your algorithms. It could be written in any language, or compiled binaries, but you should package your app as a Docker container so you can execute it with  singularity exec )   Receive all input parameters and paths for input files through  config.json  which is created by Brainlife at runtime on the current working directory of the compute resource that your App will run on.   Write all output files in the current directory in a structure expected by various Brainlife  datatype  (more later).   For more technical specification, please read  ABCD specification", 
            "title": "What is App?"
        }, 
        {
            "location": "/apps/introduction/#app-development-timeline", 
            "text": "You would normally follow following steps to develop and register your App on Brainlife.   Develop an algorithm that runs on your laptop or local cluster with your test datasets.  Create a sample  config.json  but add it to  .gitignore  so that it won't be part of your repo. (You can include  config.json.sample  and symlink it to config.json)  Create  main  that parses  config.json  and pass it to your algorithm.  Publish it as public github repo.  Register your App on Brainlife. During this step, you can define what parameters and input file should be made available to your App via  config.json .  Contact resource administrators and ask them to enable your App (more below).", 
            "title": "App Development Timeline"
        }, 
        {
            "location": "/apps/introduction/#enabling-app-on-a-compute-resource", 
            "text": "App needs to be enabled on each compute resources to run. Each user will have a different set of resources that they have access to, but Brainlife provides default  shared  resources for all users. If you want anyone in the Brainlife to be able to run your App, you can  contact the resource administrators  of these default resources to enable your Apps.  You will need to discuss with resource administrators on how to handle any dependencies/libraries that your App might require. To make things easier and reproducible, you should consider Dockerizing you App's  dependencies  (but not the App itself) so that you can run your App through your container using  singularity  from your  main .    Hint  Most compute resources now provide singularity which increases the number of resource where you might be able to run your Apps.", 
            "title": "Enabling App on a compute resource"
        }, 
        {
            "location": "/apps/introduction/#app-launch-sequence", 
            "text": "Brainlife executes an App in following steps.   A user requests to run your App through Brainlife.  Brainlife queries a list of compute resources that user has access to and currently available to run your App. Brainlife then determines the best compute resource to run your App.  Brainlife stages input datasets out of Brainlife's datasets archive, or transfer any dependent task's work directory that are required to run your App.  Brainlife creates a new working directory by git cloning your App on (normally) a resource's scratch disk space, and place  config.json  containing user specified configuration parameters and various paths the input files.  Brainlife then runs  start  hook installed on each compute resources as part of  abcd specification  (App developer should have to worry about this under the most circumstances).  On a PBS cluster,  start  hook then  qsub s your  main  script and place it on the local batch scheduler queue.  Local job scheduler runs your  main  on a compute node and your App will execute your algorithm, and generate output on the working directory.  Brainlife periodically monitors your job and relay information back to the user.  Once job is completed, user archives an output dataset if the result is valid.", 
            "title": "App Launch Sequence"
        }, 
        {
            "location": "/apps/introduction/#datatype", 
            "text": "Different Brainlife Apps can exchange input/output datasets through Brainlife  datatypes  which are developer defined file/directory structure that holds specific set of data.  Here are some example of currently registered datatypes.   neuro/anat/t1w (t1.nii.gz)  neuro/anat/t2w (t2.nii.gz)   neuro/dwi (diffusion data and bvecs/bvals)  neuro/freesurfer (entire freesurfer output)  neuro/tract (tract.tck containing fiber track data)  neuro/dtiinit (dtiinit output - dti output directory)  generic/images (a list of images)  raw (unstructured data often used during development)   Your App should read from one or more of these datatypes and write output data in a format specified by another datatype. By identifying existing datatypes that you can interoperate you can interface with datasets generated by other Apps and your output can be used by other Apps as their input.  We maintain a list of  datatypes  in our  brain-life/datatypes  repo. To create a new datatype, please open an issue, or submit a PR with a new datatype definition file (.json). We do not modify datatypes once it's published to preserve backward compatibility, but you can re-register new datatype under a different version.   Hint  Brainlife app should follow the  Do One Thing and Do It Well  principle where a complex workflow should be split into several smaller Apps (but no more than necessary nor practical) to promote code-reuse and help parallelize your workflow and run each App on the most appropriate compute resource available.    Hint  Before writing your apps, please browse  currently registered Brainlife Apps  and datatypes under Brainlife.io to make sure you are not reinventing Apps. If you find an App that is similar to what you need, please contact the developer of the App and discuss if the feature you need can be added to the App.", 
            "title": "Datatype"
        }, 
        {
            "location": "/apps/helloworld/", 
            "text": "Please read \nApp Developers / Introduction\n first. \n\n\n\n\nHelloWorld\n\n\nHere, we will create a \"HelloWorld\" Brainlife App. \n\n\nLet's begin by creating a brand new \ngithub repository\n. Please be sure to make the repo public so that Brainlife can access it. You can name it whatever you like (like \"app-helloworld\").\n\n\nGit clone your new repository to wherever you will be developing/editing and testing your App.\n\n\ngit clone git@github.com:username/app-helloworld.git\n\n\n\n\n\nNow, create a file called \nmain\n.\n\n\nmain\n\n\n#!/bin/bash\n\n\n\n#PBS -l nodes=1:ppn=1\n\n\n#PBS -l walltime=00:05:00\n\n\n\n#parse config.json for input parameters (here, we are pulling \nt1\n)\n\n\nt1\n=\n$(\njq -r .t1 config.json\n)\n\n./main.py \n$t1\n\n\n\n\n\n\nPlease be sure to set the executable bit.\n\n\nchmod +x main\n\n\n\n\n\n\n\nHint\n\n\njq\n is a command line tool used to parse a small JSON file and pull values out of it. You can install it on your machine by running something like \napt-get install jq\n or \nyum install jq\n depending on your OS/distribution. All Brainlife resources should have common binaries installed including \nbash\n, \njq\n, and \nsingularity\n.\n\n\n\n\nThe first few lines in our \nmain\n instructs PBS or Slurm batch systems to request a certain number of nodes/processes to our App. \n\n\n#PBS -l nodes=1:ppn=1\n\n\n#PBS -l walltime=00:05:00\n\n\n\n\n\n\nYou will receive all input parameters from Brainlife through a JSON file named \nconfig.json\n which is created by Brainlife when your App is executed. See [Example config.json](https://github.com/brain-life/app-dtiinit/blob/master/config.json.sample. As an App developer, you will define what parameters needs to be entered by the user and input datasets later when you register your App on Brainlife.\n\n\nFollowing lines parses the \nconfig.json\n using \njq\n and the value of \nt1\n to the main part of the application which we will create later.\n\n\n#parse config.json for input parameters\n\n\nt1\n=\n$(\njq -r .t1 config.json\n)\n\n./main.py \n$t1\n\n\n\n\n\n\nTo be able to test your application, let's create a test \nconfig.json\n.\n\n\nconfig.json\n\n\n{\n\n   \nt1\n:\n \n/somewhere/t1.nii.gz\n\n\n}\n\n\n\n\n\n\nPlease update the path to wherever you have your test \nanat/t1w\n input file. If you don't have any, you can download one from \nBrainlife/O3D\n publication page. Just click the Datasets tab, and select any \nanat/t1w\n data to download.\n\n\nYou should add \nconfig.json\n to .gitignore as \nconfig.json\n is created at runtime by Brainlife, and we just need this now to test your app. \n\n\n\n\nHint\n\n\nA good pattern might be to create a file called \nconfig.json.sample\n used to test your App, and create a symlink \nln -s config.json config.json.sample\n so that you can run your app using \nconfig.json.sample\n without including the actual \nconfig.json\n as part of your repo. This allows other users to construct their own \nconfig.json\n if they want to run your app via command line.\n\n\n\n\n\n\nNote\n\n\nInstead of parsing \nconfig.json\n inside \nmain\n, you could use other parsing library as part of your algorithm itself, like Python's \nimport json\n, or Matlab's \njsonlab\n module inside the actual program that \nmain\n will be executing.\n\n\n\n\nOur \nmain\n script runs a python script called \nmain.py\n so let's create it.\n\n\nmain.py\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n#!/usr/bin/env python\n\n\n\nimport\n \nsys\n\n\nimport\n \nnibabel\n \nas\n \nnib\n\n\n\n#just dump input image header to output.txt\n\n\nimg\n=\nnib\n.\nload\n(\nsys\n.\nargv\n[\n1\n])\n\n\nf\n=\nopen\n(\noutput.txt\n,\n \nw\n)\n\n\nf\n.\nwrite\n(\nstr\n(\nimg\n.\nheader\n))\n\n\nf\n.\nclose\n()\n\n\n\n\n\n\n\nAgain, be sure to set the executable bit.\n\n\nchmomd +x main.py\n\n\n\n\n\nAny output files from your app should be written to the current working directory and in a file structure that complies with whichever the datatype of your dataset is. For now, we are not going to worry about the output datatype (assuming we will use \nraw\n)\n\n\nPlease be sure to add any output files from your app to .gitignore so that it won't be part of your git repo. \n\n\n.gitignore\n\n\nconfig.json\noutput.txt\n\n\n\n\n\nTesting\n\n\nNow, you should be able to test run your app locally by executing \nmain\n\n\n./main\n\n\n\n\n\n\n\n\nNow, it should generate an output file called \noutput.txt\n containing the dump of all nifti headers.\n\n\nclass \nnibabel.nifti1.Nifti1Header\n object, endian=\n\nsizeof_hdr      : 348\ndata_type       : \ndb_name         : \nextents         : 0\nsession_error   : 0\nregular         : r\ndim_info        : 0\ndim             : [  3 260 311 260   1   1   1   1]\n...\n...\n...\nqoffset_x       : 90.0\nqoffset_y       : -126.0\nqoffset_z       : -72.0\nsrow_x          : [ -0.69999999   0.           0.          90.        ]\nsrow_y          : [   0.            0.69999999    0.         -126.        ]\nsrow_z          : [  0.           0.           0.69999999 -72.        ]\nintent_name     : \nmagic           : n+1\n\n\n\n\n\nPushing to Github\n\n\nIf everything looks good, push our files to the Github.\n\n\ngit add .\ngit commit -m\ncreated my first BL App!\n\ngit push\n\n\n\n\n\nCongratulations! We have just created our first Brainlife App. To summarize, we've done following.\n\n\n\n\nCreated a new public Github repo.\n\n\nCreated \nmain\n which parses \nconfig.json\n and runs our App.\n\n\nCreated a test \nconfig.json\n.\n\n\nCreated \nmain.py\n which runs our algorithm and generate output files.\n\n\nTested the App, and pushed all files to Github.\n\n\n\n\n\n\nInfo\n\n\nYou can see more concrete examples of Brainlife apps at \nBrainlife hosted apps\n.\n\n\n\n\nTo run your App through Brainlife, you will need to do following.\n\n\n\n\n\n\nRegister your App on Brainlife.\n\n\n\n\n\n\nEnable your App on at least one Brainlife compute resource. \n\n\nFor now, please email \nmailto:brlife@iu.edu\n to enable your App on our shared test resource.", 
            "title": "HelloWorld"
        }, 
        {
            "location": "/apps/helloworld/#helloworld", 
            "text": "Here, we will create a \"HelloWorld\" Brainlife App.   Let's begin by creating a brand new  github repository . Please be sure to make the repo public so that Brainlife can access it. You can name it whatever you like (like \"app-helloworld\").  Git clone your new repository to wherever you will be developing/editing and testing your App.  git clone git@github.com:username/app-helloworld.git  Now, create a file called  main .", 
            "title": "HelloWorld"
        }, 
        {
            "location": "/apps/helloworld/#main", 
            "text": "#!/bin/bash  #PBS -l nodes=1:ppn=1  #PBS -l walltime=00:05:00  #parse config.json for input parameters (here, we are pulling  t1 )  t1 = $( jq -r .t1 config.json ) \n./main.py  $t1   Please be sure to set the executable bit.  chmod +x main   Hint  jq  is a command line tool used to parse a small JSON file and pull values out of it. You can install it on your machine by running something like  apt-get install jq  or  yum install jq  depending on your OS/distribution. All Brainlife resources should have common binaries installed including  bash ,  jq , and  singularity .   The first few lines in our  main  instructs PBS or Slurm batch systems to request a certain number of nodes/processes to our App.   #PBS -l nodes=1:ppn=1  #PBS -l walltime=00:05:00   You will receive all input parameters from Brainlife through a JSON file named  config.json  which is created by Brainlife when your App is executed. See [Example config.json](https://github.com/brain-life/app-dtiinit/blob/master/config.json.sample. As an App developer, you will define what parameters needs to be entered by the user and input datasets later when you register your App on Brainlife.  Following lines parses the  config.json  using  jq  and the value of  t1  to the main part of the application which we will create later.  #parse config.json for input parameters  t1 = $( jq -r .t1 config.json ) \n./main.py  $t1   To be able to test your application, let's create a test  config.json .", 
            "title": "main"
        }, 
        {
            "location": "/apps/helloworld/#configjson", 
            "text": "{ \n    t1 :   /somewhere/t1.nii.gz  }   Please update the path to wherever you have your test  anat/t1w  input file. If you don't have any, you can download one from  Brainlife/O3D  publication page. Just click the Datasets tab, and select any  anat/t1w  data to download.  You should add  config.json  to .gitignore as  config.json  is created at runtime by Brainlife, and we just need this now to test your app.    Hint  A good pattern might be to create a file called  config.json.sample  used to test your App, and create a symlink  ln -s config.json config.json.sample  so that you can run your app using  config.json.sample  without including the actual  config.json  as part of your repo. This allows other users to construct their own  config.json  if they want to run your app via command line.    Note  Instead of parsing  config.json  inside  main , you could use other parsing library as part of your algorithm itself, like Python's  import json , or Matlab's  jsonlab  module inside the actual program that  main  will be executing.   Our  main  script runs a python script called  main.py  so let's create it.", 
            "title": "config.json"
        }, 
        {
            "location": "/apps/helloworld/#mainpy", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 #!/usr/bin/env python  import   sys  import   nibabel   as   nib  #just dump input image header to output.txt  img = nib . load ( sys . argv [ 1 ])  f = open ( output.txt ,   w )  f . write ( str ( img . header ))  f . close ()    Again, be sure to set the executable bit.  chmomd +x main.py  Any output files from your app should be written to the current working directory and in a file structure that complies with whichever the datatype of your dataset is. For now, we are not going to worry about the output datatype (assuming we will use  raw )  Please be sure to add any output files from your app to .gitignore so that it won't be part of your git repo.", 
            "title": "main.py"
        }, 
        {
            "location": "/apps/helloworld/#gitignore", 
            "text": "config.json\noutput.txt", 
            "title": ".gitignore"
        }, 
        {
            "location": "/apps/helloworld/#testing", 
            "text": "Now, you should be able to test run your app locally by executing  main  ./main   Now, it should generate an output file called  output.txt  containing the dump of all nifti headers.  class  nibabel.nifti1.Nifti1Header  object, endian= \nsizeof_hdr      : 348\ndata_type       : \ndb_name         : \nextents         : 0\nsession_error   : 0\nregular         : r\ndim_info        : 0\ndim             : [  3 260 311 260   1   1   1   1]\n...\n...\n...\nqoffset_x       : 90.0\nqoffset_y       : -126.0\nqoffset_z       : -72.0\nsrow_x          : [ -0.69999999   0.           0.          90.        ]\nsrow_y          : [   0.            0.69999999    0.         -126.        ]\nsrow_z          : [  0.           0.           0.69999999 -72.        ]\nintent_name     : \nmagic           : n+1", 
            "title": "Testing"
        }, 
        {
            "location": "/apps/helloworld/#pushing-to-github", 
            "text": "If everything looks good, push our files to the Github.  git add .\ngit commit -m created my first BL App! \ngit push  Congratulations! We have just created our first Brainlife App. To summarize, we've done following.   Created a new public Github repo.  Created  main  which parses  config.json  and runs our App.  Created a test  config.json .  Created  main.py  which runs our algorithm and generate output files.  Tested the App, and pushed all files to Github.    Info  You can see more concrete examples of Brainlife apps at  Brainlife hosted apps .   To run your App through Brainlife, you will need to do following.    Register your App on Brainlife.    Enable your App on at least one Brainlife compute resource.   For now, please email  mailto:brlife@iu.edu  to enable your App on our shared test resource.", 
            "title": "Pushing to Github"
        }, 
        {
            "location": "/apps/register/", 
            "text": "Registering App\n\n\nOnce you have developed your app (on github), you can now register it on Brainlife so that you can run it through Brainlife UI and let other users discover your app.\n\n\nUnder \nApp page\n, click \nPlus Button\n at the bottom right corner of the page. App registration form should open.\n\n\nLet's go through each sections.\n\n\nDetail\n\n\n\n\nEnter \nname\n and \nGit Repository Name\n field. On \nGit Repository Name\n, please enter only the organization / repository name (like \nbrain-life/app-life\n) not the full github repo URL.\n\n\nAll other fields in this section are optional, but you could populate following fields.\n\n\nAvatar\n\n\nYou can enter \navatar\n URL if you have URL for an avatar that you'd like to use for your app. Please choose a square image with \nhttps://\n URL (not http://). Avatar may sounds superflusou, but please keep in mind that, there are a lot of app registered in Brainlife and this might be the only visual queue that user could identify and look for among many names / descriptions for other apps.\n\n\nProject\n\n\nBy default, all app are \npublic\n; anyone can find it and execute your app (if they have resource to run them). If you'd like to make your app only available within specific project, you can enter the \nproject name\n and only the member of that project will be able to find and launch your app. This might be useful if you are still developing your app and wants to keep it hidden until you make a formal \nrelease\n. \n\n\nBranch\n\n\nIf you don't specify the github repo's branch name, it uses \nmaster\n branch by default. As with any other project, you will most likely making changes to your \nmaster\n branch after you register your app, which means user won't be able to reproduce the output with exactly the same version of the code used to generate original output. Once you finish developing your app, you should consider creating a release branch (like \nrelease_1.0\n) and freeze the code which will be executed by the Brain-Life by specifying the branch name. Please see \nVersioning Tip\n for more info.\n\n\nInput Datasets\n\n\nHere you can define input datasets with specific datatypes that your app is expecting.\n\n\n\n\nDatatype Tags\n\n\nSometimes you want to be specific about the type of dataset within a particular datatype that your app can handle. For example, \nneuro/anat/t1w\n could be ACPC aligned, or not, \nneuro/dwi\n could be single-shell, or not, etc. Brainlife adds specificity to each datatype through \ndatatype tags\n. Please enter any datatype tags that your app would require under \nDatatype Tags\n field. Brainlife will only allow users to select datasets for your app that meets specified datatype tags. \n\n\n\n\nWarning\n\n\nPlease don't get confuse \nDatatype tags\n with \nDataset tags\n that user can edit under dataset dialog. Dataset tags simply allows user to make it easier to search or bulk process subset of the datasets. Datatype tags, on the other hand is part of datatype and can not be modified once dataset is created. \n\n\n\n\nFile Mapping\n\n\nOnce you select the datatype / tags of your input data, you need to configure how you want it to be presented in the \nconfig.json\n by mapping the json key in \nconfig.json\n to a particular file / directory inside the datatype.\n\n\n{\n    \nt1\n: \n../path/to/t1.nii.gz\n\n}\n\n\n\n\n\nFor example, if you use \nanat/t1w\n as an input dataset, you can specify \nconfig.json key\n field with any json key (such as \"t1\") and selecting which file/dir within the datatype you want to reference (like.. \"../path/to/t1.nii.gz\").\n\n\nYou might be wondering why there is ID field at the top of the input form as well as the json key ID. The input ID at the top is mainly used internally by Brainlife UI for allowing user to select an input dataset. As an app developer, you really don't need to worry about the input ID.. as long as they are unique across all inputs. An input selection could be mapped to multiple fields in \nconfig.json\n which is what you probably most care about.\n\n\nOutput Datasets\n\n\nSimilar to the input dataset, you can specify the datatypes of your output datasets here. It's up to your app to produce output in the correct file structure / file names to be compatible with specified datatype. (Please read \ndatatypes page\n for more info.) \n\n\nDatatype File Mapping\n\n\nBy default, Brainlife expects you to generate all files pertaining to a specific datatype on the current working directory. However, if you have more than 1 output datasets, or have multiple datasets with the same datatype, file name collision could occur. To avoid this, you will need to output each datasets under a different filename, or use sub-directories to store files for each datasets. \n\n\nFor example, following example show that the app is outputing a file named \noutput.DT_STREAM.tck\n which is treated as a \ntrack\n file output for \nneuro/track\n datatype.\n\n\n\n\nOr, if you'd like to store the output files under a sub directory, you can specify the directory by doing following.\n\n\n\n\nraw\n datatype\n\n\nMany app creates datatypes that are not meant to be used by any other app. You can use \nraw\n datatypes to output and archive such outputs. You should avoid using \nraw\n input datatype as an input to another app. If you need to use do this, please contact the developer of the upstream app generating the \nraw\n output dataset and ask them to define a new datatype and use it instead of \nraw\n datatype.\n\n\nConfiguration Parameters\n\n\nConfiguration parameters allows users to enter any number (integer/float), boolean(true/false) or string parameters as configuration to your app. You can also define enum parameter which lets users select option from multiple selections.\n\n\n\n\n\n\n\n\nPlaceholder\n \n\n\nFor each input parameter, you can set a placeholder; a string displayed inside the form element if no value is specified. For example, you could use placeholder to let user know the behavior of the app if user doesn't specify any value for that parameter. \n\n\n\n\n\n\nDescription\n \n\n\nSome parameter types let you specify a description which will be displayed next to the input parameter to show detailed explanation about the input parameter. Please provide enough details for both novice and experienced users of your app. \n\n\n\n\nNote\n\n\nBrainlife App should not be just a simple wrapper for whatever the underlying algorithm you are executing. User should not be expected to know and provide all possible parameters for your algorithm. If your App will not work without a proper selection of parameters, then you should explain enough description for those fields so that user can figure out how to find the best values to use.\n\n\nIdeally, you should make all parameters optional or with workable default values. If user does not provide any value for required field, you should auto-compute the most optimal value at runtime if user does not provide specific values. You should allow an expert users to choose all parameters, but do all you can to make it work with default parameters for novice users.\n\n\n\n\n\n\n\n\nFinally, click \nSubmit\n. Visit the apps page to make sure everything looks OK.\n\n\nREADME / Description / Topics\n\n\nBrainlife uses as much information as possible from your github repo to pull information such as ..\n\n\n\n\n\n\nApp Description / Topics\n\n\nGithub's description is used to display Brainlife app description. Please be sure to enter description that shows what the app does, and what user can do with the output. Github topics are also used as Brainlife's app \ncategories\n. Please look through the existing categories already registered in Brainlife, and use one or more of those categories to help users find your app more easily. \n\n\n\n\n\n\n\n\nREADME.md\n\n\nBrainlife displays the copy of README.md content from your github repo. You can use any images, katex equations, and any other standard markdown content. \n\n\nYou could include information such as following in your README.md\n\n\n\n\nWhat the app does, and how it's implemented (tools, libraries used)\n\n\nWhat the app produces\n\n\nAny diagrams / sample output images, etc.\n\n\nWhat user can do with the output data.\n\n\nDetails on how the algorithm works.\n\n\nComputational cost / resources required to run the app (how long does it take to run, minimum required memory / cpu cores, etc..)\n\n\nHow can other user contribute (are you accepting PR?)\n\n\nReferences to other published papers, or list of contributors.\n\n\n\n\nBrainlife's target audiences are both novice and experienced neuroscience researchers. Please be sure to cater both audiences.\n\n\n\n\n\n\nEnabling app on resource\n\n\nOnce you registered your app on Brain-Life, you then need to have a resource where you can run your app. Resource could be any VM, HPC clusters, or public / private cloud resources. Only the resource owners can enable your app to run on their resource. Please contact the resource administrator for the resource where you'd like to submit your app.\n\n\nIf you have access to your own computing resources, you can register and run your apps there also. Please read \nregistering resource page\n for more detail. Please keep in mind that, only the owner of the resource can run the apps on personal (un-shared) resources. To allow other users to run your app, you will need to enable the app on Brainlife's shared resources.\n\n\n\n\nNote\n\n\nFor all Brainlife default resources, please contact \nSoichi Hayashi\n and request to enable your app on those resources.\n\n\n\n\nOnce your app is enabled on a resource, you should see a table of resources where the app can run under the app detail page.", 
            "title": "Registering App"
        }, 
        {
            "location": "/apps/register/#registering-app", 
            "text": "Once you have developed your app (on github), you can now register it on Brainlife so that you can run it through Brainlife UI and let other users discover your app.  Under  App page , click  Plus Button  at the bottom right corner of the page. App registration form should open.  Let's go through each sections.", 
            "title": "Registering App"
        }, 
        {
            "location": "/apps/register/#detail", 
            "text": "Enter  name  and  Git Repository Name  field. On  Git Repository Name , please enter only the organization / repository name (like  brain-life/app-life ) not the full github repo URL.  All other fields in this section are optional, but you could populate following fields.", 
            "title": "Detail"
        }, 
        {
            "location": "/apps/register/#avatar", 
            "text": "You can enter  avatar  URL if you have URL for an avatar that you'd like to use for your app. Please choose a square image with  https://  URL (not http://). Avatar may sounds superflusou, but please keep in mind that, there are a lot of app registered in Brainlife and this might be the only visual queue that user could identify and look for among many names / descriptions for other apps.", 
            "title": "Avatar"
        }, 
        {
            "location": "/apps/register/#project", 
            "text": "By default, all app are  public ; anyone can find it and execute your app (if they have resource to run them). If you'd like to make your app only available within specific project, you can enter the  project name  and only the member of that project will be able to find and launch your app. This might be useful if you are still developing your app and wants to keep it hidden until you make a formal  release .", 
            "title": "Project"
        }, 
        {
            "location": "/apps/register/#branch", 
            "text": "If you don't specify the github repo's branch name, it uses  master  branch by default. As with any other project, you will most likely making changes to your  master  branch after you register your app, which means user won't be able to reproduce the output with exactly the same version of the code used to generate original output. Once you finish developing your app, you should consider creating a release branch (like  release_1.0 ) and freeze the code which will be executed by the Brain-Life by specifying the branch name. Please see  Versioning Tip  for more info.", 
            "title": "Branch"
        }, 
        {
            "location": "/apps/register/#input-datasets", 
            "text": "Here you can define input datasets with specific datatypes that your app is expecting.", 
            "title": "Input Datasets"
        }, 
        {
            "location": "/apps/register/#datatype-tags", 
            "text": "Sometimes you want to be specific about the type of dataset within a particular datatype that your app can handle. For example,  neuro/anat/t1w  could be ACPC aligned, or not,  neuro/dwi  could be single-shell, or not, etc. Brainlife adds specificity to each datatype through  datatype tags . Please enter any datatype tags that your app would require under  Datatype Tags  field. Brainlife will only allow users to select datasets for your app that meets specified datatype tags.    Warning  Please don't get confuse  Datatype tags  with  Dataset tags  that user can edit under dataset dialog. Dataset tags simply allows user to make it easier to search or bulk process subset of the datasets. Datatype tags, on the other hand is part of datatype and can not be modified once dataset is created.", 
            "title": "Datatype Tags"
        }, 
        {
            "location": "/apps/register/#file-mapping", 
            "text": "Once you select the datatype / tags of your input data, you need to configure how you want it to be presented in the  config.json  by mapping the json key in  config.json  to a particular file / directory inside the datatype.  {\n     t1 :  ../path/to/t1.nii.gz \n}  For example, if you use  anat/t1w  as an input dataset, you can specify  config.json key  field with any json key (such as \"t1\") and selecting which file/dir within the datatype you want to reference (like.. \"../path/to/t1.nii.gz\").  You might be wondering why there is ID field at the top of the input form as well as the json key ID. The input ID at the top is mainly used internally by Brainlife UI for allowing user to select an input dataset. As an app developer, you really don't need to worry about the input ID.. as long as they are unique across all inputs. An input selection could be mapped to multiple fields in  config.json  which is what you probably most care about.", 
            "title": "File Mapping"
        }, 
        {
            "location": "/apps/register/#output-datasets", 
            "text": "Similar to the input dataset, you can specify the datatypes of your output datasets here. It's up to your app to produce output in the correct file structure / file names to be compatible with specified datatype. (Please read  datatypes page  for more info.)", 
            "title": "Output Datasets"
        }, 
        {
            "location": "/apps/register/#datatype-file-mapping", 
            "text": "By default, Brainlife expects you to generate all files pertaining to a specific datatype on the current working directory. However, if you have more than 1 output datasets, or have multiple datasets with the same datatype, file name collision could occur. To avoid this, you will need to output each datasets under a different filename, or use sub-directories to store files for each datasets.   For example, following example show that the app is outputing a file named  output.DT_STREAM.tck  which is treated as a  track  file output for  neuro/track  datatype.   Or, if you'd like to store the output files under a sub directory, you can specify the directory by doing following.", 
            "title": "Datatype File Mapping"
        }, 
        {
            "location": "/apps/register/#raw-datatype", 
            "text": "Many app creates datatypes that are not meant to be used by any other app. You can use  raw  datatypes to output and archive such outputs. You should avoid using  raw  input datatype as an input to another app. If you need to use do this, please contact the developer of the upstream app generating the  raw  output dataset and ask them to define a new datatype and use it instead of  raw  datatype.", 
            "title": "raw datatype"
        }, 
        {
            "location": "/apps/register/#configuration-parameters", 
            "text": "Configuration parameters allows users to enter any number (integer/float), boolean(true/false) or string parameters as configuration to your app. You can also define enum parameter which lets users select option from multiple selections.     Placeholder    For each input parameter, you can set a placeholder; a string displayed inside the form element if no value is specified. For example, you could use placeholder to let user know the behavior of the app if user doesn't specify any value for that parameter.     Description    Some parameter types let you specify a description which will be displayed next to the input parameter to show detailed explanation about the input parameter. Please provide enough details for both novice and experienced users of your app.    Note  Brainlife App should not be just a simple wrapper for whatever the underlying algorithm you are executing. User should not be expected to know and provide all possible parameters for your algorithm. If your App will not work without a proper selection of parameters, then you should explain enough description for those fields so that user can figure out how to find the best values to use.  Ideally, you should make all parameters optional or with workable default values. If user does not provide any value for required field, you should auto-compute the most optimal value at runtime if user does not provide specific values. You should allow an expert users to choose all parameters, but do all you can to make it work with default parameters for novice users.     Finally, click  Submit . Visit the apps page to make sure everything looks OK.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/apps/register/#readme-description-topics", 
            "text": "Brainlife uses as much information as possible from your github repo to pull information such as ..    App Description / Topics  Github's description is used to display Brainlife app description. Please be sure to enter description that shows what the app does, and what user can do with the output. Github topics are also used as Brainlife's app  categories . Please look through the existing categories already registered in Brainlife, and use one or more of those categories to help users find your app more easily.      README.md  Brainlife displays the copy of README.md content from your github repo. You can use any images, katex equations, and any other standard markdown content.   You could include information such as following in your README.md   What the app does, and how it's implemented (tools, libraries used)  What the app produces  Any diagrams / sample output images, etc.  What user can do with the output data.  Details on how the algorithm works.  Computational cost / resources required to run the app (how long does it take to run, minimum required memory / cpu cores, etc..)  How can other user contribute (are you accepting PR?)  References to other published papers, or list of contributors.   Brainlife's target audiences are both novice and experienced neuroscience researchers. Please be sure to cater both audiences.", 
            "title": "README / Description / Topics"
        }, 
        {
            "location": "/apps/register/#enabling-app-on-resource", 
            "text": "Once you registered your app on Brain-Life, you then need to have a resource where you can run your app. Resource could be any VM, HPC clusters, or public / private cloud resources. Only the resource owners can enable your app to run on their resource. Please contact the resource administrator for the resource where you'd like to submit your app.  If you have access to your own computing resources, you can register and run your apps there also. Please read  registering resource page  for more detail. Please keep in mind that, only the owner of the resource can run the apps on personal (un-shared) resources. To allow other users to run your app, you will need to enable the app on Brainlife's shared resources.   Note  For all Brainlife default resources, please contact  Soichi Hayashi  and request to enable your app on those resources.   Once your app is enabled on a resource, you should see a table of resources where the app can run under the app detail page.", 
            "title": "Enabling app on resource"
        }, 
        {
            "location": "/apps/container/", 
            "text": "Docker is a software containerization tool that allow you to package your app and its dependencies into a portable \"container\". Once built, you can run it on any machine as long as it supports singularity regardless of the type of host OS, or host software installed.\n\n\nAlthough you can create a fully functional standalone docker container for your app, normally we only need to containerize the application \ndependendencies\n. For Brainlife, we recommend not to include the main part of your app (your python or bash scripts that drives your application) on your container. Once you create container for you application \nenvironment\n, with singularity, your can use it to execute the most ideosyncratic parts of your app through it. \n\n\n\n\nWarning\n\n\nIf you do share your \nenvironment\n container, be sure to specify which version(tag) of the container you are using in your app. Like..\n\nsingularity exec -e docker://brainlife/mrtrix_on_mcr:1.0 ./ensembletracking.sh\n\n\nWithout \n1.0\n, you won't know which version of the container you are executing which defeats the reproducibility of your app.\n\n\n\n\nTo build a docker container, you need to \ninstall Docker engine on your laptop\n or find a server that has docker engine installed that you can use. (Contact \nSoichi\n if you need a help.)\n\n\nWe assume you already have your Brainlife app hosted on Github, and you are making changes inside a cloned git repo on a machine with Docker engine.\n\n\nCompile Matlab\n\n\nTo run your application through a container, all Matlab scripts need to be compiled to a binary format using the \nmcc\n MatLab command\n. You can create a script that runs something like following.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n#!/bin/bash\n\nmodule load matlab/2017a\n\nmkdir -p compiled\n\ncat \n build.m \nEND\n\n\naddpath(genpath(\n/N/u/brlife/git/vistasoft\n))\n\n\naddpath(genpath(\n/N/u/brlife/git/jsonlab\n))\n\n\naddpath(genpath(\n/N/soft/mason/SPM/spm8\n))\n\n\nmcc -m -R -nodisplay -d compiled myapp\n\n\n%# function sptensor\n\n\nexit\n\n\nEND\n\nmatlab -nodisplay -nosplash -r build\n\n\n\n\n\n\nThis script generates a Matlab script called \nbuild.m\n and immediately executes it. \nbuild.m\n will setup the path and run Matlab command called \nmcc\n which does the actual compilation of your Matlab code and generates a binary that can be run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR. You can freely download MCR library from Matlab website, or use MCR container such as brainlife/mcr to execute your binary with.\n\n\nYou will need to adjust addpath() to include all of your dependencies that your application requires. \"myapp\" is the name of the matlab script that you use to execute your application. mcc commad will create a binary with the same name \"myapp\" inside the ./compiled directory.\n\n\n\n\n-m ...\n tells the name of the main entry function (in this case it's \nmain\n) of your application (it reads your config.json and runs the whole application)\n\n\n-R -nodisplay\n sets the command line options you'd normally pass when you run MatLab on the command line.\n\n\n-d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized.\n\n\n\n\nIf your app uses any MEX files (.c code) you will need to add an extra option (-a) to specify where those MEX files are stored. For example..\n\n\nmcc -m -R -nodisplay -a /N/u/hayashis/BigRed2/git/encode/mexfiles -d compiled myapp\n\n\n\n\n\n\n\nYou can use OpenMP to parallelize your application by using a container that has libgomp1 installed.\n\n\nmcc compiled application can't run certain Matlab statements; like addpath(). You might need to create a stripped down version of the main function that does not include those statements (or wrap them inside \nif not isdeployed\n statement that gets executed only when you run it directly on Matlab) . \n\n\n\n\nIf you don't have MatLab installed on your local machine, then you can do the compliation on the machine that has Matlab installed, the build the docker container on your own machine.\n\n\n\n\nNOTE. If you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround might be temporarly edit your startup.m to not include any addpath (or rename startup.m to startup.m.disabled) then run your compile script. \n\n\n\n\nLoading an ENCODE fe structure from an mcc application\n\n\nIf you are writing a function that will load a fe structure containing a sparse tensor array created by ENCODE, you need to add the following decorator above the load function:\n\n\n%# function sptensor\n\n\nThis lets the compiler know it should include the sptensor class, which \nload()\n does not use by default. If you do not include this, a warning will occur when you run the function stating the sptensor class was not found and it will load as an empty field. \n\n\nCreate a bash script to run your app\n\n\nUnless your application sorely consists of matlab, you may want to create another bash script which will do any pre(/post)processing of the data. For example:\n\n\ndocker.sh\n\n\n#!/bin/bash\n\n./preprocess.sh \n#run some things (need to parse config.json using command line tool like jq)\n\n./mca/main \n#then execute matlab binary\n\n\n\n\n\n\nCreate Dockerfile (MatLab)\n\n\nBefore you start working on Dockerfile, make sure you are already familiar with the basic concepts of Docker and \nhow to build docker container\n. \n\n\nAll Docker containers have a base-container. This base is used to derive all other containers. If your application uses Matlab, then I recommend using\nthe Brain-Life Docker Container\n. Alternatively, you could also use a more general OS container such as a Ubuntu, CentOS. The \nneurodebian container\n is a good alternative.\n\n\nOn this example, I am going to use \nthe LiFE Docker Container\n as a template (this was compiled using MCR:2016a on Ubuntu Linux compatible with NeuroDebian), but we are going to \napp-dtiinit\n as an example to build a Docker container. To start:\n\n\n\n\ncd into your local app-dtiinit folder (this folder should have the main file). \n\n\ncopy the Dockerfile from \nonline\n into the current directory.\n\n\nOpen the \nDockerfile\n and add the following lines, which will add a series of Docker commands: \n\n\nSet the Docker base image and maintainer.\n   \nFROM brainlife/mcr:R2016a\n   MAINTAINER Your Name \nyouremail@iu.edu\n\n\nAdd dependencies. \n   This app requires the external, package FSL  to add this dependency we will the following lines to the Dockerfile.\n\n\n\n\nRUN sudo apt-get install fsl-complete\n\n   3. Add the app-dtiinit to the Docker. \n   We want to put the entire content of you git repository (in our example because we have some MatLab code, the repository has been previously made into a standalone executable and compiled and the compiled version is under app-dtiinit/msa) on to this container somewhere. I am going to put it under /app.\n   \nADD . /app\n\n\n\n\n\n\nLastly, need to specify the output directory to use\n   \nRUN mkdir /output\n   WORKDIR /output\n\n\n\n\n\n\nThen set the entry point of your application\n   \nENTRYPOINT [\"/app/docker.sh\"]\n\n\n\n\n\n\n3b. Prepare for your container to be run under singularity. Singularity allows users to run your container where they don't have root access (or docker engine access). One issue with running Docker container under singularity is that, often dynamically linked libraries creates symlinks under /usr/lib64 directory when they are first executed. (TODO - explain why this fails under singularity). To setup all necessary symlinks before singularity executes the container, run ldconfig at the end of your Dockerfile\n\n\nRUN ldconfig\n\n\nFor more information about singularity, please see http://singularity.lbl.gov/docs-docker#best-practices\n\n\n\n\nNow, create a sample config.json on your current directory to be used..\n\n\n\n\nconfig.json\n\n\n{\n   \nsome\n: \nparam\n,\n   \nanother\n: 123,\n   \ninput\n: \n/input/somefile.nii.gz\n\n}\n\n\n\n\n\n\n\nBuild the container (make sure to name it something like \n-t brainlife/someapp\n), \n\n\n\n\ndocker build -t brainlife/check.\n\n\n\n\n\n\n\nThen run it to test it\n\n\n\n\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp\n\n\n\n\n\n7 (optional). You can also save the above two commands into separate .sh files for ease of access. \n\n\nvim run_docker.sh\n(copy and paste into run_docker.sh\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp)\n./run_docker.sh\n\n\n\n\n\nDo something similar for building the container into its own build.sh file.\n\n\nCreate DockerFile (Python)\n\n\nThere are a few differences between the Matlab and Python versions of DockerFile. Namely the version and dependencies. While the Matlab one will show how to create one from an existing DockerFile, this part will show how to create a DockerFile from scratch. Please remember to install Docker before continuing.\n\n\n\n\nIn the folder of your Brain-Life application. Create a file called DockerFile.\n\n\n\n\nvim Dockerfile\n\n\n\n\n\n\n\nIn the DockerFile, set the OS and version of Python you would like to use.\n\n\n\n\nFROM ubuntu:16.04 (or whatever OS fits your fancy)\nFROM python:2\n\n\n\n\n\n\n\nNow install the dependencies your application requires.\n\n\n\n\nTo install pip and git, which you would most likely want to do, add the following to your DockerFile.\n\n\nRUN apt-get update \n apt-get install -y python-pip git\n\n\n\n\n\nTo pip install your dependencies add the following command:\n\n\nRUN pip install numpy Cython your-other-dependencies\n\n\n\n\n\nTo git clone a repository and install from there add the following\n\n\nRUN git clone the git url of the repository /rep-name\nRUN cd /rep-name \n python setup.py build_ext --inplace\n\n\n\n\n\n\n\nNow we need to make a folder for adding our main python files. The DockerFile will run your applications from here.\n\n\n\n\nRUN mkdir /app\nCOPY main.py /app\nCOPY another necessary .py file /app\n\n\n\n\n\nCopy all necessary python files into the app folder.\n\n\n\n\nWe will now create a folder for the output of your application.\n\n\n\n\nRUN mkdir /output\nWORKDIR /output\n\n\n\n\n\nIf you want to test your application locally, make sure you have your \nlocal\n config.json file in the output folder.\n\n\n\n\nIf you did any git clones and install from the setup.py in there make sure you also set the PYTHONPATH like so\n\n\n\n\nIf you installed everything from pip, you may skip this step.\n\n\nENV PYTHONPATH /my-git-repo:$PYTHONPATH\n\n\n\n\n\n\n\nFinally, we add the last line to finish up our DockerFile.\n\n\n\n\nCMD /app/main.py\n\n\n\n\n\nCongratulations! You wrote the DockerFile! You can see a full example here: \"add example here Aman\"\n\n\nBuilding and running your container\n\n\n\n\nBuild the container (make sure to name it something like \n-t brainlife/someapp\n), \n\n\n\n\ndocker build -t brainlife/check .\n\n\n\n\n\n\n\nThen run it to test it\n\n\n\n\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp\n\n\n\n\n\n3 (optional). You can also save the above two commands into separate .sh files for ease of access. \n\n\nrun_docker.sh\n\n\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp\n\n\n\n\n\nDo something similar for building the container into its own build.sh file.\n\n\nvim run_docker.sh\n(copy and paste into build.sh)\ndocker build -t brainlife/check.\n./build.sh (to run)\n\n\n\n\n\nYou only need to run the build once, unless you change something in your DockerFile. Otherwise you can directly run with the command or with run_docker.sh.\n\n\nYou will also probably need to give your run_docker.sh and build.sh permissions.\n\n\nchmod +x build.sh run_docker.sh\n\n\n\n\n\nREADME\n\n\nOnce you know that your container works, you should push your container to docker hub and update the README of your git repo to include instructions on how to run your container.\n\n\nExamples\n\n\nMatlab App Exmple \n\n\n\n\ngithub brain-life/app-life\n\n\ndockerhub brainlife/life\n (Dockerhub Autobuild)\n\n\n\n\nDipy (python) App Example\n\n\n\n\ngithub brain-life/app-dipy-workflows", 
            "title": "Containerizing App"
        }, 
        {
            "location": "/apps/container/#compile-matlab", 
            "text": "To run your application through a container, all Matlab scripts need to be compiled to a binary format using the  mcc  MatLab command . You can create a script that runs something like following.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 #!/bin/bash \nmodule load matlab/2017a\n\nmkdir -p compiled\n\ncat   build.m  END  addpath(genpath( /N/u/brlife/git/vistasoft ))  addpath(genpath( /N/u/brlife/git/jsonlab ))  addpath(genpath( /N/soft/mason/SPM/spm8 ))  mcc -m -R -nodisplay -d compiled myapp  %# function sptensor  exit  END \nmatlab -nodisplay -nosplash -r build   This script generates a Matlab script called  build.m  and immediately executes it.  build.m  will setup the path and run Matlab command called  mcc  which does the actual compilation of your Matlab code and generates a binary that can be run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR. You can freely download MCR library from Matlab website, or use MCR container such as brainlife/mcr to execute your binary with.  You will need to adjust addpath() to include all of your dependencies that your application requires. \"myapp\" is the name of the matlab script that you use to execute your application. mcc commad will create a binary with the same name \"myapp\" inside the ./compiled directory.   -m ...  tells the name of the main entry function (in this case it's  main ) of your application (it reads your config.json and runs the whole application)  -R -nodisplay  sets the command line options you'd normally pass when you run MatLab on the command line.  -d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized.   If your app uses any MEX files (.c code) you will need to add an extra option (-a) to specify where those MEX files are stored. For example..  mcc -m -R -nodisplay -a /N/u/hayashis/BigRed2/git/encode/mexfiles -d compiled myapp   You can use OpenMP to parallelize your application by using a container that has libgomp1 installed.  mcc compiled application can't run certain Matlab statements; like addpath(). You might need to create a stripped down version of the main function that does not include those statements (or wrap them inside  if not isdeployed  statement that gets executed only when you run it directly on Matlab) .    If you don't have MatLab installed on your local machine, then you can do the compliation on the machine that has Matlab installed, the build the docker container on your own machine.   NOTE. If you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround might be temporarly edit your startup.m to not include any addpath (or rename startup.m to startup.m.disabled) then run your compile script.", 
            "title": "Compile Matlab"
        }, 
        {
            "location": "/apps/container/#loading-an-encode-fe-structure-from-an-mcc-application", 
            "text": "If you are writing a function that will load a fe structure containing a sparse tensor array created by ENCODE, you need to add the following decorator above the load function:  %# function sptensor  This lets the compiler know it should include the sptensor class, which  load()  does not use by default. If you do not include this, a warning will occur when you run the function stating the sptensor class was not found and it will load as an empty field.", 
            "title": "Loading an ENCODE fe structure from an mcc application"
        }, 
        {
            "location": "/apps/container/#create-a-bash-script-to-run-your-app", 
            "text": "Unless your application sorely consists of matlab, you may want to create another bash script which will do any pre(/post)processing of the data. For example:  docker.sh  #!/bin/bash \n./preprocess.sh  #run some things (need to parse config.json using command line tool like jq) \n./mca/main  #then execute matlab binary", 
            "title": "Create a bash script to run your app"
        }, 
        {
            "location": "/apps/container/#create-dockerfile-matlab", 
            "text": "Before you start working on Dockerfile, make sure you are already familiar with the basic concepts of Docker and  how to build docker container .   All Docker containers have a base-container. This base is used to derive all other containers. If your application uses Matlab, then I recommend using the Brain-Life Docker Container . Alternatively, you could also use a more general OS container such as a Ubuntu, CentOS. The  neurodebian container  is a good alternative.  On this example, I am going to use  the LiFE Docker Container  as a template (this was compiled using MCR:2016a on Ubuntu Linux compatible with NeuroDebian), but we are going to  app-dtiinit  as an example to build a Docker container. To start:   cd into your local app-dtiinit folder (this folder should have the main file).   copy the Dockerfile from  online  into the current directory.  Open the  Dockerfile  and add the following lines, which will add a series of Docker commands:   Set the Docker base image and maintainer.\n    FROM brainlife/mcr:R2016a\n   MAINTAINER Your Name  youremail@iu.edu  Add dependencies. \n   This app requires the external, package FSL  to add this dependency we will the following lines to the Dockerfile.   RUN sudo apt-get install fsl-complete \n   3. Add the app-dtiinit to the Docker. \n   We want to put the entire content of you git repository (in our example because we have some MatLab code, the repository has been previously made into a standalone executable and compiled and the compiled version is under app-dtiinit/msa) on to this container somewhere. I am going to put it under /app.\n    ADD . /app    Lastly, need to specify the output directory to use\n    RUN mkdir /output\n   WORKDIR /output    Then set the entry point of your application\n    ENTRYPOINT [\"/app/docker.sh\"]    3b. Prepare for your container to be run under singularity. Singularity allows users to run your container where they don't have root access (or docker engine access). One issue with running Docker container under singularity is that, often dynamically linked libraries creates symlinks under /usr/lib64 directory when they are first executed. (TODO - explain why this fails under singularity). To setup all necessary symlinks before singularity executes the container, run ldconfig at the end of your Dockerfile  RUN ldconfig  For more information about singularity, please see http://singularity.lbl.gov/docs-docker#best-practices   Now, create a sample config.json on your current directory to be used..   config.json  {\n    some :  param ,\n    another : 123,\n    input :  /input/somefile.nii.gz \n}   Build the container (make sure to name it something like  -t brainlife/someapp ),    docker build -t brainlife/check.   Then run it to test it   docker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp  7 (optional). You can also save the above two commands into separate .sh files for ease of access.   vim run_docker.sh\n(copy and paste into run_docker.sh\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp)\n./run_docker.sh  Do something similar for building the container into its own build.sh file.", 
            "title": "Create Dockerfile (MatLab)"
        }, 
        {
            "location": "/apps/container/#create-dockerfile-python", 
            "text": "There are a few differences between the Matlab and Python versions of DockerFile. Namely the version and dependencies. While the Matlab one will show how to create one from an existing DockerFile, this part will show how to create a DockerFile from scratch. Please remember to install Docker before continuing.   In the folder of your Brain-Life application. Create a file called DockerFile.   vim Dockerfile   In the DockerFile, set the OS and version of Python you would like to use.   FROM ubuntu:16.04 (or whatever OS fits your fancy)\nFROM python:2   Now install the dependencies your application requires.   To install pip and git, which you would most likely want to do, add the following to your DockerFile.  RUN apt-get update   apt-get install -y python-pip git  To pip install your dependencies add the following command:  RUN pip install numpy Cython your-other-dependencies  To git clone a repository and install from there add the following  RUN git clone the git url of the repository /rep-name\nRUN cd /rep-name   python setup.py build_ext --inplace   Now we need to make a folder for adding our main python files. The DockerFile will run your applications from here.   RUN mkdir /app\nCOPY main.py /app\nCOPY another necessary .py file /app  Copy all necessary python files into the app folder.   We will now create a folder for the output of your application.   RUN mkdir /output\nWORKDIR /output  If you want to test your application locally, make sure you have your  local  config.json file in the output folder.   If you did any git clones and install from the setup.py in there make sure you also set the PYTHONPATH like so   If you installed everything from pip, you may skip this step.  ENV PYTHONPATH /my-git-repo:$PYTHONPATH   Finally, we add the last line to finish up our DockerFile.   CMD /app/main.py  Congratulations! You wrote the DockerFile! You can see a full example here: \"add example here Aman\"", 
            "title": "Create DockerFile (Python)"
        }, 
        {
            "location": "/apps/container/#building-and-running-your-container", 
            "text": "Build the container (make sure to name it something like  -t brainlife/someapp ),    docker build -t brainlife/check .   Then run it to test it   docker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp  3 (optional). You can also save the above two commands into separate .sh files for ease of access.   run_docker.sh  docker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp  Do something similar for building the container into its own build.sh file.  vim run_docker.sh\n(copy and paste into build.sh)\ndocker build -t brainlife/check.\n./build.sh (to run)  You only need to run the build once, unless you change something in your DockerFile. Otherwise you can directly run with the command or with run_docker.sh.  You will also probably need to give your run_docker.sh and build.sh permissions.  chmod +x build.sh run_docker.sh", 
            "title": "Building and running your container"
        }, 
        {
            "location": "/apps/container/#readme", 
            "text": "Once you know that your container works, you should push your container to docker hub and update the README of your git repo to include instructions on how to run your container.", 
            "title": "README"
        }, 
        {
            "location": "/apps/container/#examples", 
            "text": "Matlab App Exmple    github brain-life/app-life  dockerhub brainlife/life  (Dockerhub Autobuild)   Dipy (python) App Example   github brain-life/app-dipy-workflows", 
            "title": "Examples"
        }, 
        {
            "location": "/apps/versioning/", 
            "text": "Although Brainlife does not require you to use any specific git branching schema and you should observe any standrad practice from your own group, here are some guidelines that we suggest.\n\n\n1. git pull often\n\n\nBefore you start editing your app on your local machine each day, be sure to pull from origin. \n\n\ngit pull\n\n\n\n\n\nYou should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master.\n\n\n\n\nBy the way, see \ngit rebase\n if you are not familiar with rebasing. It helps our commit log to be clean.\n\n\n\n\n2. Always work on master branch\n\n\nWhen you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin.\n\n\n3. Create branch for new versions\n\n\nWhen we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. The easiest way to create a new branch is to do this on github UI. Just click on where it says \"Branch: master\" and enter new version number to create a new branch. The branch name should be a version number .. like \"1.0\", \"1.1\", \"1.2\", etc.., it should not be a branch names like the local branches you create on your local repo.\n\n\nOnce you create a branch, you should update the BL app to point users to use that new branch.\n\n\nYou could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also.\n\n\n4. Bug fix on master, then on branch.\n\n\nIf you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I recommend using command like \ncherry-pick\n to apply specific changes on other branches. Again, we should not add any new features on branches (only bug fixes) for reproducibility. \n\n\n5. Semver\n\n\nIf you don't know what semantic versioning is, please read \nhttps://semver.org/\n.\n\n\nFor branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new Brainlife app with different major version number so that user can continue to submit your app with previous versions.", 
            "title": "Versioning Tips"
        }, 
        {
            "location": "/apps/versioning/#1-git-pull-often", 
            "text": "Before you start editing your app on your local machine each day, be sure to pull from origin.   git pull  You should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master.   By the way, see  git rebase  if you are not familiar with rebasing. It helps our commit log to be clean.", 
            "title": "1. git pull often"
        }, 
        {
            "location": "/apps/versioning/#2-always-work-on-master-branch", 
            "text": "When you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin.", 
            "title": "2. Always work on master branch"
        }, 
        {
            "location": "/apps/versioning/#3-create-branch-for-new-versions", 
            "text": "When we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. The easiest way to create a new branch is to do this on github UI. Just click on where it says \"Branch: master\" and enter new version number to create a new branch. The branch name should be a version number .. like \"1.0\", \"1.1\", \"1.2\", etc.., it should not be a branch names like the local branches you create on your local repo.  Once you create a branch, you should update the BL app to point users to use that new branch.  You could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also.", 
            "title": "3. Create branch for new versions"
        }, 
        {
            "location": "/apps/versioning/#4-bug-fix-on-master-then-on-branch", 
            "text": "If you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I recommend using command like  cherry-pick  to apply specific changes on other branches. Again, we should not add any new features on branches (only bug fixes) for reproducibility.", 
            "title": "4. Bug fix on master, then on branch."
        }, 
        {
            "location": "/apps/versioning/#5-semver", 
            "text": "If you don't know what semantic versioning is, please read  https://semver.org/ .  For branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new Brainlife app with different major version number so that user can continue to submit your app with previous versions.", 
            "title": "5. Semver"
        }, 
        {
            "location": "/apps/customhooks/", 
            "text": "Life cycles hooks are the script used to start / stop / monitor your apps by Brain-Life. By default, it looks for executable installed on each resource in the PATH with named \nstart\n, \nstatus\n, and \nstop\n. Resource owner needs to make sure these scripts are installed and accessible by your apps. \n\n\n\n\nFor most PBS, SLURM, and vanila VM, resource owner can install \nABCD default hooks\n.\n\n\n\n\nBy default, \nstart\n hook should look for a file named \nmain\n to start your app. Therefore, the only file required to make your app runnable by Brain-Life is this \nmain\n executable on the root directory of the app's git repository. \n\n\nUnder most circumstances, app developers shouldn't have to worry about these hook scripts. However, if your app requires some special mechanism to start / stop and monitor your app, you might need to provide your own hook scripts. \n\n\nYou can specify the paths to these hook scripts by creating a file named \npackage.json\n\n\n{\n\n  \nbrainlife\n:\n \n{\n\n    \nstart\n:\n \n./start.sh\n,\n\n    \nstop\n:\n \n./stop.sh\n,\n\n    \nstatus\n:\n \n./status.sh\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThen, you will need to provide those hook scripts as part of your app.\n\n\n\n\nPlease be sure to \nchmod +x *.sh\n so that your hook scripts are executable.\n\n\n\n\nstart.sh\n\n\nFollowing is an example for \nstart\n script. It submits a file named \nmain\n (should be provided by each app) through qsub. It stores \njobid\n so that we can monitor the job status.\n\n\n1\n2\n3\n4\n5\n6\n#!/bin/bash\n\n\n\n#return code 0 = job started successfully.\n\n\n#return code non-0 = job failed to start\n\n\nqsub -d \n$PWD\n -V -o \n\\$\nPBS_JOBID.log -e \n\\$\nPBS_JOBID.err main \n jobid\n\n\n\n\n\n\nstop.sh\n\n\nFollowing is an example for \nstop\n script. This scripts reads the jobid created by \nstart\n script and call qdel to stop it.\n\n\n1\n2\n#!/bin/bash\n\nqdel \n`\ncat jobid\n`\n\n\n\n\n\n\n\nstatus.sh\n\n\nstatus hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the \njobid\n stored by start script to query the job status with \nqstat\n PBS command. \n\n\nAnything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n#!/bin/bash\n\n\n\n#return code 0 = running\n\n\n#return code 1 = finished successfully\n\n\n#return code 2 = failed\n\n\n#return code 3 = unknown (retry later)\n\n\n\nif\n \n[\n ! -f jobid \n]\n;\nthen\n\n    \necho\n \nno jobid - not yet submitted?\n\n    \nexit\n \n1\n\n\nfi\n\n\n\njobid\n=\n`\ncat jobid\n`\n\n\nif\n \n[\n -z \n$jobid\n \n]\n;\n \nthen\n\n    \necho\n \njobid is empty.. failed to submit?\n\n    \nexit\n \n3\n\n\nfi\n\n\n\njobstate\n=\n`\nqstat -f \n$jobid\n \n|\n grep job_state \n|\n cut -b17\n`\n\n\nif\n \n[\n -z \n$jobstate\n \n]\n;\n \nthen\n\n    \necho\n \nJob removed before completing - maybe timed out?\n\n    \nexit\n \n2\n\n\nfi\n\n\n\ncase\n \n$jobstate\n in\nQ\n)\n\n    showstart \n$jobid\n \n|\n grep start\n    \nexit\n \n0\n\n    \n;;\n\nR\n)\n\n    \n#get last line of last log touched\n\n    \nlogfile\n=\n$(\nls -rt *.log \n|\n tail -1\n)\n\n    tail -1 \n$logfile\n\n    \nexit\n \n0\n\n    \n;;\n\nH\n)\n\n    \necho\n \nJob held.. waiting\n\n    \nexit\n \n0\n\n    \n;;\n\nC\n)\n\n    \nexit_status\n=\n`\nqstat -f \n$jobid\n \n|\n grep exit_status \n|\n cut -d\n=\n -f2 \n|\n xargs\n`\n\n    \nif\n \n[\n \n$exit_status\n -eq \n0\n \n]\n;\n \nthen\n\n        \necho\n \nfinished with code 0\n\n        \nexit\n \n1\n\n    \nelse\n\n        \necho\n \nfinished with code \n$exit_status\n\n        \nexit\n \n2\n\n    \nfi\n\n    \n;;\n\n*\n)\n\n    \necho\n \nunknown job status \n$jobstate\n\n    \nexit\n \n2\n\n    \n;;\n\n\n\nesac", 
            "title": "Custom Hooks"
        }, 
        {
            "location": "/apps/customhooks/#startsh", 
            "text": "Following is an example for  start  script. It submits a file named  main  (should be provided by each app) through qsub. It stores  jobid  so that we can monitor the job status.  1\n2\n3\n4\n5\n6 #!/bin/bash  #return code 0 = job started successfully.  #return code non-0 = job failed to start \n\nqsub -d  $PWD  -V -o  \\$ PBS_JOBID.log -e  \\$ PBS_JOBID.err main   jobid", 
            "title": "start.sh"
        }, 
        {
            "location": "/apps/customhooks/#stopsh", 
            "text": "Following is an example for  stop  script. This scripts reads the jobid created by  start  script and call qdel to stop it.  1\n2 #!/bin/bash \nqdel  ` cat jobid `", 
            "title": "stop.sh"
        }, 
        {
            "location": "/apps/customhooks/#statussh", 
            "text": "status hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the  jobid  stored by start script to query the job status with  qstat  PBS command.   Anything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55 #!/bin/bash  #return code 0 = running  #return code 1 = finished successfully  #return code 2 = failed  #return code 3 = unknown (retry later)  if   [  ! -f jobid  ] ; then \n     echo   no jobid - not yet submitted? \n     exit   1  fi  jobid = ` cat jobid `  if   [  -z  $jobid   ] ;   then \n     echo   jobid is empty.. failed to submit? \n     exit   3  fi  jobstate = ` qstat -f  $jobid   |  grep job_state  |  cut -b17 `  if   [  -z  $jobstate   ] ;   then \n     echo   Job removed before completing - maybe timed out? \n     exit   2  fi  case   $jobstate  in\nQ ) \n    showstart  $jobid   |  grep start\n     exit   0 \n     ;; \nR ) \n     #get last line of last log touched \n     logfile = $( ls -rt *.log  |  tail -1 ) \n    tail -1  $logfile \n     exit   0 \n     ;; \nH ) \n     echo   Job held.. waiting \n     exit   0 \n     ;; \nC ) \n     exit_status = ` qstat -f  $jobid   |  grep exit_status  |  cut -d =  -f2  |  xargs ` \n     if   [   $exit_status  -eq  0   ] ;   then \n         echo   finished with code 0 \n         exit   1 \n     else \n         echo   finished with code  $exit_status \n         exit   2 \n     fi \n     ;; \n* ) \n     echo   unknown job status  $jobstate \n     exit   2 \n     ;;  esac", 
            "title": "status.sh"
        }, 
        {
            "location": "/resources/register/", 
            "text": "Background\n\n\nBrainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled. \n\n\nAs Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases.\n\n\n\n\nYou have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control.\n\n\nYou are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging.\n\n\nYou are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide.\n\n\n\n\nCurrently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact \nBrainlife Admin\n\n\n\n\nNote\n\n\nResource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact \nBrainlife Admin\n to enable your app on Brainlife default resources.\n\n\n\n\n\n\nWarning\n\n\nAlthough we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security\nespecially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information).\n\n\n\n\nRegistering Resources\n\n\nTo register your resource, go to \nBrainlife Settings\n page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields.\n\n\n\n\nName\n Enter the name of rhe resource\n\n\nHostname\n The hostname of your compute resource (usually a login/submit host)\n\n\nUsername\n Username used to ssh to this resource\n\n\nWorkdir\n Directory used to stage and store generated datasets by apps. \nYou should not share the same directory with other resources\n. Please make sure that the specified directory exits (mkdir if not).\n\n\nSSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read \nauthorized_keys\n for more detail.\n\n\n\n\nYou can leave the rest of the fields empty for now.\n\n\nClick OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good.\n\n\nConfiguring Resources\n\n\nOnce you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps.\n\n\nABCD Default Hooks\n\n\nABCD Hooks\n are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting \n$PATH\n. If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following.\n\n\ncd ~\ngit clone https://github.com/brain-life/abcd-spec\n\n\n\n\n\nThen, add one of following to your ~/.bashrc\n\n\nFor PBS cluster\n\n\nexport PATH=~/abcd-spec/hooks/pbs:$PATH\n\n\n\n\n\nFor Slurm cluster\n\n\nexport PATH=~/abcd-spec/hooks/slurm:$PATH\n\n\n\n\n\nFor direct execution - no batch submission manager\n\n\nexport PATH=~/abcd-spec/hooks/direct:$PATH\n\n\n\n\n\nCommon Binaries\n\n\nBrainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed.\n\n\n\n\njq (command line json parser commonly used by Brainlife apps to parse config.json)\n\n\ngit (used to clone / update apps installed)\n\n\nsingularity (user level container execution engine)\n\n\n\n\nFor IU HPC resource, please feel free to use following ~/bin directory which contains jq\n\n\n$ ~/.bashrc\n\nexport\n \nPATH\n=\n$PATH\n:/N/u/brlife/Carbonate/bin\n\n\n\n\n\nFor singularity, you can either install it on the system, or for most HPC systems you can simply add following in your \n~/.modules\n file.\n\n\nmodule\n \nload\n \nsingularity\n\n\n\n\n\n\nBy default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc\n\n\nexport SINGULARITY_CACHEDIR=/N/dc2/scratch/\nusername\n/singularity-cachedir\n\n\n\n\n\n\n\nPlease replace \n with your username, and make sure specified directories exists.\n\n\n\n\nOther ENV parameters\n\n\nDepending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via \nFREESURFER_LICENSE\n.\n\n\nexport FREESURFER_LICENSE=\nhayashis@iu.edu 29511 *CPmh9xvKQKHE FSg0ijTusqaQc\n\n\n\n\n\n\nEnabling Apps\n\n\nOnce you have registered and tested your resource, you can now enable apps to run on your resource.\n\n\nGo back to the Brainlife's \nresource settings page\n, and click the resource you have created. Under the services section, enter the git org/repo name (such as like \nbrain-life/app-life\n) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK.\n\n\nYou can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife. \nexample", 
            "title": "Registering Resource"
        }, 
        {
            "location": "/resources/register/#background", 
            "text": "Brainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled.   As Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases.   You have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control.  You are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging.  You are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide.   Currently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact  Brainlife Admin   Note  Resource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact  Brainlife Admin  to enable your app on Brainlife default resources.    Warning  Although we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security\nespecially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information).", 
            "title": "Background"
        }, 
        {
            "location": "/resources/register/#registering-resources", 
            "text": "To register your resource, go to  Brainlife Settings  page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields.   Name  Enter the name of rhe resource  Hostname  The hostname of your compute resource (usually a login/submit host)  Username  Username used to ssh to this resource  Workdir  Directory used to stage and store generated datasets by apps.  You should not share the same directory with other resources . Please make sure that the specified directory exits (mkdir if not).  SSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read  authorized_keys  for more detail.   You can leave the rest of the fields empty for now.  Click OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good.", 
            "title": "Registering Resources"
        }, 
        {
            "location": "/resources/register/#configuring-resources", 
            "text": "Once you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps.", 
            "title": "Configuring Resources"
        }, 
        {
            "location": "/resources/register/#abcd-default-hooks", 
            "text": "ABCD Hooks  are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting  $PATH . If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following.  cd ~\ngit clone https://github.com/brain-life/abcd-spec  Then, add one of following to your ~/.bashrc", 
            "title": "ABCD Default Hooks"
        }, 
        {
            "location": "/resources/register/#for-pbs-cluster", 
            "text": "export PATH=~/abcd-spec/hooks/pbs:$PATH", 
            "title": "For PBS cluster"
        }, 
        {
            "location": "/resources/register/#for-slurm-cluster", 
            "text": "export PATH=~/abcd-spec/hooks/slurm:$PATH", 
            "title": "For Slurm cluster"
        }, 
        {
            "location": "/resources/register/#for-direct-execution-no-batch-submission-manager", 
            "text": "export PATH=~/abcd-spec/hooks/direct:$PATH", 
            "title": "For direct execution - no batch submission manager"
        }, 
        {
            "location": "/resources/register/#common-binaries", 
            "text": "Brainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed.   jq (command line json parser commonly used by Brainlife apps to parse config.json)  git (used to clone / update apps installed)  singularity (user level container execution engine)   For IU HPC resource, please feel free to use following ~/bin directory which contains jq  $ ~/.bashrc export   PATH = $PATH :/N/u/brlife/Carbonate/bin  For singularity, you can either install it on the system, or for most HPC systems you can simply add following in your  ~/.modules  file.  module   load   singularity   By default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc  export SINGULARITY_CACHEDIR=/N/dc2/scratch/ username /singularity-cachedir   Please replace   with your username, and make sure specified directories exists.", 
            "title": "Common Binaries"
        }, 
        {
            "location": "/resources/register/#other-env-parameters", 
            "text": "Depending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via  FREESURFER_LICENSE .  export FREESURFER_LICENSE= hayashis@iu.edu 29511 *CPmh9xvKQKHE FSg0ijTusqaQc", 
            "title": "Other ENV parameters"
        }, 
        {
            "location": "/resources/register/#enabling-apps", 
            "text": "Once you have registered and tested your resource, you can now enable apps to run on your resource.  Go back to the Brainlife's  resource settings page , and click the resource you have created. Under the services section, enter the git org/repo name (such as like  brain-life/app-life ) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK.  You can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife.  example", 
            "title": "Enabling Apps"
        }, 
        {
            "location": "/technical/arthitecture/", 
            "text": "Brainlife Architecture\n\n\n\n\nTODO..", 
            "title": "Architecture"
        }, 
        {
            "location": "/technical/arthitecture/#brainlife-architecture", 
            "text": "TODO..", 
            "title": "Brainlife Architecture"
        }, 
        {
            "location": "/technical/api/", 
            "text": "Brainlife API\n\n\nThis document describes some of Brainlife's microservices in case you might be interested in directly interfacing with them through APIs.\n\n\nWarehouse\n\n\nWarehouse is the main application responsible for bulk of Brainlife platform UI.\n\n\n\n\nWarehouse Github\n\n\nWarehouse API Doc\n\n\n\n\n\n\nNote\n\n\nBrainlife CLI interacts with Warehouse API to import / export datasets, query task status, and among other things. At the moment, we have a very limited CLI support, but please try using our CLI tool if you just want to interface with Brainlife Warehouse via commandline \nCLI Github\n\n\n\n\nAmaretti\n\n\nAmaretti\n is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information.\n\n\n\n\nAmaretti Doc\n\n\nAMaretti Github\n\n\nAmaretti API Doc\n\n\n\n\nAuthentication Service\n\n\n\n\nAuth Github\n\n\nAuth API Doc\n\n\n\n\nEvent Service\n\n\n\n\nEvent Github\n\n\nEvent API Doc\n\n\n\n\nProfile Service\n\n\n\n\nProfile Github\n\n\nProfile API Doc", 
            "title": "APIs"
        }, 
        {
            "location": "/technical/api/#brainlife-api", 
            "text": "This document describes some of Brainlife's microservices in case you might be interested in directly interfacing with them through APIs.", 
            "title": "Brainlife API"
        }, 
        {
            "location": "/technical/api/#warehouse", 
            "text": "Warehouse is the main application responsible for bulk of Brainlife platform UI.   Warehouse Github  Warehouse API Doc    Note  Brainlife CLI interacts with Warehouse API to import / export datasets, query task status, and among other things. At the moment, we have a very limited CLI support, but please try using our CLI tool if you just want to interface with Brainlife Warehouse via commandline  CLI Github", 
            "title": "Warehouse"
        }, 
        {
            "location": "/technical/api/#amaretti", 
            "text": "Amaretti  is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information.   Amaretti Doc  AMaretti Github  Amaretti API Doc", 
            "title": "Amaretti"
        }, 
        {
            "location": "/technical/api/#authentication-service", 
            "text": "Auth Github  Auth API Doc", 
            "title": "Authentication Service"
        }, 
        {
            "location": "/technical/api/#event-service", 
            "text": "Event Github  Event API Doc", 
            "title": "Event Service"
        }, 
        {
            "location": "/technical/api/#profile-service", 
            "text": "Profile Github  Profile API Doc", 
            "title": "Profile Service"
        }, 
        {
            "location": "/contact/", 
            "text": "Contact\n\n\nEmail\n\n\nGeneral \n \nBrarinlife \nbrlife@iu.edu\n\n\nContact PI \n \nFranco Pestilli \nfrakkopesto@gmail.com\n\n\nNewsletter Subscription\n\n\nBrainlife User\n\n\nSlack (brainlife.slack.com)\n\n\nJoin brainlife.slack.com\n \n\n\nBug Report / Feature Request\n\n\nBrainlife UI\n\n\nFor Brainlife App specific issue, please contact the developer of the App via the github issues.", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#contact", 
            "text": "", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#email", 
            "text": "General    Brarinlife  brlife@iu.edu  Contact PI    Franco Pestilli  frakkopesto@gmail.com", 
            "title": "Email"
        }, 
        {
            "location": "/contact/#newsletter-subscription", 
            "text": "Brainlife User", 
            "title": "Newsletter Subscription"
        }, 
        {
            "location": "/contact/#slack-brainlifeslackcom", 
            "text": "Join brainlife.slack.com", 
            "title": "Slack (brainlife.slack.com)"
        }, 
        {
            "location": "/contact/#bug-report-feature-request", 
            "text": "Brainlife UI  For Brainlife App specific issue, please contact the developer of the App via the github issues.", 
            "title": "Bug Report / Feature Request"
        }, 
        {
            "location": "/terms/", 
            "text": "Terms of use\n\n\nAs of April 9th, 2018\n\n\nBrainlife License Agreement\n\n\nWE RESERVE THE RIGHT TO CHANGE THIS AGREEMENT AT ANY TIME, BUT IF WE DO, WE WILL BRING IT TO YOUR ATTENTION BY PLACING A NOTICE ON BRAINLIFE.IO WEBSITE, BY SENDING YOU AN EMAIL, AND/OR BY SOME OTHER MEANS.\n\n\nLicense Grants\n\n\nSubject to full compliance with the terms of this Agreement and the Guidelines, Brainlife.io hereby grants you a limited, personal, non-sublicensable, non-transferable, nonexclusive license to use (a) our application programming interface, (b) Brainlife web user interface, and (c) related information and documentation that, in each case, you access (this will not be a download) from https://brainlife.io for the sole purpose of allowing you to host your datasets, perform data analysis and publish processing results through our platform. Further, subject to full compliance with the terms of this Agreement, we hereby grant you a limited, personal, non-sublicensable, non-transferable, nonexclusive right to access the features, services, and apps located at brainlife.io. The Service shall include, but not be limited to, any services Brainlife performs for you. Brainlife may also impose limits on certain features or restrict your access to parts or all of the Service. No rights or licenses are granted except as expressly and unambiguously set forth herein.\n\n\nTODO..\n\n\nTODO: Describe the type of datasets that users are allowed to upload.\nTODO: Describe User's and Brainlife's responsibility for detecting, and removal of inappropriate upload.\nTODO: Describe who will be held responsible in case of uploading datasets and the publication of inappropriate datasets\n\n\nTODO: Describe the right of App developers.\nTODO: Describe who will be held responsible in case of App / platform creating invalid data derivatives.\nTODO: Describe who will be held responsible in case of malicious Apps registered and ran on compute resources.\n\n\nRestrictions\n\n\nTODO..\n\n\nOwnership\n\n\nTODO: Describe who owns the datasets uploaded to Brainlife.\nTODO: Describe who owns the IP for registered Apps and how Brainlife may use it.\n\n\nService Availability\n\n\nTODO: Describe expected service level for Brainlife.io platform\nTODO: Describe expected service level for Brainlife slack channel, mailing list, and other communication mechanisms.\nTODO: Describe expected service level for resource owned by 3rd party users.\nTODO: Describe expected service level for Apps written by users\n\n\nFeedbacks\n\n\nIf, in the course of performing under this Agreement, Licensee provides Brainlife.io with any written comments, suggestions, or feedback regarding the Service (\u201cFeedback\u201d), you hereby grant Brainlife.io a non-exclusive, worldwide, royalty-free license to use and disclose the Feedback in any manner Brainlife.io chooses and, directly or indirectly through third parties, to display, perform, copy, have copied, make, have made, use, sell, offer to sell, and otherwise dispose of Brainlife.io's products (including any improvements or modifications thereof) embodying the Feedback in any manner and via any media the Brainlife.io chooses, but without reference to you as the source of the Feedback.\n\n\nLIMITATION OF LIABILITY\n\n\nUNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, INCLUDING, BUT NOT LIMITED TO, TORT, CONTRACT, NEGLIGENCE, STRICT LIABILITY, OR OTHERWISE, SHALL BRAINLIFE.IO OR ITS LICENSORS, SUPPLIERS OR RESELLERS BE LIABLE TO YOU OR ANY OTHER PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOST PROFITS, LOSS OF GOODWILL, OR DAMAGES RESULTING FROM LICENSEE\u2019S USE OF ANY SUBJECT MATTER COVERED BY THIS AGREEMENT. BRAINLIFE.IO'S LIABILITY FOR DAMAGES OF ANY KIND WHATSOEVER ARISING OUT OF THIS AGREEMENT SHALL BE LIMITED IN THE AGGREGATE TO AMOUNTS PAID (PLUS AMOUNTS PAYABLE, IN THE EVENT OF YOUR BREACH ONLY) TO BRAINLIFE.IO DURING THE TWELVE (12) MONTH PERIOD ENDING ON THE DATE THAT A CLAIM OR DEMAND IS FIRST ASSERTED. THE FOREGOING WILL NOT APPLY TO DAMAGES FOR BODILY INJURY THAT, UNDER APPLICABLE LAW, CANNOT BE SO LIMITED. THE FOREGOING LIMITATIONS SHALL APPLY EVEN IF YOU HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. SOME STATES DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THE ABOVE LIMITATION AND EXCLUSION MAY NOT APPLY TO YOU.", 
            "title": "Terms of Use"
        }, 
        {
            "location": "/terms/#terms-of-use", 
            "text": "As of April 9th, 2018", 
            "title": "Terms of use"
        }, 
        {
            "location": "/terms/#brainlife-license-agreement", 
            "text": "WE RESERVE THE RIGHT TO CHANGE THIS AGREEMENT AT ANY TIME, BUT IF WE DO, WE WILL BRING IT TO YOUR ATTENTION BY PLACING A NOTICE ON BRAINLIFE.IO WEBSITE, BY SENDING YOU AN EMAIL, AND/OR BY SOME OTHER MEANS.", 
            "title": "Brainlife License Agreement"
        }, 
        {
            "location": "/terms/#license-grants", 
            "text": "Subject to full compliance with the terms of this Agreement and the Guidelines, Brainlife.io hereby grants you a limited, personal, non-sublicensable, non-transferable, nonexclusive license to use (a) our application programming interface, (b) Brainlife web user interface, and (c) related information and documentation that, in each case, you access (this will not be a download) from https://brainlife.io for the sole purpose of allowing you to host your datasets, perform data analysis and publish processing results through our platform. Further, subject to full compliance with the terms of this Agreement, we hereby grant you a limited, personal, non-sublicensable, non-transferable, nonexclusive right to access the features, services, and apps located at brainlife.io. The Service shall include, but not be limited to, any services Brainlife performs for you. Brainlife may also impose limits on certain features or restrict your access to parts or all of the Service. No rights or licenses are granted except as expressly and unambiguously set forth herein.", 
            "title": "License Grants"
        }, 
        {
            "location": "/terms/#todo", 
            "text": "TODO: Describe the type of datasets that users are allowed to upload.\nTODO: Describe User's and Brainlife's responsibility for detecting, and removal of inappropriate upload.\nTODO: Describe who will be held responsible in case of uploading datasets and the publication of inappropriate datasets  TODO: Describe the right of App developers.\nTODO: Describe who will be held responsible in case of App / platform creating invalid data derivatives.\nTODO: Describe who will be held responsible in case of malicious Apps registered and ran on compute resources.", 
            "title": "TODO.."
        }, 
        {
            "location": "/terms/#restrictions", 
            "text": "TODO..", 
            "title": "Restrictions"
        }, 
        {
            "location": "/terms/#ownership", 
            "text": "TODO: Describe who owns the datasets uploaded to Brainlife.\nTODO: Describe who owns the IP for registered Apps and how Brainlife may use it.", 
            "title": "Ownership"
        }, 
        {
            "location": "/terms/#service-availability", 
            "text": "TODO: Describe expected service level for Brainlife.io platform\nTODO: Describe expected service level for Brainlife slack channel, mailing list, and other communication mechanisms.\nTODO: Describe expected service level for resource owned by 3rd party users.\nTODO: Describe expected service level for Apps written by users", 
            "title": "Service Availability"
        }, 
        {
            "location": "/terms/#feedbacks", 
            "text": "If, in the course of performing under this Agreement, Licensee provides Brainlife.io with any written comments, suggestions, or feedback regarding the Service (\u201cFeedback\u201d), you hereby grant Brainlife.io a non-exclusive, worldwide, royalty-free license to use and disclose the Feedback in any manner Brainlife.io chooses and, directly or indirectly through third parties, to display, perform, copy, have copied, make, have made, use, sell, offer to sell, and otherwise dispose of Brainlife.io's products (including any improvements or modifications thereof) embodying the Feedback in any manner and via any media the Brainlife.io chooses, but without reference to you as the source of the Feedback.", 
            "title": "Feedbacks"
        }, 
        {
            "location": "/terms/#limitation-of-liability", 
            "text": "UNDER NO CIRCUMSTANCES AND UNDER NO LEGAL THEORY, INCLUDING, BUT NOT LIMITED TO, TORT, CONTRACT, NEGLIGENCE, STRICT LIABILITY, OR OTHERWISE, SHALL BRAINLIFE.IO OR ITS LICENSORS, SUPPLIERS OR RESELLERS BE LIABLE TO YOU OR ANY OTHER PERSON FOR ANY INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOST PROFITS, LOSS OF GOODWILL, OR DAMAGES RESULTING FROM LICENSEE\u2019S USE OF ANY SUBJECT MATTER COVERED BY THIS AGREEMENT. BRAINLIFE.IO'S LIABILITY FOR DAMAGES OF ANY KIND WHATSOEVER ARISING OUT OF THIS AGREEMENT SHALL BE LIMITED IN THE AGGREGATE TO AMOUNTS PAID (PLUS AMOUNTS PAYABLE, IN THE EVENT OF YOUR BREACH ONLY) TO BRAINLIFE.IO DURING THE TWELVE (12) MONTH PERIOD ENDING ON THE DATE THAT A CLAIM OR DEMAND IS FIRST ASSERTED. THE FOREGOING WILL NOT APPLY TO DAMAGES FOR BODILY INJURY THAT, UNDER APPLICABLE LAW, CANNOT BE SO LIMITED. THE FOREGOING LIMITATIONS SHALL APPLY EVEN IF YOU HAVE BEEN INFORMED OF THE POSSIBILITY OF SUCH DAMAGES. SOME STATES DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THE ABOVE LIMITATION AND EXCLUSION MAY NOT APPLY TO YOU.", 
            "title": "LIMITATION OF LIABILITY"
        }, 
        {
            "location": "/releases/", 
            "text": "Upcoming\n\n\n First Production Release\n\n\n September 2018?\n\n\nTBD..\n\n\n Beta 2\n\n\n August 2018?\n\n\nTBD...\n\n\n Beta Release\n\n\n April 8th, 2018\n\n\n(warehouse v1.1.2) \n\n\nOur first official \nrelease\n! We now have most of our features functional and ready to be used by users outside our labs. We've developed an initial set of documentations and defined or identified standard operating protocols needed for sustainable operation of this platform.\n\n\nPast Releases", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#upcoming", 
            "text": "", 
            "title": "Upcoming"
        }, 
        {
            "location": "/releases/#first-production-release", 
            "text": "", 
            "title": "First Production Release"
        }, 
        {
            "location": "/releases/#september-2018", 
            "text": "TBD..", 
            "title": "September 2018?"
        }, 
        {
            "location": "/releases/#beta-2", 
            "text": "", 
            "title": "Beta 2"
        }, 
        {
            "location": "/releases/#august-2018", 
            "text": "TBD...", 
            "title": "August 2018?"
        }, 
        {
            "location": "/releases/#beta-release", 
            "text": "", 
            "title": "Beta Release"
        }, 
        {
            "location": "/releases/#april-8th-2018", 
            "text": "(warehouse v1.1.2)   Our first official  release ! We now have most of our features functional and ready to be used by users outside our labs. We've developed an initial set of documentations and defined or identified standard operating protocols needed for sustainable operation of this platform.", 
            "title": "April 8th, 2018"
        }, 
        {
            "location": "/releases/#past-releases", 
            "text": "", 
            "title": "Past Releases"
        }
    ]
}