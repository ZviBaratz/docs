{
    "docs": [
        {
            "location": "/", 
            "text": "Brainlife Documentation\n\n\nWelcome to the Brainlife documentation portal! This site contains up-to-date information about the Brainlife platform. The information is organized based in sections focusing on our different target communities. We envision three primary groups of Brainlife users:\n\n\nEnd Users.\n\n\nNeuroscientists, Psychologists and other investigators interested in using Brainlife to run Apps on public or private data to generate scientific insights and publish their results.\n\n\nApp Developers.\n\n\nComputational neuroscientists, computer scientists and statisticians interested in using Brainlife to share their analysis methods and algorithms by developing Apps and making them available on the platform.\n\n\nResource Providers.\n\n\nClusters managers, and cyberinfrastructure managers interested in providing a resource where Brainlife Apps and users can run.", 
            "title": "Home"
        }, 
        {
            "location": "/#brainlife-documentation", 
            "text": "Welcome to the Brainlife documentation portal! This site contains up-to-date information about the Brainlife platform. The information is organized based in sections focusing on our different target communities. We envision three primary groups of Brainlife users:", 
            "title": "Brainlife Documentation"
        }, 
        {
            "location": "/#end-users", 
            "text": "Neuroscientists, Psychologists and other investigators interested in using Brainlife to run Apps on public or private data to generate scientific insights and publish their results.", 
            "title": "End Users."
        }, 
        {
            "location": "/#app-developers", 
            "text": "Computational neuroscientists, computer scientists and statisticians interested in using Brainlife to share their analysis methods and algorithms by developing Apps and making them available on the platform.", 
            "title": "App Developers."
        }, 
        {
            "location": "/#resource-providers", 
            "text": "Clusters managers, and cyberinfrastructure managers interested in providing a resource where Brainlife Apps and users can run.", 
            "title": "Resource Providers."
        }, 
        {
            "location": "/about/", 
            "text": "What is Brainlife?\n\n\nA modern platform that uses both cloud and high-performance computing systems to support reproducible analyses, data management and visualization. Brainlife also provide unique mechanisms to publish all research assets associated with a scientific project (data and analyses) embedded in a cloud computing environment and referenced by a single digital-object-identifier (DOI). The platform is unique because of its focus on supporting scientific reproducibility beyond open code and open data, by providing fundamental smart mechanisms for what we refer to as \u201cOpen Services.\u201d \n\n\nBrainlife Apps\n\n\nBrainlife uses Apps to analyze data. \nApps\n are small programs, small modules or compute units, that can b made part of a larger series of steps in a full data analysis workflow for a publication. Brainlife Apps are meant to do a small but meaningful step in a longer analysis pipeline. Apps are modules and the platform allows users to develop, use, combine, and reuse Apps to simply build complex pipelines for customized brain data analyses. Most Apps indeed do only one thing, they process data in a specific way and are meant to perform a small set of operations and handle small sets of data; they do one thing, they do it well.\n\n\nApps can be developed and published on the Brainlife platform by anyone. App developers can be computational neuroscientists, cognitive neuroscientists, but also computer scientists or engineers. Apps are snippets of code implementing algorithms or analyses. By follow a few easy steps code to publish the code on the Brainlife platform as an App. Publishing code as an App allows scientists to use the data and computational resources available through Brainlife. Apps published on Brainlife can be used privately or shared publicly with the platform users community. \n\n\nBrainlife Datatypes\n\n\nBrainlife Apps communicate via Brainlife \u201cDatatypes.\u201d A Datatype defines expected list of file names / directory structure that Apps that uses that datatype to generate as output data. It can also be used as input dataset by other App that expects the same datatype. Datatypes allow multiple apps developed by independent developers to be joined together to form a \nworkflows\n. The Brainlife Datatypes are similar to BIDS in concept, except they are maintained by individual developers participating in a specific datatype, and they mainly concerns \ndata derivatives\n. Brainlife allows user to download datasets in BIDS structure.\n\n\nBrainlife Clouds (Compute Resource)\n\n\nBrainlife allows orchestration of data and computing across mix systems of Clouds and high-performance computing clusters (HPC). We refer to both HCP and Clouds as Brainlife Clouds, technically not precise terminology, but simple. Less is more. Brainlife orchestration allows users and compute resource providers to register a compute resource to make it available publicly to the full Brainlife users community or privately to a subset of users. Brainlife has smart mechanisms that allow Apps to run on different resources, privately or publicly. With a traditional approach of running an entire workflow on a small set of resources, some part of the workflow might not be optimal to run on a given resource, Brainlife allows App Developers to identify resources that best fit their Apps and score the compute resources available on the Brainlife platform depending on how well they work with the App they develop.\n\n\nBrainlife Viz (Cloud Visualization)\n\n\nBrainlife has mechanisms to run brain data visualization on Brainlife Clouds. Data visualization is meant to provide users with an agile way to get feedback on the quality of the results generatd by Apps and pipelines. Visualization is impleelemnted with smart cloud-side methods, so that data are not moved from the Clould to the users computer. THis increases seecurity and improves management. Brainlife Viz allows running major software for data visualization familiar to the neuroscience community (e.g., FreeView, FSLview, MRview), and running GPU rendered visualizations. Visualization Apps can be openly contributed by developers tot hee Brainlife platform.", 
            "title": "About"
        }, 
        {
            "location": "/about/#what-is-brainlife", 
            "text": "A modern platform that uses both cloud and high-performance computing systems to support reproducible analyses, data management and visualization. Brainlife also provide unique mechanisms to publish all research assets associated with a scientific project (data and analyses) embedded in a cloud computing environment and referenced by a single digital-object-identifier (DOI). The platform is unique because of its focus on supporting scientific reproducibility beyond open code and open data, by providing fundamental smart mechanisms for what we refer to as \u201cOpen Services.\u201d", 
            "title": "What is Brainlife?"
        }, 
        {
            "location": "/about/#brainlife-apps", 
            "text": "Brainlife uses Apps to analyze data.  Apps  are small programs, small modules or compute units, that can b made part of a larger series of steps in a full data analysis workflow for a publication. Brainlife Apps are meant to do a small but meaningful step in a longer analysis pipeline. Apps are modules and the platform allows users to develop, use, combine, and reuse Apps to simply build complex pipelines for customized brain data analyses. Most Apps indeed do only one thing, they process data in a specific way and are meant to perform a small set of operations and handle small sets of data; they do one thing, they do it well.  Apps can be developed and published on the Brainlife platform by anyone. App developers can be computational neuroscientists, cognitive neuroscientists, but also computer scientists or engineers. Apps are snippets of code implementing algorithms or analyses. By follow a few easy steps code to publish the code on the Brainlife platform as an App. Publishing code as an App allows scientists to use the data and computational resources available through Brainlife. Apps published on Brainlife can be used privately or shared publicly with the platform users community.", 
            "title": "Brainlife Apps"
        }, 
        {
            "location": "/about/#brainlife-datatypes", 
            "text": "Brainlife Apps communicate via Brainlife \u201cDatatypes.\u201d A Datatype defines expected list of file names / directory structure that Apps that uses that datatype to generate as output data. It can also be used as input dataset by other App that expects the same datatype. Datatypes allow multiple apps developed by independent developers to be joined together to form a  workflows . The Brainlife Datatypes are similar to BIDS in concept, except they are maintained by individual developers participating in a specific datatype, and they mainly concerns  data derivatives . Brainlife allows user to download datasets in BIDS structure.", 
            "title": "Brainlife Datatypes"
        }, 
        {
            "location": "/about/#brainlife-clouds-compute-resource", 
            "text": "Brainlife allows orchestration of data and computing across mix systems of Clouds and high-performance computing clusters (HPC). We refer to both HCP and Clouds as Brainlife Clouds, technically not precise terminology, but simple. Less is more. Brainlife orchestration allows users and compute resource providers to register a compute resource to make it available publicly to the full Brainlife users community or privately to a subset of users. Brainlife has smart mechanisms that allow Apps to run on different resources, privately or publicly. With a traditional approach of running an entire workflow on a small set of resources, some part of the workflow might not be optimal to run on a given resource, Brainlife allows App Developers to identify resources that best fit their Apps and score the compute resources available on the Brainlife platform depending on how well they work with the App they develop.", 
            "title": "Brainlife Clouds (Compute Resource)"
        }, 
        {
            "location": "/about/#brainlife-viz-cloud-visualization", 
            "text": "Brainlife has mechanisms to run brain data visualization on Brainlife Clouds. Data visualization is meant to provide users with an agile way to get feedback on the quality of the results generatd by Apps and pipelines. Visualization is impleelemnted with smart cloud-side methods, so that data are not moved from the Clould to the users computer. THis increases seecurity and improves management. Brainlife Viz allows running major software for data visualization familiar to the neuroscience community (e.g., FreeView, FSLview, MRview), and running GPU rendered visualizations. Visualization Apps can be openly contributed by developers tot hee Brainlife platform.", 
            "title": "Brainlife Viz (Cloud Visualization)"
        }, 
        {
            "location": "/user/tutorial/", 
            "text": "Tutorial\n\n\nThis tutorial will guide you through the following functionality of Brainlife. \n\n\n\n\nSigning up\n\n\nCreating new projects and uploading datasets\n\n\nLaunching visualizers to visualize your data\n\n\nRunning processes on datasets and archiving results.\n\n\n\n\nFor a more high-level overview of Brainlife, see \nAbout Page\n.\n\n\nSign Up\n\n\nIf you have not registered on Brainlife.io yet, please do so by visiting The \nAuthentication Page\n and clicking on a preferred authentication method: Google, ORCID, Github, or through your institution.\n\n\n\n\nWarning\n\n\nIf you register through a 3rd party authenticator, please use the same authenticator each time you login, or you will end up creating multiple Brainlife accounts. \n\n\n\n\nIf you would like to setup a dedicated username/password for Brainlife, please click the \"Sign Up\" link. You will be asked to confirm your email address once you register.\n\n\n\n\nNote\n\n\nYou can associate multiple authenticators to your account once you register by going to Settings / Account, Connected Accounts, and click \"Connect\" next to various 3rd party authenticators. \n\n\n\n\nCreate Project\n\n\nOnce you login, you will land on the Brainlife Apps page. \n\n\nBefore we can start using Brainlife, you will need to create a new project. \n\n\nClick on \nProject\n button on the left hand side menu, then click a plus side button at the bottom of the project list.\n\n\n\n\n\n\nNote\n\n\nProject is where you can organize your datasets, do data processing, and share datasets with your project members. For more information about project, please read \nproject page\n\n\n\n\nEnter any \nname\n and \ndescription\n, and leave everything else default. Click \nSubmit\n. \n\n\nCongratulations! You just created your first private project!\n\n\nUpload Dataset\n\n\nNow, let's upload some test datasets. Open the \nDatasets\n tab.\n\n\n\n\nNote\n\n\nIn Brainlife, datasets are sets of files/directories for specific modality or data derivatives for a specific subject. All data processing is done at the subject level. Datasets are immutable; you can only modify the metadata, but not the data files once you create them.\n\n\n\n\nBrainlife has 2 kinds of data storage. \n\n\n\n\n\n\nDatasets Archive\n\n\nThe datasets tab you are seeing now shows the current content of your dataset archive. Datasets under this tab are stored in our object storage permanently (and some are backed up to our tape archives also).\n\n\n\n\n\n\nProcess Scratch Space\n\n\nYou cannot directly use archived datasets to run Apps. To run Apps, datasets will be automatically staged out of your archive and transferred to Brainlife's scratch space and on various compute resources where Apps are executed.\n\n\nDatasets on process scratch space will be automatically removed within 25 days or sooner. If you have any output datasets that you'd like to keep permanently, you will need to archive them back to the Datasets Archive. \n\n\n\n\n\n\nNow, click \nplus button\n at the bottom of the screen to open the dataset upload dialog.\n\n\n\n\nSelect Datatype that you'd like to upload (currently limited to t1/t2 and dwi) and upload your dataset. \n\n\n\n\nNote\n\n\nIf you don't have any data to upload, you can skip this step and you can use pre-uploaded datasets from various public projects.\n\n\n\n\nThe Upload form will run a server side validation and data normalization service. You can check the results from this step. If everything looks good, click \nArchive\n.\n\n\n\n\nOnce uploaded, you should see a new dialog showing details about your new datasets. All archived datasets are immutable (read-only), but you can copy them between projects or make changes to the metadata if necessary (description, tags, etc..).\n\n\n\n\n\n\nNote\n\n\nStored in\n field shows where your dataset is archived. For a large dataset, it might take a while for it to be archived. Please give it a few minutes.\n\n\n\n\nVisualize Dataset\n\n\nAll visualization on Brainlife is donee Cloud side. THis means that data seets do not have to be moved from their storage location to the user browser. Any datasets stored in Brainlife can be visualized using Web-based and Native (via \nWeb-VNC\n) visualization Apps registered for specific datatypes. To launch a visualization program, click on the visualizer icon (\n) at the top of dataset dialog.\n\n\nFor example, \nneuro/anat/t1\n datasets can be visualized by the following set of Visualization Apps.\n\n\n\n\n\n\nNote\n\n\nSimilar to \nApps\n, developers can develop and contribute new visualization Apps to run on Brainlife. If you are developing visualization Apps, or have Apps that you'd like us to add, please contact us at \nbrlife@iu.edu\n.\n\n\n\n\nClick any of the visualization Apps that you'd like to launch to visualize your data. \n\n\nDownloading BIDS\n\n\nYou can search/select and bulk download datasets. On the dataset table, select the datasets you'd like to download by clicking on the check box, then click \nDownload (BIDS)\n button.\n\n\n\n\nBrainlife will stage selected datasets, organize them into a \nBIDS structure\n, and let you download the whole structure as a single tar ball. Once it's ready, click \nDownload\n.\n\n\n\n\nNote\n\n\nAt the moment, all Brainlife datasets will simply be stored under \n/derivatives\n directory regardless of the datatype.\n\n\n\n\nApps\n\n\nBefore we proceed to \nProcess\n tab, let's take a quick detour and visit the \nApps\n page.\n\n\n\n\nThe \nApps\n page shows all Brainlife Apps that are publicly available that you can execute on resources and datasets that you have access to. Please take a look and see what type of Apps are currently available. You can click on each tile to see more details.\n\n\nOn Brainlife, Apps are normally small programs that perform a specific data processing. Although we have a few Apps that behave more like a typical \npipeline\n or \nworkflow\n (including pre/post processing, data analysis, reporting, etc..), most Brainlife Apps should only do one thing, and one thing well. Please see \nabout\n for more details.\n\n\nDatatypes\n\n\nBrainlife Apps exchange data through \ndatatypes\n. Developers involved with interoperating input/output datasets should discuss and agree on the set of files/directory structure and their semantics, and register a new datatype by submitting an issue on \ndatatypes github repo\n.\n\n\n\n\nVarious colored boxes show the input and output datatypes. For example, the above image shows that this app will take \ndwi\n input dataset, and generate another \ndwi\n dataset with a datatype tag of \"masked\", and also output another dataset of a datatype \nmask\n. \n\n\nFor more information on datatype, please visit \ndatatypes page\n\n\n\n\nData Processing\n\n\nNow that we know what \nApps\n are, we can go back to your private project, to practice data processing. Open the \nProcesses\n tab. You should see an empty page as you do not have any processes yet. On Brainlife, \nProcess\n is where you can submit a group of tasks/Apps that can share input/output datasets. \n\n\nWe will create a new process by clicking the \n+\n button at the right bottom corner of the page. Enter any name you would like for your process. You should see a screen that looks like this now.\n\n\n\n\nTo process data, you first need to stage any dataset from our archive to your process. Each process can only process data that is either staged or generated by other Apps. Click \nStage New Dataset\n button. On the Select Datasets dialog, please select \nNKI (Rockland Sample)\n project, and select any \nanat/t1w\n dataset.\n\n\n\n\nClick \nOK\n to stage. You should see a box showing \"Staging Datasets\" with selected datasets. While it is staging your data, please submit your first App. Click \nSubmit New App\n button. A dialog should show up with a list of Apps that you can submit using your \nanat/t1w\n dataset. Brainlife allows you to select only the Apps where you have all required input datasets.\n\n\nPlease run the \nACPC alignment with ART\n App on your data. \nACPC alignment\n is a common alignment tool used to re-orient/re-position the Brain image in common orientation suited for further image analysis. \n\n\nFind and click the App, then make sure that Brainlife has automatically selected your staged data as input. Leave other options default. Click \nSubmit\n. Brainlife should now find the most appropriate resource to run this App, and transfer data to the resource and submit it to the local batch scheduler.\n\n\nOnce started, a task should take a few minutes to run. Once completed, you should see a screen that looks like this.\n\n\n\n\nYou can browse/download any output files as they are generated under \nRaw Output\n section. Once completed successfully, you can launch various visualization tools by clicking the \n button next to the Output section. Open \nVolume Viewer\n on both the original input data and the ACPC alignment output data and see how this algorithm has re-oriented your data. \n\n\nBelow is the before/after view. Can you see that bottom one is better aligned/re-positioned at ACPC line? For more info on ACPC alignment see \nhere\n\n\n\n\nNow that you have finished running ACPC alignment, you will be able to submit a few new Apps under \nSubmit New App\n dialog that you couldn't submit before. Please feel free to submit other Apps or stage more datasets. \n\n\n\n\nHint\n\n\nIf you are not sure which datasets to stage, please see \nApps\n page and find which datatype each App requires to run.\n\n\n\n\nArchiving\n\n\nSo far, you have staged datasets, submitted an App that generated data derivatives, and visualized them.\n\n\nNow, it is important to note that all processes are meant to be temporary and Brainlife will remove processes within 25 days of data generation. If you would like to permanently keep the output datasets you just generated, you will need to \narchive\n them by clicking on \n button next to the Output section. You can edit any metadata and description, and click the \nArchive\n button to archive it.\n\n\nAfter you archive your data, open the \nDatasets\n tab and make sure that your dataset is listed there. You can click on the dataset record to see more details.\n\n\nDatasets\n\n\nAs you submit more Apps and generate datasets from them, it becomes harder to keep up with how a given dataset was generated. Brainlife keeps track of a record of how a given dataset was generated all the way from the original input dataset (called \ndata provenance\n).\n\n\nUnder \nDatasets\n tab, select any dataset you have generated and look under \nProvenance\n tab.\n\n\n\n\nThe green boxes are the input datasets (uploaded to Brainlife from outside) and the white boxes are the Apps run to generate the data derivatives. You can pan/zoom the diagram, or re-layout some items by dragging/dropping.\n\n\nWhat's Next\n\n\nYou should now be familiar with basic functionalities of Brainlife. Please take a look at other pages for more information. For example, if you'd like to write your app and register on Brainlife, please take a look at \nApp Developer Guide\n. If you'd like to learn how to bulk process multiple subjects, please take a look at \nPipeline\n.\n\n\nPlease let us know how we can improve this tutorial, or send us pull requests with your edits.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/user/tutorial/#tutorial", 
            "text": "This tutorial will guide you through the following functionality of Brainlife.    Signing up  Creating new projects and uploading datasets  Launching visualizers to visualize your data  Running processes on datasets and archiving results.   For a more high-level overview of Brainlife, see  About Page .", 
            "title": "Tutorial"
        }, 
        {
            "location": "/user/tutorial/#sign-up", 
            "text": "If you have not registered on Brainlife.io yet, please do so by visiting The  Authentication Page  and clicking on a preferred authentication method: Google, ORCID, Github, or through your institution.   Warning  If you register through a 3rd party authenticator, please use the same authenticator each time you login, or you will end up creating multiple Brainlife accounts.    If you would like to setup a dedicated username/password for Brainlife, please click the \"Sign Up\" link. You will be asked to confirm your email address once you register.   Note  You can associate multiple authenticators to your account once you register by going to Settings / Account, Connected Accounts, and click \"Connect\" next to various 3rd party authenticators.", 
            "title": "Sign Up"
        }, 
        {
            "location": "/user/tutorial/#create-project", 
            "text": "Once you login, you will land on the Brainlife Apps page.   Before we can start using Brainlife, you will need to create a new project.   Click on  Project  button on the left hand side menu, then click a plus side button at the bottom of the project list.    Note  Project is where you can organize your datasets, do data processing, and share datasets with your project members. For more information about project, please read  project page   Enter any  name  and  description , and leave everything else default. Click  Submit .   Congratulations! You just created your first private project!", 
            "title": "Create Project"
        }, 
        {
            "location": "/user/tutorial/#upload-dataset", 
            "text": "Now, let's upload some test datasets. Open the  Datasets  tab.   Note  In Brainlife, datasets are sets of files/directories for specific modality or data derivatives for a specific subject. All data processing is done at the subject level. Datasets are immutable; you can only modify the metadata, but not the data files once you create them.   Brainlife has 2 kinds of data storage.     Datasets Archive  The datasets tab you are seeing now shows the current content of your dataset archive. Datasets under this tab are stored in our object storage permanently (and some are backed up to our tape archives also).    Process Scratch Space  You cannot directly use archived datasets to run Apps. To run Apps, datasets will be automatically staged out of your archive and transferred to Brainlife's scratch space and on various compute resources where Apps are executed.  Datasets on process scratch space will be automatically removed within 25 days or sooner. If you have any output datasets that you'd like to keep permanently, you will need to archive them back to the Datasets Archive.     Now, click  plus button  at the bottom of the screen to open the dataset upload dialog.   Select Datatype that you'd like to upload (currently limited to t1/t2 and dwi) and upload your dataset.    Note  If you don't have any data to upload, you can skip this step and you can use pre-uploaded datasets from various public projects.   The Upload form will run a server side validation and data normalization service. You can check the results from this step. If everything looks good, click  Archive .   Once uploaded, you should see a new dialog showing details about your new datasets. All archived datasets are immutable (read-only), but you can copy them between projects or make changes to the metadata if necessary (description, tags, etc..).    Note  Stored in  field shows where your dataset is archived. For a large dataset, it might take a while for it to be archived. Please give it a few minutes.", 
            "title": "Upload Dataset"
        }, 
        {
            "location": "/user/tutorial/#visualize-dataset", 
            "text": "All visualization on Brainlife is donee Cloud side. THis means that data seets do not have to be moved from their storage location to the user browser. Any datasets stored in Brainlife can be visualized using Web-based and Native (via  Web-VNC ) visualization Apps registered for specific datatypes. To launch a visualization program, click on the visualizer icon ( ) at the top of dataset dialog.  For example,  neuro/anat/t1  datasets can be visualized by the following set of Visualization Apps.    Note  Similar to  Apps , developers can develop and contribute new visualization Apps to run on Brainlife. If you are developing visualization Apps, or have Apps that you'd like us to add, please contact us at  brlife@iu.edu .   Click any of the visualization Apps that you'd like to launch to visualize your data.", 
            "title": "Visualize Dataset"
        }, 
        {
            "location": "/user/tutorial/#downloading-bids", 
            "text": "You can search/select and bulk download datasets. On the dataset table, select the datasets you'd like to download by clicking on the check box, then click  Download (BIDS)  button.   Brainlife will stage selected datasets, organize them into a  BIDS structure , and let you download the whole structure as a single tar ball. Once it's ready, click  Download .   Note  At the moment, all Brainlife datasets will simply be stored under  /derivatives  directory regardless of the datatype.", 
            "title": "Downloading BIDS"
        }, 
        {
            "location": "/user/tutorial/#apps", 
            "text": "Before we proceed to  Process  tab, let's take a quick detour and visit the  Apps  page.   The  Apps  page shows all Brainlife Apps that are publicly available that you can execute on resources and datasets that you have access to. Please take a look and see what type of Apps are currently available. You can click on each tile to see more details.  On Brainlife, Apps are normally small programs that perform a specific data processing. Although we have a few Apps that behave more like a typical  pipeline  or  workflow  (including pre/post processing, data analysis, reporting, etc..), most Brainlife Apps should only do one thing, and one thing well. Please see  about  for more details.", 
            "title": "Apps"
        }, 
        {
            "location": "/user/tutorial/#datatypes", 
            "text": "Brainlife Apps exchange data through  datatypes . Developers involved with interoperating input/output datasets should discuss and agree on the set of files/directory structure and their semantics, and register a new datatype by submitting an issue on  datatypes github repo .   Various colored boxes show the input and output datatypes. For example, the above image shows that this app will take  dwi  input dataset, and generate another  dwi  dataset with a datatype tag of \"masked\", and also output another dataset of a datatype  mask .   For more information on datatype, please visit  datatypes page", 
            "title": "Datatypes"
        }, 
        {
            "location": "/user/tutorial/#data-processing", 
            "text": "Now that we know what  Apps  are, we can go back to your private project, to practice data processing. Open the  Processes  tab. You should see an empty page as you do not have any processes yet. On Brainlife,  Process  is where you can submit a group of tasks/Apps that can share input/output datasets.   We will create a new process by clicking the  +  button at the right bottom corner of the page. Enter any name you would like for your process. You should see a screen that looks like this now.   To process data, you first need to stage any dataset from our archive to your process. Each process can only process data that is either staged or generated by other Apps. Click  Stage New Dataset  button. On the Select Datasets dialog, please select  NKI (Rockland Sample)  project, and select any  anat/t1w  dataset.   Click  OK  to stage. You should see a box showing \"Staging Datasets\" with selected datasets. While it is staging your data, please submit your first App. Click  Submit New App  button. A dialog should show up with a list of Apps that you can submit using your  anat/t1w  dataset. Brainlife allows you to select only the Apps where you have all required input datasets.  Please run the  ACPC alignment with ART  App on your data.  ACPC alignment  is a common alignment tool used to re-orient/re-position the Brain image in common orientation suited for further image analysis.   Find and click the App, then make sure that Brainlife has automatically selected your staged data as input. Leave other options default. Click  Submit . Brainlife should now find the most appropriate resource to run this App, and transfer data to the resource and submit it to the local batch scheduler.  Once started, a task should take a few minutes to run. Once completed, you should see a screen that looks like this.   You can browse/download any output files as they are generated under  Raw Output  section. Once completed successfully, you can launch various visualization tools by clicking the   button next to the Output section. Open  Volume Viewer  on both the original input data and the ACPC alignment output data and see how this algorithm has re-oriented your data.   Below is the before/after view. Can you see that bottom one is better aligned/re-positioned at ACPC line? For more info on ACPC alignment see  here   Now that you have finished running ACPC alignment, you will be able to submit a few new Apps under  Submit New App  dialog that you couldn't submit before. Please feel free to submit other Apps or stage more datasets.    Hint  If you are not sure which datasets to stage, please see  Apps  page and find which datatype each App requires to run.", 
            "title": "Data Processing"
        }, 
        {
            "location": "/user/tutorial/#archiving", 
            "text": "So far, you have staged datasets, submitted an App that generated data derivatives, and visualized them.  Now, it is important to note that all processes are meant to be temporary and Brainlife will remove processes within 25 days of data generation. If you would like to permanently keep the output datasets you just generated, you will need to  archive  them by clicking on   button next to the Output section. You can edit any metadata and description, and click the  Archive  button to archive it.  After you archive your data, open the  Datasets  tab and make sure that your dataset is listed there. You can click on the dataset record to see more details.", 
            "title": "Archiving"
        }, 
        {
            "location": "/user/tutorial/#datasets", 
            "text": "As you submit more Apps and generate datasets from them, it becomes harder to keep up with how a given dataset was generated. Brainlife keeps track of a record of how a given dataset was generated all the way from the original input dataset (called  data provenance ).  Under  Datasets  tab, select any dataset you have generated and look under  Provenance  tab.   The green boxes are the input datasets (uploaded to Brainlife from outside) and the white boxes are the Apps run to generate the data derivatives. You can pan/zoom the diagram, or re-layout some items by dragging/dropping.", 
            "title": "Datasets"
        }, 
        {
            "location": "/user/tutorial/#whats-next", 
            "text": "You should now be familiar with basic functionalities of Brainlife. Please take a look at other pages for more information. For example, if you'd like to write your app and register on Brainlife, please take a look at  App Developer Guide . If you'd like to learn how to bulk process multiple subjects, please take a look at  Pipeline .  Please let us know how we can improve this tutorial, or send us pull requests with your edits.", 
            "title": "What's Next"
        }, 
        {
            "location": "/user/project/", 
            "text": "Project\n\n\nProject\n is where you can organize your datasets, do data processing, and share them with your project members.\n\n\nEach project can have a specific set of users for \nadmins\n, \nmembers\n, or \nguests\n groups. \nAdmin\n can update the project access policy and edit various groups for the project. \nMembers\n have read/write access to the datasets but cannot make changes to the group members. \nGuests\n have read access to datasets and processes, but cannot modify them.\n\n\nBrainlife currently supports the following project access policies.\n\n\n\n\n\n\nPublic\n\n\nPublic project allows anyone registered on Brainlife (not just a member of the project) to download and use archived datasets to process data. All users can view all public projects on Brainlife under project list.\n\n\n\n\n\n\nPrivate\n\n\nPrivate project allows only the project members to download or use archived datasets to run Apps. Only the admins and project members can find the project under project list.\n\n\n\n\n\n\n\n\nNote\n\n\nList project summary for all users\n \n\n\nIf you'd like to keep your project private while allowing other users to know its existance through the project list, please check this check box. You can solicit other users to join the project for full access or become guest users to have read access to the datasets.", 
            "title": "Projects"
        }, 
        {
            "location": "/user/project/#project", 
            "text": "Project  is where you can organize your datasets, do data processing, and share them with your project members.  Each project can have a specific set of users for  admins ,  members , or  guests  groups.  Admin  can update the project access policy and edit various groups for the project.  Members  have read/write access to the datasets but cannot make changes to the group members.  Guests  have read access to datasets and processes, but cannot modify them.  Brainlife currently supports the following project access policies.    Public  Public project allows anyone registered on Brainlife (not just a member of the project) to download and use archived datasets to process data. All users can view all public projects on Brainlife under project list.    Private  Private project allows only the project members to download or use archived datasets to run Apps. Only the admins and project members can find the project under project list.     Note  List project summary for all users    If you'd like to keep your project private while allowing other users to know its existance through the project list, please check this check box. You can solicit other users to join the project for full access or become guest users to have read access to the datasets.", 
            "title": "Project"
        }, 
        {
            "location": "/user/datatypes/", 
            "text": "Datatypes\n\n\n\n\nPlease read \nTutorial / Datatypes\n first.\n\n\n\n\nBrainlife Apps exchange data through \ndatatypes\n.\n\n\n\n\nEach datatype consists of \nname\n, \ndescription\n, and a list of files (or directories) that define the overall structure of the datatype. \n\n\nFor example, the following is a datatype definition for \nneuro/life\n datatype.\n\n\n{\n\n    \nname\n \n:\n \nneuro/life\n,\n\n    \ndesc\n \n:\n \nLiFE Output (fe structure)\n,\n\n    \nfiles\n \n:\n \n[\n \n        \n{\n\n            \nid\n \n:\n \nfe\n,\n\n            \nfilename\n \n:\n \noutput_fe.mat\n,\n\n            \ndesc\n \n:\n \nFE structure\n,\n\n            \next\n \n:\n \n.mat\n,\n\n            \nrequired\n \n:\n \ntrue\n\n        \n},\n \n        \n{\n\n            \nid\n \n:\n \nlife_results\n,\n\n            \nfilename\n \n:\n \nlife_results.json\n,\n\n            \nrequired\n \n:\n \ntrue\n\n        \n},\n \n        \n{\n\n            \nid\n \n:\n \ntracts\n,\n\n            \ndirname\n \n:\n \ntracts\n,\n\n            \nrequired\n \n:\n \ntrue\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\nFor this example datatype, \nbrain-life/app-life\n App generates a dataset with this datatype, and other Apps that want to use \nneuro/life\n output can request to have those files made available to their Apps by registering them on Brainlife App registration form. (Please read \nRegistering App\n page for more info). The actual content/semantics of each file are up to developers exchanging the dataset to decide, and it should be well documented as part of the datatype registration process.\n\n\nPlease see other datatypes defined in \nbrain-life/datatypes\n.\n\n\nBrainlife datatype might sound similar to BIDS specification, but it differs in following areas.\n\n\n\n\n\n\nBrainlife datatypes mainly concern data derivatives generated by Apps and used only by Apps exchanging those datasets. They are only used within Brainlife platform and not meant to become standards for that particular data format.\n\n\n\n\n\n\nBrainlife datatypes are defined by App developers involved in exchanging input/output datasets, not by Brainlife platform developers. App developers should discuss and agree on the structure of the datatype and what each file means. They can submit an issue on \nbrain-life/datatypes\n and/or a pull request containing the list of files/directories to be registered on Brainlife. Once Brainlife team incorporate the PR, you will be able to use the new datatype for your App.\n\n\n\n\n\n\nDatatype Tags\n\n\nSometimes you want to be more specific about the type of dataset for a particular datatype. For example, \nneuro/anat/t1w\n could be ACPC aligned or not, \nneuro/dwi\n could be single-shell or multi-shell, etc. Brainlife allows you to adds specificity to each datatype through \ndatatype tags\n. \n\n\n\n\nWarning\n\n\nPlease don't confuse \nDatatype tag\n with \nDataset tag\n. \"Dataset tag\" is a tag that user can freely edit under dataset dialog to allow for easier searching or bulk processing of datasets with specific tags. \"Datatype tag\", on the other hand, can only be set by App developer and it is a part of datatype and can not be modified once dataset is created.\n\n\n\n\nIt is important to note that, \ndatatype tag\n should always be used to add specificity to datasets, but not to generalize it. For example, we don't have \"multi-shell\" datatype tags because \nneuro/dwi\n is by default a \"multi-shell\" data; it is perfectly valid to have different b-values in \ndwi.bvals\n file. \"single-shell\" is a special case for \nneuro/dwi\n datatype where b-values happens to be all same number. Therefore, we introduce \"single-shell\" tag to describe such dwi dataset.\n\n\nBy always using datatype tags to add specificity, Brainlife can correctly identify which datasets can be used for which Apps by examining dataset's datatype tags and App's input dataset tags. \n\n\n\n\nHint\n\n\nPlease consult the #datatype slack channel on Brainlife slack team for any datatype related questions.", 
            "title": "Datatypes"
        }, 
        {
            "location": "/user/datatypes/#datatypes", 
            "text": "Please read  Tutorial / Datatypes  first.   Brainlife Apps exchange data through  datatypes .   Each datatype consists of  name ,  description , and a list of files (or directories) that define the overall structure of the datatype.   For example, the following is a datatype definition for  neuro/life  datatype.  { \n     name   :   neuro/life , \n     desc   :   LiFE Output (fe structure) , \n     files   :   [  \n         { \n             id   :   fe , \n             filename   :   output_fe.mat , \n             desc   :   FE structure , \n             ext   :   .mat , \n             required   :   true \n         },  \n         { \n             id   :   life_results , \n             filename   :   life_results.json , \n             required   :   true \n         },  \n         { \n             id   :   tracts , \n             dirname   :   tracts , \n             required   :   true \n         } \n     ]  }   For this example datatype,  brain-life/app-life  App generates a dataset with this datatype, and other Apps that want to use  neuro/life  output can request to have those files made available to their Apps by registering them on Brainlife App registration form. (Please read  Registering App  page for more info). The actual content/semantics of each file are up to developers exchanging the dataset to decide, and it should be well documented as part of the datatype registration process.  Please see other datatypes defined in  brain-life/datatypes .  Brainlife datatype might sound similar to BIDS specification, but it differs in following areas.    Brainlife datatypes mainly concern data derivatives generated by Apps and used only by Apps exchanging those datasets. They are only used within Brainlife platform and not meant to become standards for that particular data format.    Brainlife datatypes are defined by App developers involved in exchanging input/output datasets, not by Brainlife platform developers. App developers should discuss and agree on the structure of the datatype and what each file means. They can submit an issue on  brain-life/datatypes  and/or a pull request containing the list of files/directories to be registered on Brainlife. Once Brainlife team incorporate the PR, you will be able to use the new datatype for your App.", 
            "title": "Datatypes"
        }, 
        {
            "location": "/user/datatypes/#datatype-tags", 
            "text": "Sometimes you want to be more specific about the type of dataset for a particular datatype. For example,  neuro/anat/t1w  could be ACPC aligned or not,  neuro/dwi  could be single-shell or multi-shell, etc. Brainlife allows you to adds specificity to each datatype through  datatype tags .    Warning  Please don't confuse  Datatype tag  with  Dataset tag . \"Dataset tag\" is a tag that user can freely edit under dataset dialog to allow for easier searching or bulk processing of datasets with specific tags. \"Datatype tag\", on the other hand, can only be set by App developer and it is a part of datatype and can not be modified once dataset is created.   It is important to note that,  datatype tag  should always be used to add specificity to datasets, but not to generalize it. For example, we don't have \"multi-shell\" datatype tags because  neuro/dwi  is by default a \"multi-shell\" data; it is perfectly valid to have different b-values in  dwi.bvals  file. \"single-shell\" is a special case for  neuro/dwi  datatype where b-values happens to be all same number. Therefore, we introduce \"single-shell\" tag to describe such dwi dataset.  By always using datatype tags to add specificity, Brainlife can correctly identify which datasets can be used for which Apps by examining dataset's datatype tags and App's input dataset tags.    Hint  Please consult the #datatype slack channel on Brainlife slack team for any datatype related questions.", 
            "title": "Datatype Tags"
        }, 
        {
            "location": "/user/process/", 
            "text": "Processes\n\n\n\n\nPlease read \nTutorial / Data Processing\n first.\n\n\n\n\nUnder Project page, Processes tab is where you can perform data analysis on Brainlife.\n\n\n\n\nEach process is a logical grouping of various data analysis/processing tasks that share input and output datasets. Each task you submit will be assigned to various computing resources that you have access to and are currently available. Brainlife's task orchestration engine (\nAmaretti\n) takes care of data transfer and monitoring of your tasks.\n\n\nTo begin processing of your data, first you need to stage the initial datasets. You can stage from any project that you have read access to and for any subject. However, it is best to create a separate Process for each subject, as it would make it easier to submit Apps by allowing Brainlife to auto-populate various input datasets.\n\n\nMonitoring Tasks\n\n\nBrainlife monitors task status on remote resources and relays the most recent log entries back to the UI.\n\n\n\n\nYou can also see the entire content of the log by opening the \nRaw Output\n section of the task and selecting any log files you'd like to examine.\n\n\n\n\n\n\nNote\n\n\nRaw Output\n section will not be available for tasks that are not yet assigned to any resource.\n\n\n\n\nIf you'd like to download files, instead of opening directly via the browser, you can click the download (\n) button to download individual files, or the entire directories.\n\n\nTask Status\n\n\nBrainlife task can have one of the following task statuses.\n\n\n\n\n\n\nRequested\n\n\nWhen you first submit your task, Brainlife will place them under the Requested state and wait for Brainlife to assign a resource to run on. If there are many other tasks being processed, it might take a while for it to be picked up, but it should not take more than a few minutes. Once the resource is assigned, Brainlife will transfer any dependent input datasets and setup a \nwork directory\n on the resource.\n\n\n\n\n\n\nRunning\n\n\nOnce the task is ready to be executed, Brainlife will make a request to the local resource to start your task. Most resources have their own local batch scheduling systems, and your task will be placed in a queue where it waits for it to be actually executed on the system. Brainlife status might show that it's \nrunning\n even though it is actually just waiting in the remote queue.\n\n\n\n\n\n\nFinished\n\n\nThe task has completed successfully. You can visualize output datasets using Brainlife's built in visualization tool by clicking \n buttons next to each output dataset. If you have requested to auto-archive the output datasets (at submit time), those datasets will be copied to the Project's datasets archive. Any dependent task will be placed in Requested status.\n\n\n\n\n\n\nFailed\n\n\nIf the App terminates with non-0 exit code, the task will be marked as Failed. Please examine the output and determine the cause of the failure. Please help improving the App by contacting the App developer and/or submitting github issues.\n\n\n\n\n\n\nRemoved\n\n\nMost resources use what is called a \nscratch space\n to stage the task's work directory. Normally scratch space has a time limit on how long the data files can be left on those systems (typically 30-90 days). When Brainlife detects that the task directory no longer exists on remote systems, it will mark those tasks as \nRemoved\n.\n\n\n\n\nNote\n\n\nBrainlife will try to clean up old task directories in 25 days after the successful completion of the task to provide consistent behavior across various resources. This also reduces the disk space usage on various resources. If you have any output dataset that you'd like to keep, please archive ig or submit your task with the auto-archiving flag checked.\n\n\nIf you archive your output, you will see a list of datasets archived from this output.\n\n\n\n\n\n\n\n\n\n\nSubmitting Apps\n\n\nYou can submit Apps in a couple of different ways.\n\n\nOne way is to use the \nSubmit New App\n button under each process, as you have done already. Brainlife selects Apps that you can currently submit based on available datasets within the process and required input datasets for each App.\n\n\n\n\nThe more datasets you stage or generate, the more Apps you can submit. If you don't find an App that you are looking for, please head over to the App page and find the app you are trying to submit. You can either go back to the Process page and generate or stage required datasets, or you can execute the App directly from the App page by selecting the \nExecute\n tab under the App, which is our second way to submit an App.\n\n\n\n\nWhen you submit App via the Execute tab, Brainlife will create a new process under the selected project and stage all input files you have selected and submit your app in a single step. \n\n\n\n\nTip\n\n\nIf you are looking for just any sample dataset, you can try O3D project which contains a lot of common data derivatives.", 
            "title": "Processes"
        }, 
        {
            "location": "/user/process/#processes", 
            "text": "Please read  Tutorial / Data Processing  first.   Under Project page, Processes tab is where you can perform data analysis on Brainlife.   Each process is a logical grouping of various data analysis/processing tasks that share input and output datasets. Each task you submit will be assigned to various computing resources that you have access to and are currently available. Brainlife's task orchestration engine ( Amaretti ) takes care of data transfer and monitoring of your tasks.  To begin processing of your data, first you need to stage the initial datasets. You can stage from any project that you have read access to and for any subject. However, it is best to create a separate Process for each subject, as it would make it easier to submit Apps by allowing Brainlife to auto-populate various input datasets.", 
            "title": "Processes"
        }, 
        {
            "location": "/user/process/#monitoring-tasks", 
            "text": "Brainlife monitors task status on remote resources and relays the most recent log entries back to the UI.   You can also see the entire content of the log by opening the  Raw Output  section of the task and selecting any log files you'd like to examine.    Note  Raw Output  section will not be available for tasks that are not yet assigned to any resource.   If you'd like to download files, instead of opening directly via the browser, you can click the download ( ) button to download individual files, or the entire directories.", 
            "title": "Monitoring Tasks"
        }, 
        {
            "location": "/user/process/#task-status", 
            "text": "Brainlife task can have one of the following task statuses.    Requested  When you first submit your task, Brainlife will place them under the Requested state and wait for Brainlife to assign a resource to run on. If there are many other tasks being processed, it might take a while for it to be picked up, but it should not take more than a few minutes. Once the resource is assigned, Brainlife will transfer any dependent input datasets and setup a  work directory  on the resource.    Running  Once the task is ready to be executed, Brainlife will make a request to the local resource to start your task. Most resources have their own local batch scheduling systems, and your task will be placed in a queue where it waits for it to be actually executed on the system. Brainlife status might show that it's  running  even though it is actually just waiting in the remote queue.    Finished  The task has completed successfully. You can visualize output datasets using Brainlife's built in visualization tool by clicking   buttons next to each output dataset. If you have requested to auto-archive the output datasets (at submit time), those datasets will be copied to the Project's datasets archive. Any dependent task will be placed in Requested status.    Failed  If the App terminates with non-0 exit code, the task will be marked as Failed. Please examine the output and determine the cause of the failure. Please help improving the App by contacting the App developer and/or submitting github issues.    Removed  Most resources use what is called a  scratch space  to stage the task's work directory. Normally scratch space has a time limit on how long the data files can be left on those systems (typically 30-90 days). When Brainlife detects that the task directory no longer exists on remote systems, it will mark those tasks as  Removed .   Note  Brainlife will try to clean up old task directories in 25 days after the successful completion of the task to provide consistent behavior across various resources. This also reduces the disk space usage on various resources. If you have any output dataset that you'd like to keep, please archive ig or submit your task with the auto-archiving flag checked.  If you archive your output, you will see a list of datasets archived from this output.", 
            "title": "Task Status"
        }, 
        {
            "location": "/user/process/#submitting-apps", 
            "text": "You can submit Apps in a couple of different ways.  One way is to use the  Submit New App  button under each process, as you have done already. Brainlife selects Apps that you can currently submit based on available datasets within the process and required input datasets for each App.   The more datasets you stage or generate, the more Apps you can submit. If you don't find an App that you are looking for, please head over to the App page and find the app you are trying to submit. You can either go back to the Process page and generate or stage required datasets, or you can execute the App directly from the App page by selecting the  Execute  tab under the App, which is our second way to submit an App.   When you submit App via the Execute tab, Brainlife will create a new process under the selected project and stage all input files you have selected and submit your app in a single step.    Tip  If you are looking for just any sample dataset, you can try O3D project which contains a lot of common data derivatives.", 
            "title": "Submitting Apps"
        }, 
        {
            "location": "/user/pipeline/", 
            "text": "Pipelines\n\n\nThe \nProcesses\n tab allows you to submit tasks one at a time. This is great if you are exploring different Apps or experimenting with different configurations that can best process your datasets. However, once you find the optimal set of Apps, you would probably want to run it across many subjects and submitting them one by one is simply not a good option.\n\n\n\n\n\nBrainlife allows you to setup a series of submission rules called \npipeline rules\n. Instead of describing the entire workflow that you submit \nonce\n (or re-submit if something fails), you will define a set of individual rules which will be continuously evaluated until you deactivate them. It is similar to how a factory assembly line produces products. When a subject fails to produce an output dataset for a specific rule, you can examine and handle it manually. Once you are able to produce a valid output, the rest of the pipeline rules will pick it up as if it came from the original rule. \n\n\n\n\n\nSetting up Pipeline Rule\n\n\nTo setup a new pipeline rule, go to Project \n \nPipelines\n tab and click a plus button at the bottom right corner of the page.\n\n\nEach rule will be responsible for submitting a specific App with a specific set of configuration. Enter \nName\n field, and search for the App that you'd like to submit. Once you select an App, you will be able to set its configuration parameters.\n\n\n\n\nAll Brainlife Apps have a defined list of input datatypes that Apps needs to run. Using this information, Brainlife will look for any subject that provides all input datatypes required by the App, and submit new process for each subject found to run your App. If you have more than one dataset that matches the required datatype for a subject, you can specify which datasets to use by specifying a dataset tags (not datatype tags). By default, it will use the latest dataset available for a given datatype.\n\n\nWhen you are submitting your first rule, you probably don't have any dataset archived inside your project. If you'd like to use datasets from other project, you can specify the \nProject\n field to look for the input datasets there.\n\n\n\n\nAbove rule will submit processes for each subject found on ABIDE2 project that provides \ndwi\n datatype with a dataset tag of \"ABIDEII-BNI_1\".\n\n\nBrainlife will only submit new process if it hasn't submitted a new process for each subject. Brainlife also won't submit new process if your project already has an output datasets (maybe generated by other rules, or generated manually). To be more specific about which datasets are generated by which rule, you can specify output dataset tags under outputs section.\n\n\n\n\nYou can leave this default if you know you there won't be any other App generating the same output datatype. We recommend to always set output dataset tags just in case.\n\n\nLastly, you can set a \nSubject Filtering\n which limits the subjects that gets processed.\n\n\n\n\nAbove example will make this rule to only submit for subjects with names that start with \"100\" or \"200\". When you are setting up your first rule, it's always good to limit number of subjects to make sure your rule is setup correctly.\n\n\n\n\nHint\n\n\nThere are number of regular expression tutorials available online. Also, please feel free to send us your question via Brainlife slack team.\n\n\n\n\nMonitoring Pipeline Rules\n\n\nOnce you submit your pipeline rule, it might take 10 - 20 minutes before you start seeing new processes submitted by Brainlife's pipeline handler. \n\n\n\n\nNode\n\n\nBrainlife also limit number of running processes at around 50 processes for each rule so that any given rule won't consume all available computing resources.\n\n\n\n\n\n\nYou can treat these processes as you normally do with any other processes; examine outputs, stop, restart, etc.. The output datasets will be automatically archived once each task have completed successfully.\n\n\n\n\nNote\n\n\nIf you remove a process or task, Brainlife will resubmit another process to handle that subject if the subject has all required input datasets and has not produce the output from the requested app yet.", 
            "title": "Pipelines"
        }, 
        {
            "location": "/user/pipeline/#pipelines", 
            "text": "The  Processes  tab allows you to submit tasks one at a time. This is great if you are exploring different Apps or experimenting with different configurations that can best process your datasets. However, once you find the optimal set of Apps, you would probably want to run it across many subjects and submitting them one by one is simply not a good option.   Brainlife allows you to setup a series of submission rules called  pipeline rules . Instead of describing the entire workflow that you submit  once  (or re-submit if something fails), you will define a set of individual rules which will be continuously evaluated until you deactivate them. It is similar to how a factory assembly line produces products. When a subject fails to produce an output dataset for a specific rule, you can examine and handle it manually. Once you are able to produce a valid output, the rest of the pipeline rules will pick it up as if it came from the original rule.", 
            "title": "Pipelines"
        }, 
        {
            "location": "/user/pipeline/#setting-up-pipeline-rule", 
            "text": "To setup a new pipeline rule, go to Project    Pipelines  tab and click a plus button at the bottom right corner of the page.  Each rule will be responsible for submitting a specific App with a specific set of configuration. Enter  Name  field, and search for the App that you'd like to submit. Once you select an App, you will be able to set its configuration parameters.   All Brainlife Apps have a defined list of input datatypes that Apps needs to run. Using this information, Brainlife will look for any subject that provides all input datatypes required by the App, and submit new process for each subject found to run your App. If you have more than one dataset that matches the required datatype for a subject, you can specify which datasets to use by specifying a dataset tags (not datatype tags). By default, it will use the latest dataset available for a given datatype.  When you are submitting your first rule, you probably don't have any dataset archived inside your project. If you'd like to use datasets from other project, you can specify the  Project  field to look for the input datasets there.   Above rule will submit processes for each subject found on ABIDE2 project that provides  dwi  datatype with a dataset tag of \"ABIDEII-BNI_1\".  Brainlife will only submit new process if it hasn't submitted a new process for each subject. Brainlife also won't submit new process if your project already has an output datasets (maybe generated by other rules, or generated manually). To be more specific about which datasets are generated by which rule, you can specify output dataset tags under outputs section.   You can leave this default if you know you there won't be any other App generating the same output datatype. We recommend to always set output dataset tags just in case.  Lastly, you can set a  Subject Filtering  which limits the subjects that gets processed.   Above example will make this rule to only submit for subjects with names that start with \"100\" or \"200\". When you are setting up your first rule, it's always good to limit number of subjects to make sure your rule is setup correctly.   Hint  There are number of regular expression tutorials available online. Also, please feel free to send us your question via Brainlife slack team.", 
            "title": "Setting up Pipeline Rule"
        }, 
        {
            "location": "/user/pipeline/#monitoring-pipeline-rules", 
            "text": "Once you submit your pipeline rule, it might take 10 - 20 minutes before you start seeing new processes submitted by Brainlife's pipeline handler.    Node  Brainlife also limit number of running processes at around 50 processes for each rule so that any given rule won't consume all available computing resources.    You can treat these processes as you normally do with any other processes; examine outputs, stop, restart, etc.. The output datasets will be automatically archived once each task have completed successfully.   Note  If you remove a process or task, Brainlife will resubmit another process to handle that subject if the subject has all required input datasets and has not produce the output from the requested app yet.", 
            "title": "Monitoring Pipeline Rules"
        }, 
        {
            "location": "/user/publication/", 
            "text": "Publications\n\n\nOnce you finish processing your datasets and produced all data derivatives, you can publish them by creating a current snapshots of your datasets and create a new publication page.\n\n\n\n\nA publication page can be accessed and datasets to be downloaded by anyone on the internet without having to login to Brainlife.\n\n\nEach publication page may include following information.\n\n\n\n\nA permanent-URL for the publication page.\n\n\nUnique DOI that redirects to the permanent-URL.\n\n\nPublication details such as authors, description, funders, data access license, etc..\n\n\nCitation template that visitors can use to cite your publication.\n\n\nA list of datasets that you'd like to make it part of your publication. For each dataset listed, visitors can view dataset detail including provenance, App used to generate the dataset and a link to download the dataset.\n\n\n\n\n\n\nDanger\n\n\nOnce you publish your datasets, those datasets will become \"world-readable\" even for a private project. Also, currently, you cannot change the list of published dataset. Please be careful which datasets to publish.\n\n\n\n\nCreating Publication Page\n\n\nTo create a new publication page, go to a project where you want to publish your datasets, then open to the \nPublication\n tab. Click the \nPlus\n button at the bottom right corner of the screen. You should see a list of datatypes and the total files sizes like below.\n\n\n\n\nSelect datatypes that you'd like to include in your publication. Click \nNext\n.\n\n\n\n\nDanger\n\n\nBrainlife currently does not allow you to make modification to the list of datasets included in your publication once published. Please examine carefully as you will not be able to change/un-publish datasets once they are made public.\n\n\n\n\nNext page allows you to enter title, description and various other metadata. You can edit these information after you publish. When you are done, click \nSubmit\n. The creation of publication page might take a few minute depending on the number of datasets. Brainlife will issue a new DOI from Datacite at this point.\n\n\nOnce you've successfully published your new publication, you should see it listed under the Publications tab under your project as well as the global Publication page which lists all currently registered publications from all projects.", 
            "title": "Publications"
        }, 
        {
            "location": "/user/publication/#publications", 
            "text": "Once you finish processing your datasets and produced all data derivatives, you can publish them by creating a current snapshots of your datasets and create a new publication page.   A publication page can be accessed and datasets to be downloaded by anyone on the internet without having to login to Brainlife.  Each publication page may include following information.   A permanent-URL for the publication page.  Unique DOI that redirects to the permanent-URL.  Publication details such as authors, description, funders, data access license, etc..  Citation template that visitors can use to cite your publication.  A list of datasets that you'd like to make it part of your publication. For each dataset listed, visitors can view dataset detail including provenance, App used to generate the dataset and a link to download the dataset.    Danger  Once you publish your datasets, those datasets will become \"world-readable\" even for a private project. Also, currently, you cannot change the list of published dataset. Please be careful which datasets to publish.", 
            "title": "Publications"
        }, 
        {
            "location": "/user/publication/#creating-publication-page", 
            "text": "To create a new publication page, go to a project where you want to publish your datasets, then open to the  Publication  tab. Click the  Plus  button at the bottom right corner of the screen. You should see a list of datatypes and the total files sizes like below.   Select datatypes that you'd like to include in your publication. Click  Next .   Danger  Brainlife currently does not allow you to make modification to the list of datasets included in your publication once published. Please examine carefully as you will not be able to change/un-publish datasets once they are made public.   Next page allows you to enter title, description and various other metadata. You can edit these information after you publish. When you are done, click  Submit . The creation of publication page might take a few minute depending on the number of datasets. Brainlife will issue a new DOI from Datacite at this point.  Once you've successfully published your new publication, you should see it listed under the Publications tab under your project as well as the global Publication page which lists all currently registered publications from all projects.", 
            "title": "Creating Publication Page"
        }, 
        {
            "location": "/apps/introduction/", 
            "text": "Introduction\n\n\nWhat is \nApp\n?\n\n\nBrainlife Apps are snippets of code comprising a (short) series of processing steps within a larger data analysis workflow. Apps are meant to be reusable by other users and not just by the App developer. Apps usage is value added to the work of the App Developer. So, the code in each App should use general tools and clarity in code writing so to make the App undeerstandable by other users. \n\n\n\n\nApps are hosted on public \nGitHub.com\n repositories. Apps can comprise any combination of MatLab, Python or other types code.\n\n\nApps must have a single executable file named \nmain\n in the root directory of the git repository. In the most common case, \nmain\n is a UNIX bash script that calls other code in the repository to run the algorithms for data analysis. The code for data analysis can be written in any language, or can be compiled binary code. \n\n\nApps must read all input parameters and data files from a \nconfig.json\n file. \nconfig.json\n is created by Brainlife.io at runtime on the current working directory (\n./\n, \nrelative path\n) of the compute resource that your App will run on. But you do not have to think about this actually, just write a \nrelative path\n in your code when loading files from the \nconfig.json\n file, no need for \nabsolute paths\n. \n\n\nWrite all output files in the current directory (\n./\n), in a structure defined in a Brainlife \ndatatype\n. More information about \nBrainlife datatypes\n later.\n\n\n\n\nIdeally, Apps should be packaged into \nDocker containers\n. But that is not a requirement. Apps Dockerizing will allow a broader App usage, because Apps can run on multiple compute systems and will most likely increase the impact of the code you write, with higher likelyhood of increasing the impact of your work as a Brainlife App developer. More information abotu \nApps Dockerization can be found here\n.\n\n\nBrainlife Apps follow a technical specification called Application for Big Computational Data analysis or \nABCD\n\n\nApp Development Timeline\n\n\nYou would normally follow following steps to develop and register your App on Brainlife.\n\n\n\n\nDevelop an algorithm that runs on your laptop or local cluster with your test datasets.\n\n\nCreate a sample \nconfig.json\n but add it to \n.gitignore\n so that it won't be part of your repo. (You can include \nconfig.json.sample\n and symlink it to config.json)\n\n\nCreate \nmain\n that parses \nconfig.json\n and pass it to your algorithm.\n\n\nPublish it as public github repo.\n\n\nRegister your App on Brainlife. During this step, you can define what parameters and input file should be made available to your App via \nconfig.json\n.\n\n\nContact resource administrators and ask them to enable your App (more below). \n\n\n\n\nEnabling App on a compute resource\n\n\nApp needs to be enabled on each compute resources to run. Each user will have a different set of resources that they have access to, but Brainlife provides default \nshared\n resources for all users. If you want anyone in the Brainlife to be able to run your App, you can \ncontact the resource administrators\n of these default resources to enable your Apps.\n\n\nYou will need to discuss with resource administrators on how to handle any dependencies/libraries that your App might require. To make things easier and reproducible, you should consider Dockerizing you App's \ndependencies\n (but not the App itself) so that you can run your App through your container using \nsingularity\n from your \nmain\n. \n\n\n\n\nHint\n\n\nMost compute resources now provide singularity which increases the number of resource where you might be able to run your Apps.\n\n\n\n\nApp Launch Sequence\n\n\nBrainlife executes an App in following steps.\n\n\n\n\nA user requests to run your App through Brainlife.\n\n\nBrainlife queries a list of compute resources that user has access to and currently available to run your App. Brainlife then determines the best compute resource to run your App.\n\n\nBrainlife stages input datasets out of Brainlife's datasets archive, or transfer any dependent task's work directory that are required to run your App.\n\n\nBrainlife creates a new working directory by git cloning your App on (normally) a resource's scratch disk space, and place \nconfig.json\n containing user specified configuration parameters and various paths the input files.\n\n\nBrainlife then runs \nstart\n hook installed on each compute resources as part of \nabcd specification\n (App developer should have to worry about this under the most circumstances).\n\n\nOn a PBS cluster, \nstart\n hook then \nqsub\ns your \nmain\n script and place it on the local batch scheduler queue.\n\n\nLocal job scheduler runs your \nmain\n on a compute node and your App will execute your algorithm, and generate output on the working directory.\n\n\nBrainlife periodically monitors your job and relay information back to the user.\n\n\nOnce job is completed, user archives an output dataset if the result is valid.\n\n\n\n\nDatatype\n\n\nDifferent Brainlife Apps can exchange input/output datasets through Brainlife \ndatatypes\n which are developer defined file/directory structure that holds specific set of data.\n\n\nHere are some example of currently registered datatypes.\n\n\n\n\nneuro/anat/t1w (t1.nii.gz)\n\n\nneuro/anat/t2w (t2.nii.gz) \n\n\nneuro/dwi (diffusion data and bvecs/bvals)\n\n\nneuro/freesurfer (entire freesurfer output)\n\n\nneuro/tract (tract.tck containing fiber track data)\n\n\nneuro/dtiinit (dtiinit output - dti output directory)\n\n\ngeneric/images (a list of images)\n\n\nraw (unstructured data often used during development)\n\n\n\n\nYour App should read from one or more of these datatypes and write output data in a format specified by another datatype. By identifying existing datatypes that you can interoperate you can interface with datasets generated by other Apps and your output can be used by other Apps as their input.\n\n\nWe maintain a list of \ndatatypes\n in our \nbrain-life/datatypes\n repo. To create a new datatype, please open an issue, or submit a PR with a new datatype definition file (.json). We do not modify datatypes once it's published to preserve backward compatibility, but you can re-register new datatype under a different version.\n\n\n\n\nHint\n\n\nBrainlife app should follow the \nDo One Thing and Do It Well\n principle where a complex workflow should be split into several smaller Apps (but no more than necessary nor practical) to promote code-reuse and help parallelize your workflow and run each App on the most appropriate compute resource available.\n\n\n\n\n\n\nHint\n\n\nBefore writing your apps, please browse \ncurrently registered Brainlife Apps\n and datatypes under Brainlife.io to make sure you are not reinventing Apps. If you find an App that is similar to what you need, please contact the developer of the App and discuss if the feature you need can be added to the App.", 
            "title": "Introduction"
        }, 
        {
            "location": "/apps/introduction/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/apps/introduction/#what-is-app", 
            "text": "Brainlife Apps are snippets of code comprising a (short) series of processing steps within a larger data analysis workflow. Apps are meant to be reusable by other users and not just by the App developer. Apps usage is value added to the work of the App Developer. So, the code in each App should use general tools and clarity in code writing so to make the App undeerstandable by other users.    Apps are hosted on public  GitHub.com  repositories. Apps can comprise any combination of MatLab, Python or other types code.  Apps must have a single executable file named  main  in the root directory of the git repository. In the most common case,  main  is a UNIX bash script that calls other code in the repository to run the algorithms for data analysis. The code for data analysis can be written in any language, or can be compiled binary code.   Apps must read all input parameters and data files from a  config.json  file.  config.json  is created by Brainlife.io at runtime on the current working directory ( ./ ,  relative path ) of the compute resource that your App will run on. But you do not have to think about this actually, just write a  relative path  in your code when loading files from the  config.json  file, no need for  absolute paths .   Write all output files in the current directory ( ./ ), in a structure defined in a Brainlife  datatype . More information about  Brainlife datatypes  later.   Ideally, Apps should be packaged into  Docker containers . But that is not a requirement. Apps Dockerizing will allow a broader App usage, because Apps can run on multiple compute systems and will most likely increase the impact of the code you write, with higher likelyhood of increasing the impact of your work as a Brainlife App developer. More information abotu  Apps Dockerization can be found here .  Brainlife Apps follow a technical specification called Application for Big Computational Data analysis or  ABCD", 
            "title": "What is App?"
        }, 
        {
            "location": "/apps/introduction/#app-development-timeline", 
            "text": "You would normally follow following steps to develop and register your App on Brainlife.   Develop an algorithm that runs on your laptop or local cluster with your test datasets.  Create a sample  config.json  but add it to  .gitignore  so that it won't be part of your repo. (You can include  config.json.sample  and symlink it to config.json)  Create  main  that parses  config.json  and pass it to your algorithm.  Publish it as public github repo.  Register your App on Brainlife. During this step, you can define what parameters and input file should be made available to your App via  config.json .  Contact resource administrators and ask them to enable your App (more below).", 
            "title": "App Development Timeline"
        }, 
        {
            "location": "/apps/introduction/#enabling-app-on-a-compute-resource", 
            "text": "App needs to be enabled on each compute resources to run. Each user will have a different set of resources that they have access to, but Brainlife provides default  shared  resources for all users. If you want anyone in the Brainlife to be able to run your App, you can  contact the resource administrators  of these default resources to enable your Apps.  You will need to discuss with resource administrators on how to handle any dependencies/libraries that your App might require. To make things easier and reproducible, you should consider Dockerizing you App's  dependencies  (but not the App itself) so that you can run your App through your container using  singularity  from your  main .    Hint  Most compute resources now provide singularity which increases the number of resource where you might be able to run your Apps.", 
            "title": "Enabling App on a compute resource"
        }, 
        {
            "location": "/apps/introduction/#app-launch-sequence", 
            "text": "Brainlife executes an App in following steps.   A user requests to run your App through Brainlife.  Brainlife queries a list of compute resources that user has access to and currently available to run your App. Brainlife then determines the best compute resource to run your App.  Brainlife stages input datasets out of Brainlife's datasets archive, or transfer any dependent task's work directory that are required to run your App.  Brainlife creates a new working directory by git cloning your App on (normally) a resource's scratch disk space, and place  config.json  containing user specified configuration parameters and various paths the input files.  Brainlife then runs  start  hook installed on each compute resources as part of  abcd specification  (App developer should have to worry about this under the most circumstances).  On a PBS cluster,  start  hook then  qsub s your  main  script and place it on the local batch scheduler queue.  Local job scheduler runs your  main  on a compute node and your App will execute your algorithm, and generate output on the working directory.  Brainlife periodically monitors your job and relay information back to the user.  Once job is completed, user archives an output dataset if the result is valid.", 
            "title": "App Launch Sequence"
        }, 
        {
            "location": "/apps/introduction/#datatype", 
            "text": "Different Brainlife Apps can exchange input/output datasets through Brainlife  datatypes  which are developer defined file/directory structure that holds specific set of data.  Here are some example of currently registered datatypes.   neuro/anat/t1w (t1.nii.gz)  neuro/anat/t2w (t2.nii.gz)   neuro/dwi (diffusion data and bvecs/bvals)  neuro/freesurfer (entire freesurfer output)  neuro/tract (tract.tck containing fiber track data)  neuro/dtiinit (dtiinit output - dti output directory)  generic/images (a list of images)  raw (unstructured data often used during development)   Your App should read from one or more of these datatypes and write output data in a format specified by another datatype. By identifying existing datatypes that you can interoperate you can interface with datasets generated by other Apps and your output can be used by other Apps as their input.  We maintain a list of  datatypes  in our  brain-life/datatypes  repo. To create a new datatype, please open an issue, or submit a PR with a new datatype definition file (.json). We do not modify datatypes once it's published to preserve backward compatibility, but you can re-register new datatype under a different version.   Hint  Brainlife app should follow the  Do One Thing and Do It Well  principle where a complex workflow should be split into several smaller Apps (but no more than necessary nor practical) to promote code-reuse and help parallelize your workflow and run each App on the most appropriate compute resource available.    Hint  Before writing your apps, please browse  currently registered Brainlife Apps  and datatypes under Brainlife.io to make sure you are not reinventing Apps. If you find an App that is similar to what you need, please contact the developer of the App and discuss if the feature you need can be added to the App.", 
            "title": "Datatype"
        }, 
        {
            "location": "/apps/helloworld/", 
            "text": "Please read \nApp Developers / Introduction\n first. \n\n\n\n\nHelloWorld\n\n\nHere, we will create a \"HelloWorld\" Brainlife App. \n\n\nWe will show how to create a brand new \ngithub repository\n containing a Brainlife App. Please be sure to make the repo public so that the \nbrainlife.io\n platform will be able to access it. You can name the repository as you prefer, the Brainlife Team has beeen naming apps starting with the prefix \napp-\n, for example take a look at \nthese Apps\n.\n\n\nAs a start we will create a HelloWorld App, i.e., \napp-helloworld\n, \nhere is an example\n. \nGit clone\n your new repository on your local machine - where you will be developing/editing and testing your App.\n\n\ngit clone git@github.com:francopestilli/app-helloworld.git\n\n\n\n\n\nor (depending on your www.github.com settings):\n\n\ngit clone https://github.com/francopestilli/app-helloworld.git\n\n\n\n\n\nNow, cd inside the local directory of the repository and create a file called \nmain\n. This file contains some information about the UNIX environment (\nbash-related collands\n), the procedure to submit jobs in a cluster environment (\nPBS-related commands\n), parsing inputs from the config.json file using \njq\n (see \nhere\n for more information about \njq\n). For example:\n\n\ntouch main\n\n\n\n\n\nmain\n\n\nAfter creating the file \nmain\n inside your local folder for the github repository app-helloworld, we will edit the content of the file and make it executable. Use your preferred editor and edit the file. Copy the text below insde the edited \nmain\n file, and save it back to disk.\n\n\n#!/bin/bash\n\n\n\n#PBS -l nodes=1:ppn=1\n\n\n#PBS -l walltime=00:05:00\n\n\n\n#parse config.json for input parameters (here, we are pulling \nt1\n)\n\n\nt1\n=\n$(\njq -r .t1 config.json\n)\n\n./main.py \n$t1\n\n\n\n\n\n\nPlease be sure to set the file \nmain\n is executable. You can do that by running thee following command in a terminal, before pushing to the github repository.\n\n\nchmod +x main\n\n\n\n\n\nFinally, \nadd\n the file to the git repository and \ncommit\n to github.com by running thee following:\n\n\ngit add main\n\n\ngit commit -am \"Added main file\"\n\n\ngit push\n\n\n\n\nNote\n\n\njq\n is a command line tool used to parse a small JSON file and pull values out of it. You can install it on your machine by running something like \napt-get install jq\n or \nyum install jq\n or \nbrew install jq\n depending on your Operative System (OS) or OS distribution. Also note that thee Brainlife computational resources (Cloud) wheere that App will need to run, will need to have common binaries installed including \nbash\n, \njq\n, and \nsingularity\n. \n\n\n\n\n\n\nFor Mac Users\n\n\nYou will need to have \nthe XCODE, Apple Development Tools\n and \nhomebrew\n to install \njq\n. Once Xcode is installed run this command \n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n and then this command \nbrew install jq\n in a terminal.\n\n\n\n\nThe first few lines in our \nmain\n instructs PBS or Slurm batch systems to request a certain number of nodes/processes to our App. \n\n\n#PBS -l nodes=1:ppn=1\n\n\n#PBS -l walltime=00:05:00\n\n\n\n\n\n\n\n\nNote\n\n\nYou will receive all input parameters from Brainlife through a JSON file named \nconfig.json\n which is created by Brainlife when your App is executed. See \nExample config.json\n. As an App developer, you will define what parameters needs to be entered by the user and input datasets later when you register your App on Brainlife.\n\n\n\n\nFollowing lines parses the \nconfig.json\n using \njq\n and the value of \nt1\n to the main part of the application which we will create later.\n\n\n#parse config.json for input parameters\n\n\nt1\n=\n$(\njq -r .t1 config.json\n)\n\n./main.py \n$t1\n\n\n\n\n\n\nTo be able to test your application, let's create a test \nconfig.json\n.\n\n\nconfig.json\n\n\n{\n\n   \nt1\n:\n \n~/data/t1.nii.gz\n\n\n}\n\n\n\n\n\n\nPlease update the path to wherever you have your test \nanat/t1w\n input file. If you don't have any, you can download one from an the \nOpen Diffusion Data Derivatives\n publication page. Just click the Datasets tab, and select any \nanat/t1w\n data to download. Then create a directory in your home directory and move the t1w.nii.gz file in there and unpack it: \n\n\ncd ~\n\n\nmkdir data\n\n\ncp -v /path/to/your/downloaded/5a050966eec2b300611abff2.tar ~/data/\n\n\ntar -xvf ~/data/5a050966eec2b300611abff2.tar\n\n\nAt this point, \n~/data/\n should contain a file named t1w.nii.gz. Next, you should add \nconfig.json\n to \n.gitignore\n as \nconfig.json\n is created at runtime by Brainlife, and we just need this now to test your app. \n\n\n\n\nHint\n\n\nA good pattern might be to create a file called \nconfig.json.sample\n used to test your App, and create a symlink \nln -s config.json config.json.sample\n so that you can run your app using \nconfig.json.sample\n without including the actual \nconfig.json\n as part of your repo. This allows other users to construct their own \nconfig.json\n if they want to run your app via command line.\n\n\n\n\n\n\nNote\n\n\nInstead of parsing \nconfig.json\n inside \nmain\n, you could use other parsing library as part of your algorithm itself, like Python's \nimport json\n, or Matlab's \njsonlab\n module inside the actual program that \nmain\n will be executing.\n\n\n\n\nOur \nmain\n script runs a python script called \nmain.py\n so let's create it and edit it by compying its content as reported below.\n\n\ncd ~/git/app-helloworld\n\n\ntouch main.py\n\n\nmain.py\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n#!/usr/bin/env python\n\n\n\nimport\n \nsys\n\n\nimport\n \nnibabel\n \nas\n \nnib\n\n\n\n#just dump input image header to output.txt\n\n\nimg\n=\nnib\n.\nload\n(\nsys\n.\nargv\n[\n1\n])\n\n\nf\n=\nopen\n(\noutput.txt\n,\n \nw\n)\n\n\nf\n.\nwrite\n(\nstr\n(\nimg\n.\nheader\n))\n\n\nf\n.\nclose\n()\n\n\n\n\n\n\n\nAgain, be sure to make \nmain.py\n also executable.\n\n\nchmomd +x main.py\n\n\n\n\n\nFinally, \nadd\n the file to the git repository and \ncommit\n to github.com by running thee following:\n\n\ngit add main\n\n\ngit commit -am \"Added main.py file\"\n\n\ngit push\n\n\nAny output files from your app should be written to the current working directory and in a file structure that complies with whichever the datatype of your dataset is. For now, we are not going to worry about the output datatype (assuming we will use \nraw\n)\n\n\nPlease be sure to add any output files from your app to .gitignore so that it won't be part of your git repo. \n\n\n.gitignore\n\n\nconfig.json\noutput.txt\n\n\n\n\n\nTesting\n\n\nNow, you should be able to test run your app locally by executing \nmain\n\n\n./main\n\n\n\n\n\n\n\n\nNow, it should generate an output file called \noutput.txt\n containing the dump of all nifti headers.\n\n\nclass \nnibabel.nifti1.Nifti1Header\n object, endian=\n\nsizeof_hdr      : 348\ndata_type       : \ndb_name         : \nextents         : 0\nsession_error   : 0\nregular         : r\ndim_info        : 0\ndim             : [  3 260 311 260   1   1   1   1]\n...\n...\n...\nqoffset_x       : 90.0\nqoffset_y       : -126.0\nqoffset_z       : -72.0\nsrow_x          : [ -0.69999999   0.           0.          90.        ]\nsrow_y          : [   0.            0.69999999    0.         -126.        ]\nsrow_z          : [  0.           0.           0.69999999 -72.        ]\nintent_name     : \nmagic           : n+1\n\n\n\n\n\nPushing to Github\n\n\nIf everything looks good, push our files to the Github.\n\n\ngit add .\ngit commit -m\ncreated my first BL App!\n\ngit push\n\n\n\n\n\nCongratulations! We have just created our first Brainlife App. To summarize, we've done following.\n\n\n\n\nCreated a new public Github repo.\n\n\nCreated \nmain\n which parses \nconfig.json\n and runs our App.\n\n\nCreated a test \nconfig.json\n.\n\n\nCreated \nmain.py\n which runs our algorithm and generate output files.\n\n\nTested the App, and pushed all files to Github.\n\n\n\n\n\n\nInfo\n\n\nYou can see more concrete examples of Brainlife apps at \nBrainlife hosted apps\n.\n\n\n\n\nTo run your App on Brainlife, you will need to do following.\n\n\n\n\n\n\nRegister your App on Brainlife.\n\n\n\n\n\n\nEnable your App on at least one Brainlife compute resource. \n\n\nFor now, please email \nmailto:brlife@iu.edu\n to enable your App on our shared test resource.", 
            "title": "HelloWorld"
        }, 
        {
            "location": "/apps/helloworld/#helloworld", 
            "text": "Here, we will create a \"HelloWorld\" Brainlife App.   We will show how to create a brand new  github repository  containing a Brainlife App. Please be sure to make the repo public so that the  brainlife.io  platform will be able to access it. You can name the repository as you prefer, the Brainlife Team has beeen naming apps starting with the prefix  app- , for example take a look at  these Apps .  As a start we will create a HelloWorld App, i.e.,  app-helloworld ,  here is an example .  Git clone  your new repository on your local machine - where you will be developing/editing and testing your App.  git clone git@github.com:francopestilli/app-helloworld.git  or (depending on your www.github.com settings):  git clone https://github.com/francopestilli/app-helloworld.git  Now, cd inside the local directory of the repository and create a file called  main . This file contains some information about the UNIX environment ( bash-related collands ), the procedure to submit jobs in a cluster environment ( PBS-related commands ), parsing inputs from the config.json file using  jq  (see  here  for more information about  jq ). For example:  touch main", 
            "title": "HelloWorld"
        }, 
        {
            "location": "/apps/helloworld/#main", 
            "text": "After creating the file  main  inside your local folder for the github repository app-helloworld, we will edit the content of the file and make it executable. Use your preferred editor and edit the file. Copy the text below insde the edited  main  file, and save it back to disk.  #!/bin/bash  #PBS -l nodes=1:ppn=1  #PBS -l walltime=00:05:00  #parse config.json for input parameters (here, we are pulling  t1 )  t1 = $( jq -r .t1 config.json ) \n./main.py  $t1   Please be sure to set the file  main  is executable. You can do that by running thee following command in a terminal, before pushing to the github repository.  chmod +x main  Finally,  add  the file to the git repository and  commit  to github.com by running thee following:  git add main  git commit -am \"Added main file\"  git push   Note  jq  is a command line tool used to parse a small JSON file and pull values out of it. You can install it on your machine by running something like  apt-get install jq  or  yum install jq  or  brew install jq  depending on your Operative System (OS) or OS distribution. Also note that thee Brainlife computational resources (Cloud) wheere that App will need to run, will need to have common binaries installed including  bash ,  jq , and  singularity .     For Mac Users  You will need to have  the XCODE, Apple Development Tools  and  homebrew  to install  jq . Once Xcode is installed run this command  /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"  and then this command  brew install jq  in a terminal.   The first few lines in our  main  instructs PBS or Slurm batch systems to request a certain number of nodes/processes to our App.   #PBS -l nodes=1:ppn=1  #PBS -l walltime=00:05:00    Note  You will receive all input parameters from Brainlife through a JSON file named  config.json  which is created by Brainlife when your App is executed. See  Example config.json . As an App developer, you will define what parameters needs to be entered by the user and input datasets later when you register your App on Brainlife.   Following lines parses the  config.json  using  jq  and the value of  t1  to the main part of the application which we will create later.  #parse config.json for input parameters  t1 = $( jq -r .t1 config.json ) \n./main.py  $t1   To be able to test your application, let's create a test  config.json .", 
            "title": "main"
        }, 
        {
            "location": "/apps/helloworld/#configjson", 
            "text": "{ \n    t1 :   ~/data/t1.nii.gz  }   Please update the path to wherever you have your test  anat/t1w  input file. If you don't have any, you can download one from an the  Open Diffusion Data Derivatives  publication page. Just click the Datasets tab, and select any  anat/t1w  data to download. Then create a directory in your home directory and move the t1w.nii.gz file in there and unpack it:   cd ~  mkdir data  cp -v /path/to/your/downloaded/5a050966eec2b300611abff2.tar ~/data/  tar -xvf ~/data/5a050966eec2b300611abff2.tar  At this point,  ~/data/  should contain a file named t1w.nii.gz. Next, you should add  config.json  to  .gitignore  as  config.json  is created at runtime by Brainlife, and we just need this now to test your app.    Hint  A good pattern might be to create a file called  config.json.sample  used to test your App, and create a symlink  ln -s config.json config.json.sample  so that you can run your app using  config.json.sample  without including the actual  config.json  as part of your repo. This allows other users to construct their own  config.json  if they want to run your app via command line.    Note  Instead of parsing  config.json  inside  main , you could use other parsing library as part of your algorithm itself, like Python's  import json , or Matlab's  jsonlab  module inside the actual program that  main  will be executing.   Our  main  script runs a python script called  main.py  so let's create it and edit it by compying its content as reported below.  cd ~/git/app-helloworld  touch main.py", 
            "title": "config.json"
        }, 
        {
            "location": "/apps/helloworld/#mainpy", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 #!/usr/bin/env python  import   sys  import   nibabel   as   nib  #just dump input image header to output.txt  img = nib . load ( sys . argv [ 1 ])  f = open ( output.txt ,   w )  f . write ( str ( img . header ))  f . close ()    Again, be sure to make  main.py  also executable.  chmomd +x main.py  Finally,  add  the file to the git repository and  commit  to github.com by running thee following:  git add main  git commit -am \"Added main.py file\"  git push  Any output files from your app should be written to the current working directory and in a file structure that complies with whichever the datatype of your dataset is. For now, we are not going to worry about the output datatype (assuming we will use  raw )  Please be sure to add any output files from your app to .gitignore so that it won't be part of your git repo.", 
            "title": "main.py"
        }, 
        {
            "location": "/apps/helloworld/#gitignore", 
            "text": "config.json\noutput.txt", 
            "title": ".gitignore"
        }, 
        {
            "location": "/apps/helloworld/#testing", 
            "text": "Now, you should be able to test run your app locally by executing  main  ./main   Now, it should generate an output file called  output.txt  containing the dump of all nifti headers.  class  nibabel.nifti1.Nifti1Header  object, endian= \nsizeof_hdr      : 348\ndata_type       : \ndb_name         : \nextents         : 0\nsession_error   : 0\nregular         : r\ndim_info        : 0\ndim             : [  3 260 311 260   1   1   1   1]\n...\n...\n...\nqoffset_x       : 90.0\nqoffset_y       : -126.0\nqoffset_z       : -72.0\nsrow_x          : [ -0.69999999   0.           0.          90.        ]\nsrow_y          : [   0.            0.69999999    0.         -126.        ]\nsrow_z          : [  0.           0.           0.69999999 -72.        ]\nintent_name     : \nmagic           : n+1", 
            "title": "Testing"
        }, 
        {
            "location": "/apps/helloworld/#pushing-to-github", 
            "text": "If everything looks good, push our files to the Github.  git add .\ngit commit -m created my first BL App! \ngit push  Congratulations! We have just created our first Brainlife App. To summarize, we've done following.   Created a new public Github repo.  Created  main  which parses  config.json  and runs our App.  Created a test  config.json .  Created  main.py  which runs our algorithm and generate output files.  Tested the App, and pushed all files to Github.    Info  You can see more concrete examples of Brainlife apps at  Brainlife hosted apps .   To run your App on Brainlife, you will need to do following.    Register your App on Brainlife.    Enable your App on at least one Brainlife compute resource.   For now, please email  mailto:brlife@iu.edu  to enable your App on our shared test resource.", 
            "title": "Pushing to Github"
        }, 
        {
            "location": "/apps/register/", 
            "text": "Registering App\n\n\nOnce your App is published on github, you can now register it on Brainlife and let you and other users discover your App and execute on Brainlife.\n\n\nFirst, go to the \nApps page\n on Brainlife, click \nPlus Button\n at the bottom right corner of the page. App registration form should appear.\n\n\nLet's go through each sections.\n\n\nDetail\n\n\n\n\nEnter any \nname\n for your App and \nGit Repository Name\n field which is the the organization / repository name (like \nyourname/app-name\n) of your github repo. Please do not enter the full github URL.\n\n\nAll other fields in this section are optional, but you could populate following fields.\n\n\nAvatar\n\n\nYou can enter \navatar\n URL if you have URL for an avatar that you'd like to use for your App. Please choose a square image with \nhttps://\n URL (not \nhttp://\n). Avatar may sounds superfluous, but please keep in mind that there are many other Apps registered on Brainlife and this might be the only visual queue for users to identify and search for your App. If you don't specify Avatar URL, Brainlife will use a randomly generated (robots) Avatar. \n\n\nProject\n\n\nBy default, all Apps are \npublic\n meaning any user can find your App and execute your App. If you'd like to make your App only available on a specific project (and their group members), you can enter project names under \nProject\n field and only the member of that project will be able to access your App. This might be useful if you are still developing your App and wants to keep it hidden until you make a formal \nrelease\n, or if your App would only function on datasets stored under a specific project.\n\n\nBranch\n\n\nIf you don't specify the github repo's branch name, it uses \nmaster\n branch by default. Most developer makes the latest code changes on \nmaster\n branch. If you leave the \nBranch\n field empty and let it use the \nmaster\n branch by default, a user won't be able to reproduce the output with exactly the same version of your code if you make any changes to it.\n\n\nOnce you finish developing your App, you should consider creating a release branch (like \n1.0\n) and specifying that branch name for your App so that Brainlife will always run the specific version of your App. Brainlife stores the branch name used to execute each task, so this allows users to reproduce the output using the same version of the code.\n\n\nPlease see \nVersioning Tip\n for more info.\n\n\nInput Datasets\n\n\nHere you can define a list of input datasets that your App is expecting.\n\n\n\n\nID\n\n\nThis is just an ID to uniquely identify this input dataset. Please enter any ID you'd like to use. It just has to be unique among all other input datasets.\n\n\nDatatype/Tags\n\n\nThe datatype/tags of this input dataset. Please enter any datatype tags that your App would require under \nDatatype Tags\n field. Brainlife will only allow users to select dataset that meets specified datatype tags.\n\n\n\n\nHint\n\n\nIf you don't know which datatype to use, please consult the #datatype slack channel on Brainlife slack team.\n\n\n\n\nPlease read \ndatatypes\n for more information. \n\n\nFile Mapping\n\n\nOnce you select the datatype/tags for your input dataset, you then need to configure how you want the selected dataset to be represented in the \nconfig.json\n. Each datatype consists of various files and directories. Here, you can map the object key in \nconfig.json\n to a particular file / directory within the datatype.\n\n\nFor example, \nneuro/dwi\n datatype consists of dwi, bvecs, and bvals files. If your App somehow only uses the dwi file, and if you'd like to receive the path to the dwi as \ndwi\n inside the \nconfig.json\n, you can configure it as following.\n\n\n\n\nWhen a user submits your App, it will generate \nconfig.json\n that looks like this\n\n\n{\n\n    \ndwi\n:\n \n../path/to/dwi.nii.gz\n\n\n}\n\n\n\n\n\n\nYour App can parse \nconfig.json\n and use the value for \ndwi\n as the file path pointing to the input dwi image.\n\n\n\n\nNote\n\n\nIf you want to use all 3 files from \nneuro/dwi\n datatype, you have to create 3 file mappings for each files; dwi, bvecs, and bvals.\n\n\n\n\nOptional\n\n\nClick this to make this input dataset optional. Leave it unchecked if it's a required field. If you make it optional and user doesn't provide the input, Brainlife will generate \nconfig.json\n without any keys defined in the File Mapping section.\n\n\nMulti\n\n\nClick this if you'd like to allow users to select multiple input datasets. Selected datasests will be placed inside a json array. If user select only 1 dataset, it will still be placed inside a json array. For example, above \nconfig.json\n will be generated as the following.\n\n\n{\n\n    \ndwi\n:\n \n[\n\n    \n../path/to/dwi.nii.gz\n\n    \n]\n\n\n}\n\n\n\n\n\n\n\n\nNote\n\n\nThe index of the datasets listed in the array will be preserved across all File Mappings if there are more than 1 file mapping for this input.\n\n\n\n\nOutput Datasets\n\n\nSimilar to the input datasets, you can specify the datatypes of your output datasets here. It's up to developer to decide which datatype to use, and produce output files in the correct file structure / file names according to the specification of the datatype. \n\n\n\n\nDatatype Tags\n\n\nYou can add specificities / context to the selected datatype. For example, above screenshot shows this App outputs \nanat/t1w\n datatype with a tag \nacpc_aligned\n. If there is an App that only works with ACPC aligned \nanat/t1w\n as an input dataset, it can specify the same tag as a required input datatype tag to be more specific about its input dataset. \n\n\nPlease read \ndatatypes page\n for more information on datatypes.\n\n\nTag Passthrough\n\n\nSome App behaves as a \nfilter\n; it receives an input dataset and produces another dataset with the same datetype. In this circumstance, the App often needs to \nadd\n new datatype tags rather than completely replacing them. To accomplish this, you can set this field to the ID of the input dataset that you'd like to copy all datatype tags from. For example, if the input dataset contains \ndefaced\n datatype tag,  the output dataset will have both \nacpc_aligned\n and \ndefaced\n as the output datatype tags.\n\n\nDatatype File Mapping\n\n\nBy default, Brainlife expects you to generate all output files in a file structure expected by each datatype on the root of the current working directory. However, if you have more than one output datasets with the same datatype, or have multiple datasets with colliding file/directory names as specified by each datatype, you will not be able to output them all under the root of the current directory. \n\n\nTo solve this issue, you can output each dataset under a different filename or in a sub-directory, and configure the \nDatatype File Mapping\n field to override which file/dir should corresponds to which file/dir within each datatype.\n\n\nFor example, following example show that the App is producing a file called \noutput.DT_STREAM.tck\n which should treated as a \ntrack.tck\n file output for \nneuro/track\n datatype. (\"track\" is the file ID for \"track.tck\" as defined by \nneuro/track\n datatype).\n\n\n\n\nOr, if you'd like to store the entire raw output files in a sub directory called \"output\", you can specify the directory by..\n\n\n\n\n\n\nNote\n\n\nThe JSON \nkey\n for each file mapping needs to be the file/dir ID defined for each datatype. You will need to find the datatype definition to know which file/dir ID to use. Please consult Brainlife slack team if you need a help.\n\n\n\n\nraw\n datatype\n\n\nYour App may generate output data that is not meant to be used by any other Apps, at least initially. You can use \nraw\n datatype to output and archive such output data. You should avoid using \nraw\n datatype as an input datatype, however. If you are trying to use other App's \nraw\n data as your input dataset, it is probably a good indication that the upstream App developer and you should discuss and define a new datatype so that both Apps can interoperate through a well defined datatype. \n\n\nPlease contact the other developer and discuss how the data should be structured, and submit a new issue on \nbrain-life/datatypes\n and/or a pull request containing the list of files/directories to be registered on Brainlife.\n\n\nConfiguration Parameters\n\n\nConfiguration parameters allow users to enter any \nnumber\n (integer/float), \nboolean\n(true/false) or \nstring\n parameters as configuration parameters for your App. You can also define a \nenum\n parameter which lets users select from multiple options.\n\n\n\n\n\n\n\n\nPlaceholder\n \n\n\nFor each input parameter, you can set a \nplaceholder\n (a string displayed inside the form element if no value is entered yet). For example, you could use placeholder to let user know the format of the values, or some samples.\n\n\n\n\n\n\nDescription\n \n\n\nSome configuration parameters let you specify a description which will be displayed next to the input parameter to show detailed explanation for the input parameter. Please provide enough details for both novice and experienced users of your App. \n\n\n\n\nNote\n\n\n For Novice Users \n\n\nBrainlife is a platform for both novice and experienced users. For novice user, please add enough details for each configuration parameter and instruction about how to find a correct values to set for each parameters (a link to other webpage, point to README, etcs) \n\n\nPlease make as many parameters optional as possible, and auto-detect the optimal values at runtime if user does not provide/override the default option.\n\n\n For Experienced Users \n\n\nAt the same time, your App should not become a blackbox for experienced users. You should allow them to choose/override any parameters and allow them to be fully in control of how the algorithm works. \n\n\n\n\n\n\n\n\nFinally, click \nSubmit\n. Visit the Apps page to make sure everything looks OK.\n\n\nREADME / Description / Topics\n\n\nBrainlife re-uses information stored in github repo.\n\n\n\n\n\n\nApp Description / Topics\n\n\nYour Github repo description is used to display Brainlife App description. Please be sure to enter description that shows what the App does, and what user can do with the output. \n\n\nGithub topics are also used to organize Brainlife's App by placing them under various \ncategories\n. Please look through the existing categories already registered in Brainlife, and reuse one or more of those categories to help users find your App more easily. \n\n\n\n\n\n\nNote\n\n\nPlease avoid using too many \ntopics\n. Also please avoid using \ntopics\n that are not yet used by any other Apps (it will create a category with a single App)\n\n\n\n\n\n\n\n\nREADME.md\n\n\nBrainlife displays README.md content from your github repo. You can include any images, katex equations, or any other standard \nmarkdown syntax\n. \n\n\nYou should include information such as..\n\n\n\n\nWhat your App does, and how it's implemented (tools, libraries used)\n\n\nWhat you App produces and what user can do with it\n\n\nAny diagrams / sample output images\n\n\nDetails on how the algorithm works\n\n\nComputational cost / resources required to run your App (how long does it take to run, minimum required memory / cpu cores, etc..)\n\n\nHow can other users contribute (Are you accepting any PR?)\n\n\nReferences to other published papers, or list of contributors.\n\n\n\n\n\n\nNote\n\n\nBrainlife is for both novice and experienced neuroscience researchers. Please try to cater for both groups of users.\n\n\n\n\n\n\n\n\nEnabling App on resource\n\n\nOnce you registered your App on Brainlife, you then need to enable your App on resources where you can run it. A resource could be any VM, HPC cluster, or public / private cloud resource. Only the resource owners/administrators can enable your App to run on their resources. Please contact the resource administrator for the resource where you'd like to submit your App.\n\n\n\n\nNote\n\n\nIf you are not sure who the resource administrator is, please contact \nBrainlife\n.\n\n\n\n\nIf you have access to your own computing resources, you can register personal resources and run your App there to test. Please read \nregistering resource page\n for more detail. Please keep in mind that, on personal resources, only you can run enabled Apps on those resources. To allow other users to run your App, you will need to enable it on Brainlife's shared resources.\n\n\nOnce your App is enabled on various resources, you should be able to see them listed under computing resources section in the App details page.", 
            "title": "Registering App"
        }, 
        {
            "location": "/apps/register/#registering-app", 
            "text": "Once your App is published on github, you can now register it on Brainlife and let you and other users discover your App and execute on Brainlife.  First, go to the  Apps page  on Brainlife, click  Plus Button  at the bottom right corner of the page. App registration form should appear.  Let's go through each sections.", 
            "title": "Registering App"
        }, 
        {
            "location": "/apps/register/#detail", 
            "text": "Enter any  name  for your App and  Git Repository Name  field which is the the organization / repository name (like  yourname/app-name ) of your github repo. Please do not enter the full github URL.  All other fields in this section are optional, but you could populate following fields.", 
            "title": "Detail"
        }, 
        {
            "location": "/apps/register/#avatar", 
            "text": "You can enter  avatar  URL if you have URL for an avatar that you'd like to use for your App. Please choose a square image with  https://  URL (not  http:// ). Avatar may sounds superfluous, but please keep in mind that there are many other Apps registered on Brainlife and this might be the only visual queue for users to identify and search for your App. If you don't specify Avatar URL, Brainlife will use a randomly generated (robots) Avatar.", 
            "title": "Avatar"
        }, 
        {
            "location": "/apps/register/#project", 
            "text": "By default, all Apps are  public  meaning any user can find your App and execute your App. If you'd like to make your App only available on a specific project (and their group members), you can enter project names under  Project  field and only the member of that project will be able to access your App. This might be useful if you are still developing your App and wants to keep it hidden until you make a formal  release , or if your App would only function on datasets stored under a specific project.", 
            "title": "Project"
        }, 
        {
            "location": "/apps/register/#branch", 
            "text": "If you don't specify the github repo's branch name, it uses  master  branch by default. Most developer makes the latest code changes on  master  branch. If you leave the  Branch  field empty and let it use the  master  branch by default, a user won't be able to reproduce the output with exactly the same version of your code if you make any changes to it.  Once you finish developing your App, you should consider creating a release branch (like  1.0 ) and specifying that branch name for your App so that Brainlife will always run the specific version of your App. Brainlife stores the branch name used to execute each task, so this allows users to reproduce the output using the same version of the code.  Please see  Versioning Tip  for more info.", 
            "title": "Branch"
        }, 
        {
            "location": "/apps/register/#input-datasets", 
            "text": "Here you can define a list of input datasets that your App is expecting.", 
            "title": "Input Datasets"
        }, 
        {
            "location": "/apps/register/#id", 
            "text": "This is just an ID to uniquely identify this input dataset. Please enter any ID you'd like to use. It just has to be unique among all other input datasets.", 
            "title": "ID"
        }, 
        {
            "location": "/apps/register/#datatypetags", 
            "text": "The datatype/tags of this input dataset. Please enter any datatype tags that your App would require under  Datatype Tags  field. Brainlife will only allow users to select dataset that meets specified datatype tags.   Hint  If you don't know which datatype to use, please consult the #datatype slack channel on Brainlife slack team.   Please read  datatypes  for more information.", 
            "title": "Datatype/Tags"
        }, 
        {
            "location": "/apps/register/#file-mapping", 
            "text": "Once you select the datatype/tags for your input dataset, you then need to configure how you want the selected dataset to be represented in the  config.json . Each datatype consists of various files and directories. Here, you can map the object key in  config.json  to a particular file / directory within the datatype.  For example,  neuro/dwi  datatype consists of dwi, bvecs, and bvals files. If your App somehow only uses the dwi file, and if you'd like to receive the path to the dwi as  dwi  inside the  config.json , you can configure it as following.   When a user submits your App, it will generate  config.json  that looks like this  { \n     dwi :   ../path/to/dwi.nii.gz  }   Your App can parse  config.json  and use the value for  dwi  as the file path pointing to the input dwi image.   Note  If you want to use all 3 files from  neuro/dwi  datatype, you have to create 3 file mappings for each files; dwi, bvecs, and bvals.", 
            "title": "File Mapping"
        }, 
        {
            "location": "/apps/register/#optional", 
            "text": "Click this to make this input dataset optional. Leave it unchecked if it's a required field. If you make it optional and user doesn't provide the input, Brainlife will generate  config.json  without any keys defined in the File Mapping section.", 
            "title": "Optional"
        }, 
        {
            "location": "/apps/register/#multi", 
            "text": "Click this if you'd like to allow users to select multiple input datasets. Selected datasests will be placed inside a json array. If user select only 1 dataset, it will still be placed inside a json array. For example, above  config.json  will be generated as the following.  { \n     dwi :   [ \n     ../path/to/dwi.nii.gz \n     ]  }    Note  The index of the datasets listed in the array will be preserved across all File Mappings if there are more than 1 file mapping for this input.", 
            "title": "Multi"
        }, 
        {
            "location": "/apps/register/#output-datasets", 
            "text": "Similar to the input datasets, you can specify the datatypes of your output datasets here. It's up to developer to decide which datatype to use, and produce output files in the correct file structure / file names according to the specification of the datatype.", 
            "title": "Output Datasets"
        }, 
        {
            "location": "/apps/register/#datatype-tags", 
            "text": "You can add specificities / context to the selected datatype. For example, above screenshot shows this App outputs  anat/t1w  datatype with a tag  acpc_aligned . If there is an App that only works with ACPC aligned  anat/t1w  as an input dataset, it can specify the same tag as a required input datatype tag to be more specific about its input dataset.   Please read  datatypes page  for more information on datatypes.", 
            "title": "Datatype Tags"
        }, 
        {
            "location": "/apps/register/#tag-passthrough", 
            "text": "Some App behaves as a  filter ; it receives an input dataset and produces another dataset with the same datetype. In this circumstance, the App often needs to  add  new datatype tags rather than completely replacing them. To accomplish this, you can set this field to the ID of the input dataset that you'd like to copy all datatype tags from. For example, if the input dataset contains  defaced  datatype tag,  the output dataset will have both  acpc_aligned  and  defaced  as the output datatype tags.", 
            "title": "Tag Passthrough"
        }, 
        {
            "location": "/apps/register/#datatype-file-mapping", 
            "text": "By default, Brainlife expects you to generate all output files in a file structure expected by each datatype on the root of the current working directory. However, if you have more than one output datasets with the same datatype, or have multiple datasets with colliding file/directory names as specified by each datatype, you will not be able to output them all under the root of the current directory.   To solve this issue, you can output each dataset under a different filename or in a sub-directory, and configure the  Datatype File Mapping  field to override which file/dir should corresponds to which file/dir within each datatype.  For example, following example show that the App is producing a file called  output.DT_STREAM.tck  which should treated as a  track.tck  file output for  neuro/track  datatype. (\"track\" is the file ID for \"track.tck\" as defined by  neuro/track  datatype).   Or, if you'd like to store the entire raw output files in a sub directory called \"output\", you can specify the directory by..    Note  The JSON  key  for each file mapping needs to be the file/dir ID defined for each datatype. You will need to find the datatype definition to know which file/dir ID to use. Please consult Brainlife slack team if you need a help.", 
            "title": "Datatype File Mapping"
        }, 
        {
            "location": "/apps/register/#raw-datatype", 
            "text": "Your App may generate output data that is not meant to be used by any other Apps, at least initially. You can use  raw  datatype to output and archive such output data. You should avoid using  raw  datatype as an input datatype, however. If you are trying to use other App's  raw  data as your input dataset, it is probably a good indication that the upstream App developer and you should discuss and define a new datatype so that both Apps can interoperate through a well defined datatype.   Please contact the other developer and discuss how the data should be structured, and submit a new issue on  brain-life/datatypes  and/or a pull request containing the list of files/directories to be registered on Brainlife.", 
            "title": "raw datatype"
        }, 
        {
            "location": "/apps/register/#configuration-parameters", 
            "text": "Configuration parameters allow users to enter any  number  (integer/float),  boolean (true/false) or  string  parameters as configuration parameters for your App. You can also define a  enum  parameter which lets users select from multiple options.     Placeholder    For each input parameter, you can set a  placeholder  (a string displayed inside the form element if no value is entered yet). For example, you could use placeholder to let user know the format of the values, or some samples.    Description    Some configuration parameters let you specify a description which will be displayed next to the input parameter to show detailed explanation for the input parameter. Please provide enough details for both novice and experienced users of your App.    Note   For Novice Users   Brainlife is a platform for both novice and experienced users. For novice user, please add enough details for each configuration parameter and instruction about how to find a correct values to set for each parameters (a link to other webpage, point to README, etcs)   Please make as many parameters optional as possible, and auto-detect the optimal values at runtime if user does not provide/override the default option.   For Experienced Users   At the same time, your App should not become a blackbox for experienced users. You should allow them to choose/override any parameters and allow them to be fully in control of how the algorithm works.      Finally, click  Submit . Visit the Apps page to make sure everything looks OK.", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/apps/register/#readme-description-topics", 
            "text": "Brainlife re-uses information stored in github repo.    App Description / Topics  Your Github repo description is used to display Brainlife App description. Please be sure to enter description that shows what the App does, and what user can do with the output.   Github topics are also used to organize Brainlife's App by placing them under various  categories . Please look through the existing categories already registered in Brainlife, and reuse one or more of those categories to help users find your App more easily.     Note  Please avoid using too many  topics . Also please avoid using  topics  that are not yet used by any other Apps (it will create a category with a single App)     README.md  Brainlife displays README.md content from your github repo. You can include any images, katex equations, or any other standard  markdown syntax .   You should include information such as..   What your App does, and how it's implemented (tools, libraries used)  What you App produces and what user can do with it  Any diagrams / sample output images  Details on how the algorithm works  Computational cost / resources required to run your App (how long does it take to run, minimum required memory / cpu cores, etc..)  How can other users contribute (Are you accepting any PR?)  References to other published papers, or list of contributors.    Note  Brainlife is for both novice and experienced neuroscience researchers. Please try to cater for both groups of users.", 
            "title": "README / Description / Topics"
        }, 
        {
            "location": "/apps/register/#enabling-app-on-resource", 
            "text": "Once you registered your App on Brainlife, you then need to enable your App on resources where you can run it. A resource could be any VM, HPC cluster, or public / private cloud resource. Only the resource owners/administrators can enable your App to run on their resources. Please contact the resource administrator for the resource where you'd like to submit your App.   Note  If you are not sure who the resource administrator is, please contact  Brainlife .   If you have access to your own computing resources, you can register personal resources and run your App there to test. Please read  registering resource page  for more detail. Please keep in mind that, on personal resources, only you can run enabled Apps on those resources. To allow other users to run your App, you will need to enable it on Brainlife's shared resources.  Once your App is enabled on various resources, you should be able to see them listed under computing resources section in the App details page.", 
            "title": "Enabling App on resource"
        }, 
        {
            "location": "/apps/container/", 
            "text": "Containerizing App\n\n\nDocker is a software containerization tool that allow you to package your App and its dependencies into a portable \ncontainer\n that can you can run on any machine that supports Docker engine, or singularity.\n\n\nAlthough you can create a fully functional standalone Docker container for your app, for Brainlife, we recommend to containerize only the App's \nenvironment and the dependencies\n, and not include the main part of your App (your python or Matlab scripts that drives your algorithm) on your container.\n\n\nYou can use singularity to run the most ideosyncratic parts of your App which can be stored and maintained inside your github repo and injected into an environment container at runtime. Like..\n\n\nsingularity \nexec\n -e docker://brainlife/dipy:0.13 ./app.py\n\n\n\n\n\nIn above example, I am using \nbrainlife/dipy:0.13\n as an environment container, and running app.py which is stored locally (comes with the github repo for my App) and injected into my environment at runtime. \n\n\nYou can even share the same Docker container across multiple Apps that you and your lab members maintain, but you should make sure to specify the container version (\"0.13\" in above example) so that your App won't be affected when the container gets updated. \n\n\nDocker Engine\n\n\nTo build a docker container, you need to \ninstall Docker engine on your laptop\n or find a server that has docker engine installed that you can use. (Contact \nSoichi\n if you need a help.)\n\n\nWe assume you already have your Brainlife app hosted on Github, and you are making changes inside a cloned git repo on a machine with Docker engine.\n\n\nCompiling Matlab Scripts\n\n\n\n\nSkip this section if you are not using Matlab\n\n\n\n\nMatlab code requires a matlab license to run. If you are App contains any Matlab code, it needs to be compiled to a binary format using \nmcc\n command\n which allows you to execute your code without Matlab license. You will still need to run it against a special Matlab compiled runtime called \nMCR\n which can be distributed freely and does not require any license.\n\n\nYou can create a script that compiles your matlab code.\n\n\ncompile.sh\n\n\n#!/bin/bash\n\nmodule load matlab/2017a\n\nmkdir -p compiled\n\ncat \n build.m \nEND\n\n\naddpath(genpath(\n/N/u/brlife/git/vistasoft\n))\n\n\naddpath(genpath(\n/N/u/brlife/git/jsonlab\n))\n\n\naddpath(genpath(\n/N/soft/mason/SPM/spm8\n))\n\n\nmcc -m -R -nodisplay -a /N/u/brlife/git/vistasoft/mrDiffusion/templates -d compiled myapp\n\n\n\n%# function sptensor\n\n\nexit\n\n\nEND\n\nmatlab -nodisplay -nosplash -r build\n\n\n\n\n\nThis script generates a Matlab script called \nbuild.m\n and immediately executes it. \nbuild.m\n will load Matlab paths and run Matlab command called \nmcc\n which actually compiles of your Matlab code into an executable binary that can run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR which can be download from Matlab website. \n\n\n\n\nNote\nBrainlife team has built a Docker MCR container \nbrainlife/mcr\n which can be used to execute your compiled Matlab code with singularity.\n\n\n\n\n\n\nFor the sample build script above, you will need to adjust addpath() to include all of your Matlab dependencies that your Matlab script requires. \nmyapp\n is the name of the matlab entry function that you use to execute your application. mcc command will create a binary with the same name \nmyapp\n inside the ./compiled directory.\n\n\nA few other mcc options to note.\n\n\n\n\n-m ...\n tells the name of the main entry function (in this case it's \nmain\n) of your application (it reads your config.json and runs the whole application)\n\n\n-R -nodisplay\n sets the command line options you'd normally pass when you run MatLab on the command line.\n\n\n-d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized.\n\n\n\n\nIf your app or your Matlab libraries access any non-Matlab files (mexfiles, datafiles, models, templates, etc..) you will need to include them by specifying paths with \n-a\n. mcc will include specified files/directories as part of your compiled binary and make it available to the compiled code as if it is available through local filesystem (similar to how Docker containerizes things).\n\n\n\n\n\n\nIf you are using OpenMP, you will need to include libgomp1 library installed in your MCR container.\n\n\n\n\nmcc compiled application can't run certain Matlab statements; like addpath. You will need to wrap some code with \nif ~isdeployed\n type statement to prevent it from getting executed.\n\n\n\n\n\n\nNote\nIf you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround is to temporarly rename your startup.m to something else while you are compiling your code.s\n\n\n\n\n\n\nLoading custom Matlab data structure\n\n\nIf your Matlab code is loading any data structure that contains custom Matlab data structures (like sparse tensor array in encode's fe structure), addpath() is not enough to include those data structures. You will need to explicitly tell Matlab compiler that you are using those data structures, like..\n\n\n%# function sptensor\n\n\nThis odd looking \ncomment\n lets the Matlab compiler know that it should include the sptensor class to the compiled binary.\n\n\n\n\n\nCreating Docker Container\n\n\nThere are quite a few materials detailing how to write Dockerfile / containers online. If you are looking for a place to start, we recommend \nDocker's Getting Started Guide\n.\n\n\nAs most Brainlife Apps execute Docker containers through singularity, there are a few Brainlife specific items that you might need to be aware so that your App will run properly on Brainlife.\n\n\nldconfig\n\n\nAs singularity runs containers as a normal user, we must perform some OS level initialization steps outside the singularity.\n\n\nWhen Docker installs OS packages, it won't initialize dynamicly linked library links until they are first invoked. As singularity runs as normal user with set uid disabled, it won't be able to setup these paths once container is executed through singularity. To fix this, we can run \nldconfig\n command. This command creates the necessary links and cache to the most recent shared libraries installed in the lib directories. In your Dockerfile, please be sure to run ldconfig as the last command. Like...\n\n\n#make it work under singularity\nRUN ldconfig \n\n\n\n\n\nIU HPC paths\n\n\nSome OS (like RHEL6) don't allow singularity to mount a host path unless there is a corresponding directory already present inside the container (due to lack of overlayfs support by the kernel).\n\n\nTo make your App run on RHEL6 hosts (like IU Karst), please add following somewhere inside your Dockerfile\n\n\nRUN mkdir -p /N/u /N/home /N/dc2 /N/soft\n\n\n\n\n\nUse Bash\n\n\nIf you are using ubuntu, by default it changes the default /bin/sh to point to a thing called \"dash\" rather than \"bash\". \"dash\" is simpler/faster to load than \"bash\", but unfortunately it breaks a lot of programs that expects /bin/sh to point to bash. To correct this, you can add following in your Dockerfile to reset it to bash.\n\n\n#https://wiki.ubuntu.com/DashAsBinSh\nRUN rm /bin/sh \n ln -s /bin/bash /bin/sh\n\n\n\n\n\nNow, your container should be ready to GO!\n\n\nExample Dockerfile for environment containers\n\n\n\n\nbrain-life/app-dipy-workflows\n\n\nbrain-life/docker-mcr\n\n\n\n\nExamples Apps using environment containers\n\n\n\n\ngithub brain-life/app-freesurfer\n\n\ngithub brain-life/app-wmaSeg", 
            "title": "Containerizing App"
        }, 
        {
            "location": "/apps/container/#containerizing-app", 
            "text": "Docker is a software containerization tool that allow you to package your App and its dependencies into a portable  container  that can you can run on any machine that supports Docker engine, or singularity.  Although you can create a fully functional standalone Docker container for your app, for Brainlife, we recommend to containerize only the App's  environment and the dependencies , and not include the main part of your App (your python or Matlab scripts that drives your algorithm) on your container.  You can use singularity to run the most ideosyncratic parts of your App which can be stored and maintained inside your github repo and injected into an environment container at runtime. Like..  singularity  exec  -e docker://brainlife/dipy:0.13 ./app.py  In above example, I am using  brainlife/dipy:0.13  as an environment container, and running app.py which is stored locally (comes with the github repo for my App) and injected into my environment at runtime.   You can even share the same Docker container across multiple Apps that you and your lab members maintain, but you should make sure to specify the container version (\"0.13\" in above example) so that your App won't be affected when the container gets updated.", 
            "title": "Containerizing App"
        }, 
        {
            "location": "/apps/container/#docker-engine", 
            "text": "To build a docker container, you need to  install Docker engine on your laptop  or find a server that has docker engine installed that you can use. (Contact  Soichi  if you need a help.)  We assume you already have your Brainlife app hosted on Github, and you are making changes inside a cloned git repo on a machine with Docker engine.", 
            "title": "Docker Engine"
        }, 
        {
            "location": "/apps/container/#compiling-matlab-scripts", 
            "text": "Skip this section if you are not using Matlab   Matlab code requires a matlab license to run. If you are App contains any Matlab code, it needs to be compiled to a binary format using  mcc  command  which allows you to execute your code without Matlab license. You will still need to run it against a special Matlab compiled runtime called  MCR  which can be distributed freely and does not require any license.  You can create a script that compiles your matlab code.", 
            "title": "Compiling Matlab Scripts"
        }, 
        {
            "location": "/apps/container/#compilesh", 
            "text": "#!/bin/bash \nmodule load matlab/2017a\n\nmkdir -p compiled\n\ncat   build.m  END  addpath(genpath( /N/u/brlife/git/vistasoft ))  addpath(genpath( /N/u/brlife/git/jsonlab ))  addpath(genpath( /N/soft/mason/SPM/spm8 ))  mcc -m -R -nodisplay -a /N/u/brlife/git/vistasoft/mrDiffusion/templates -d compiled myapp  %# function sptensor  exit  END \nmatlab -nodisplay -nosplash -r build  This script generates a Matlab script called  build.m  and immediately executes it.  build.m  will load Matlab paths and run Matlab command called  mcc  which actually compiles of your Matlab code into an executable binary that can run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR which can be download from Matlab website.    Note Brainlife team has built a Docker MCR container  brainlife/mcr  which can be used to execute your compiled Matlab code with singularity.    For the sample build script above, you will need to adjust addpath() to include all of your Matlab dependencies that your Matlab script requires.  myapp  is the name of the matlab entry function that you use to execute your application. mcc command will create a binary with the same name  myapp  inside the ./compiled directory.  A few other mcc options to note.   -m ...  tells the name of the main entry function (in this case it's  main ) of your application (it reads your config.json and runs the whole application)  -R -nodisplay  sets the command line options you'd normally pass when you run MatLab on the command line.  -d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized.   If your app or your Matlab libraries access any non-Matlab files (mexfiles, datafiles, models, templates, etc..) you will need to include them by specifying paths with  -a . mcc will include specified files/directories as part of your compiled binary and make it available to the compiled code as if it is available through local filesystem (similar to how Docker containerizes things).    If you are using OpenMP, you will need to include libgomp1 library installed in your MCR container.   mcc compiled application can't run certain Matlab statements; like addpath. You will need to wrap some code with  if ~isdeployed  type statement to prevent it from getting executed.    Note If you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround is to temporarly rename your startup.m to something else while you are compiling your code.s", 
            "title": "compile.sh"
        }, 
        {
            "location": "/apps/container/#loading-custom-matlab-data-structure", 
            "text": "If your Matlab code is loading any data structure that contains custom Matlab data structures (like sparse tensor array in encode's fe structure), addpath() is not enough to include those data structures. You will need to explicitly tell Matlab compiler that you are using those data structures, like..  %# function sptensor  This odd looking  comment  lets the Matlab compiler know that it should include the sptensor class to the compiled binary.", 
            "title": "Loading custom Matlab data structure"
        }, 
        {
            "location": "/apps/container/#creating-docker-container", 
            "text": "There are quite a few materials detailing how to write Dockerfile / containers online. If you are looking for a place to start, we recommend  Docker's Getting Started Guide .  As most Brainlife Apps execute Docker containers through singularity, there are a few Brainlife specific items that you might need to be aware so that your App will run properly on Brainlife.", 
            "title": "Creating Docker Container"
        }, 
        {
            "location": "/apps/container/#ldconfig", 
            "text": "As singularity runs containers as a normal user, we must perform some OS level initialization steps outside the singularity.  When Docker installs OS packages, it won't initialize dynamicly linked library links until they are first invoked. As singularity runs as normal user with set uid disabled, it won't be able to setup these paths once container is executed through singularity. To fix this, we can run  ldconfig  command. This command creates the necessary links and cache to the most recent shared libraries installed in the lib directories. In your Dockerfile, please be sure to run ldconfig as the last command. Like...  #make it work under singularity\nRUN ldconfig", 
            "title": "ldconfig"
        }, 
        {
            "location": "/apps/container/#iu-hpc-paths", 
            "text": "Some OS (like RHEL6) don't allow singularity to mount a host path unless there is a corresponding directory already present inside the container (due to lack of overlayfs support by the kernel).  To make your App run on RHEL6 hosts (like IU Karst), please add following somewhere inside your Dockerfile  RUN mkdir -p /N/u /N/home /N/dc2 /N/soft", 
            "title": "IU HPC paths"
        }, 
        {
            "location": "/apps/container/#use-bash", 
            "text": "If you are using ubuntu, by default it changes the default /bin/sh to point to a thing called \"dash\" rather than \"bash\". \"dash\" is simpler/faster to load than \"bash\", but unfortunately it breaks a lot of programs that expects /bin/sh to point to bash. To correct this, you can add following in your Dockerfile to reset it to bash.  #https://wiki.ubuntu.com/DashAsBinSh\nRUN rm /bin/sh   ln -s /bin/bash /bin/sh  Now, your container should be ready to GO!", 
            "title": "Use Bash"
        }, 
        {
            "location": "/apps/container/#example-dockerfile-for-environment-containers", 
            "text": "brain-life/app-dipy-workflows  brain-life/docker-mcr", 
            "title": "Example Dockerfile for environment containers"
        }, 
        {
            "location": "/apps/container/#examples-apps-using-environment-containers", 
            "text": "github brain-life/app-freesurfer  github brain-life/app-wmaSeg", 
            "title": "Examples Apps using environment containers"
        }, 
        {
            "location": "/apps/versioning/", 
            "text": "Although Brainlife does not require you to use any specific git branching schema and you should observe any standrad practice from your own group, here are some guidelines that we suggest.\n\n\n1. git pull often\n\n\nBefore you start editing your app on your local machine each day, be sure to pull from origin. \n\n\ngit pull\n\n\n\n\n\nYou should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master.\n\n\n\n\nBy the way, see \ngit rebase\n if you are not familiar with rebasing. It helps our commit log to be clean.\n\n\n\n\n2. Always work on master branch\n\n\nWhen you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin.\n\n\n3. Create branch for new versions\n\n\nWhen we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. The easiest way to create a new branch is to do this on github UI. Just click on where it says \"Branch: master\" and enter new version number to create a new branch. The branch name should be a version number .. like \"1.0\", \"1.1\", \"1.2\", etc.., it should not be a branch names like the local branches you create on your local repo.\n\n\nOnce you create a branch, you should update the BL app to point users to use that new branch.\n\n\nYou could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also.\n\n\n4. Bug fix on master, then on branch.\n\n\nIf you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I recommend using command like \ncherry-pick\n to apply specific changes on other branches. Again, we should not add any new features on branches (only bug fixes) for reproducibility. \n\n\n5. Semver\n\n\nIf you don't know what semantic versioning is, please read \nhttps://semver.org/\n.\n\n\nFor branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new Brainlife app with different major version number so that user can continue to submit your app with previous versions.", 
            "title": "Versioning Tips"
        }, 
        {
            "location": "/apps/versioning/#1-git-pull-often", 
            "text": "Before you start editing your app on your local machine each day, be sure to pull from origin.   git pull  You should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master.   By the way, see  git rebase  if you are not familiar with rebasing. It helps our commit log to be clean.", 
            "title": "1. git pull often"
        }, 
        {
            "location": "/apps/versioning/#2-always-work-on-master-branch", 
            "text": "When you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin.", 
            "title": "2. Always work on master branch"
        }, 
        {
            "location": "/apps/versioning/#3-create-branch-for-new-versions", 
            "text": "When we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. The easiest way to create a new branch is to do this on github UI. Just click on where it says \"Branch: master\" and enter new version number to create a new branch. The branch name should be a version number .. like \"1.0\", \"1.1\", \"1.2\", etc.., it should not be a branch names like the local branches you create on your local repo.  Once you create a branch, you should update the BL app to point users to use that new branch.  You could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also.", 
            "title": "3. Create branch for new versions"
        }, 
        {
            "location": "/apps/versioning/#4-bug-fix-on-master-then-on-branch", 
            "text": "If you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I recommend using command like  cherry-pick  to apply specific changes on other branches. Again, we should not add any new features on branches (only bug fixes) for reproducibility.", 
            "title": "4. Bug fix on master, then on branch."
        }, 
        {
            "location": "/apps/versioning/#5-semver", 
            "text": "If you don't know what semantic versioning is, please read  https://semver.org/ .  For branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new Brainlife app with different major version number so that user can continue to submit your app with previous versions.", 
            "title": "5. Semver"
        }, 
        {
            "location": "/apps/customhooks/", 
            "text": "Life cycles hooks are the script used to start / stop / monitor your apps by Brain-Life. By default, it looks for executable installed on each resource in the PATH with named \nstart\n, \nstatus\n, and \nstop\n. Resource owner needs to make sure these scripts are installed and accessible by your apps. \n\n\n\n\nFor most PBS, SLURM, and vanila VM, resource owner can install \nABCD default hooks\n.\n\n\n\n\nBy default, \nstart\n hook should look for a file named \nmain\n to start your app. Therefore, the only file required to make your app runnable by Brain-Life is this \nmain\n executable on the root directory of the app's git repository. \n\n\nUnder most circumstances, app developers shouldn't have to worry about these hook scripts. However, if your app requires some special mechanism to start / stop and monitor your app, you might need to provide your own hook scripts. \n\n\nYou can specify the paths to these hook scripts by creating a file named \npackage.json\n\n\n{\n\n  \nbrainlife\n:\n \n{\n\n    \nstart\n:\n \n./start.sh\n,\n\n    \nstop\n:\n \n./stop.sh\n,\n\n    \nstatus\n:\n \n./status.sh\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThen, you will need to provide those hook scripts as part of your app.\n\n\n\n\nPlease be sure to \nchmod +x *.sh\n so that your hook scripts are executable.\n\n\n\n\nstart.sh\n\n\nFollowing is an example for \nstart\n script. It submits a file named \nmain\n (should be provided by each app) through qsub. It stores \njobid\n so that we can monitor the job status.\n\n\n1\n2\n3\n4\n5\n6\n#!/bin/bash\n\n\n\n#return code 0 = job started successfully.\n\n\n#return code non-0 = job failed to start\n\n\nqsub -d \n$PWD\n -V -o \n\\$\nPBS_JOBID.log -e \n\\$\nPBS_JOBID.err main \n jobid\n\n\n\n\n\n\nstop.sh\n\n\nFollowing is an example for \nstop\n script. This scripts reads the jobid created by \nstart\n script and call qdel to stop it.\n\n\n1\n2\n#!/bin/bash\n\nqdel \n`\ncat jobid\n`\n\n\n\n\n\n\n\nstatus.sh\n\n\nstatus hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the \njobid\n stored by start script to query the job status with \nqstat\n PBS command. \n\n\nAnything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n#!/bin/bash\n\n\n\n#return code 0 = running\n\n\n#return code 1 = finished successfully\n\n\n#return code 2 = failed\n\n\n#return code 3 = unknown (retry later)\n\n\n\nif\n \n[\n ! -f jobid \n]\n;\nthen\n\n    \necho\n \nno jobid - not yet submitted?\n\n    \nexit\n \n1\n\n\nfi\n\n\n\njobid\n=\n`\ncat jobid\n`\n\n\nif\n \n[\n -z \n$jobid\n \n]\n;\n \nthen\n\n    \necho\n \njobid is empty.. failed to submit?\n\n    \nexit\n \n3\n\n\nfi\n\n\n\njobstate\n=\n`\nqstat -f \n$jobid\n \n|\n grep job_state \n|\n cut -b17\n`\n\n\nif\n \n[\n -z \n$jobstate\n \n]\n;\n \nthen\n\n    \necho\n \nJob removed before completing - maybe timed out?\n\n    \nexit\n \n2\n\n\nfi\n\n\n\ncase\n \n$jobstate\n in\nQ\n)\n\n    showstart \n$jobid\n \n|\n grep start\n    \nexit\n \n0\n\n    \n;;\n\nR\n)\n\n    \n#get last line of last log touched\n\n    \nlogfile\n=\n$(\nls -rt *.log \n|\n tail -1\n)\n\n    tail -1 \n$logfile\n\n    \nexit\n \n0\n\n    \n;;\n\nH\n)\n\n    \necho\n \nJob held.. waiting\n\n    \nexit\n \n0\n\n    \n;;\n\nC\n)\n\n    \nexit_status\n=\n`\nqstat -f \n$jobid\n \n|\n grep exit_status \n|\n cut -d\n=\n -f2 \n|\n xargs\n`\n\n    \nif\n \n[\n \n$exit_status\n -eq \n0\n \n]\n;\n \nthen\n\n        \necho\n \nfinished with code 0\n\n        \nexit\n \n1\n\n    \nelse\n\n        \necho\n \nfinished with code \n$exit_status\n\n        \nexit\n \n2\n\n    \nfi\n\n    \n;;\n\n*\n)\n\n    \necho\n \nunknown job status \n$jobstate\n .. will check later\n\n    \nexit\n \n3\n\n    \n;;\n\n\n\nesac", 
            "title": "Custom Hooks"
        }, 
        {
            "location": "/apps/customhooks/#startsh", 
            "text": "Following is an example for  start  script. It submits a file named  main  (should be provided by each app) through qsub. It stores  jobid  so that we can monitor the job status.  1\n2\n3\n4\n5\n6 #!/bin/bash  #return code 0 = job started successfully.  #return code non-0 = job failed to start \n\nqsub -d  $PWD  -V -o  \\$ PBS_JOBID.log -e  \\$ PBS_JOBID.err main   jobid", 
            "title": "start.sh"
        }, 
        {
            "location": "/apps/customhooks/#stopsh", 
            "text": "Following is an example for  stop  script. This scripts reads the jobid created by  start  script and call qdel to stop it.  1\n2 #!/bin/bash \nqdel  ` cat jobid `", 
            "title": "stop.sh"
        }, 
        {
            "location": "/apps/customhooks/#statussh", 
            "text": "status hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the  jobid  stored by start script to query the job status with  qstat  PBS command.   Anything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55 #!/bin/bash  #return code 0 = running  #return code 1 = finished successfully  #return code 2 = failed  #return code 3 = unknown (retry later)  if   [  ! -f jobid  ] ; then \n     echo   no jobid - not yet submitted? \n     exit   1  fi  jobid = ` cat jobid `  if   [  -z  $jobid   ] ;   then \n     echo   jobid is empty.. failed to submit? \n     exit   3  fi  jobstate = ` qstat -f  $jobid   |  grep job_state  |  cut -b17 `  if   [  -z  $jobstate   ] ;   then \n     echo   Job removed before completing - maybe timed out? \n     exit   2  fi  case   $jobstate  in\nQ ) \n    showstart  $jobid   |  grep start\n     exit   0 \n     ;; \nR ) \n     #get last line of last log touched \n     logfile = $( ls -rt *.log  |  tail -1 ) \n    tail -1  $logfile \n     exit   0 \n     ;; \nH ) \n     echo   Job held.. waiting \n     exit   0 \n     ;; \nC ) \n     exit_status = ` qstat -f  $jobid   |  grep exit_status  |  cut -d =  -f2  |  xargs ` \n     if   [   $exit_status  -eq  0   ] ;   then \n         echo   finished with code 0 \n         exit   1 \n     else \n         echo   finished with code  $exit_status \n         exit   2 \n     fi \n     ;; \n* ) \n     echo   unknown job status  $jobstate  .. will check later \n     exit   3 \n     ;;  esac", 
            "title": "status.sh"
        }, 
        {
            "location": "/resources/register/", 
            "text": "Background\n\n\nBrainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled. \n\n\nAs Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases.\n\n\n\n\nYou have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control.\n\n\nYou are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging.\n\n\nYou are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide.\n\n\n\n\nCurrently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact \nBrainlife Admin\n\n\n\n\nNote\n\n\nResource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact \nBrainlife Admin\n to enable your app on Brainlife default resources.\n\n\n\n\n\n\nWarning\n\n\nAlthough we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security\nespecially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information).\n\n\n\n\nRegistering Resources\n\n\nTo register your resource, go to \nBrainlife Settings\n page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields.\n\n\n\n\nName\n Enter the name of rhe resource\n\n\nHostname\n The hostname of your compute resource (usually a login/submit host)\n\n\nUsername\n Username used to ssh to this resource\n\n\nWorkdir\n Directory used to stage and store generated datasets by apps. \nYou should not share the same directory with other resources\n. Please make sure that the specified directory exits (mkdir if not).\n\n\nSSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read \nauthorized_keys\n for more detail.\n\n\n\n\nYou can leave the rest of the fields empty for now.\n\n\nClick OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good.\n\n\nConfiguring Resources\n\n\nOnce you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps.\n\n\nABCD Default Hooks\n\n\nABCD Hooks\n are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting \n$PATH\n. If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following.\n\n\ncd ~\ngit clone https://github.com/brain-life/abcd-spec\n\n\n\n\n\nThen, add one of following to your ~/.bashrc\n\n\nFor PBS cluster\n\n\nexport PATH=~/abcd-spec/hooks/pbs:$PATH\n\n\n\n\n\nFor Slurm cluster\n\n\nexport PATH=~/abcd-spec/hooks/slurm:$PATH\n\n\n\n\n\nFor direct execution - no batch submission manager\n\n\nexport PATH=~/abcd-spec/hooks/direct:$PATH\n\n\n\n\n\nCommon Binaries\n\n\nBrainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed.\n\n\n\n\njq (command line json parser commonly used by Brainlife apps to parse config.json)\n\n\ngit (used to clone / update apps installed)\n\n\nsingularity (user level container execution engine)\n\n\n\n\nFor IU HPC resource, please feel free to use following ~/bin directory which contains jq\n\n\n$ ~/.bashrc\n\nexport\n \nPATH\n=\n$PATH\n:/N/u/brlife/Carbonate/bin\n\n\n\n\n\nFor singularity, you can either install it on the system, or for most HPC systems you can simply add following in your \n~/.modules\n file.\n\n\nmodule\n \nload\n \nsingularity\n\n\n\n\n\n\nBy default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc\n\n\nexport SINGULARITY_CACHEDIR=/N/dc2/scratch/\nusername\n/singularity-cachedir\n\n\n\n\n\n\n\nPlease replace \n with your username, and make sure specified directories exists.\n\n\n\n\nOther ENV parameters\n\n\nDepending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via \nFREESURFER_LICENSE\n.\n\n\nexport FREESURFER_LICENSE=\nhayashis@iu.edu 29511 *xxxxxxxxxxx xxxxxxxxxxx\n\n\n\n\n\n\nEnabling Apps\n\n\nOnce you have registered and tested your resource, you can now enable apps to run on your resource.\n\n\nGo back to the Brainlife's \nresource settings page\n, and click the resource you have created. Under the services section, enter the git org/repo name (such as like \nbrain-life/app-life\n) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK.\n\n\nYou can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife. \nexample", 
            "title": "Registering Resource"
        }, 
        {
            "location": "/resources/register/#background", 
            "text": "Brainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled.   As Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases.   You have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control.  You are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging.  You are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide.   Currently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact  Brainlife Admin   Note  Resource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact  Brainlife Admin  to enable your app on Brainlife default resources.    Warning  Although we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security\nespecially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information).", 
            "title": "Background"
        }, 
        {
            "location": "/resources/register/#registering-resources", 
            "text": "To register your resource, go to  Brainlife Settings  page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields.   Name  Enter the name of rhe resource  Hostname  The hostname of your compute resource (usually a login/submit host)  Username  Username used to ssh to this resource  Workdir  Directory used to stage and store generated datasets by apps.  You should not share the same directory with other resources . Please make sure that the specified directory exits (mkdir if not).  SSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read  authorized_keys  for more detail.   You can leave the rest of the fields empty for now.  Click OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good.", 
            "title": "Registering Resources"
        }, 
        {
            "location": "/resources/register/#configuring-resources", 
            "text": "Once you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps.", 
            "title": "Configuring Resources"
        }, 
        {
            "location": "/resources/register/#abcd-default-hooks", 
            "text": "ABCD Hooks  are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting  $PATH . If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following.  cd ~\ngit clone https://github.com/brain-life/abcd-spec  Then, add one of following to your ~/.bashrc", 
            "title": "ABCD Default Hooks"
        }, 
        {
            "location": "/resources/register/#for-pbs-cluster", 
            "text": "export PATH=~/abcd-spec/hooks/pbs:$PATH", 
            "title": "For PBS cluster"
        }, 
        {
            "location": "/resources/register/#for-slurm-cluster", 
            "text": "export PATH=~/abcd-spec/hooks/slurm:$PATH", 
            "title": "For Slurm cluster"
        }, 
        {
            "location": "/resources/register/#for-direct-execution-no-batch-submission-manager", 
            "text": "export PATH=~/abcd-spec/hooks/direct:$PATH", 
            "title": "For direct execution - no batch submission manager"
        }, 
        {
            "location": "/resources/register/#common-binaries", 
            "text": "Brainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed.   jq (command line json parser commonly used by Brainlife apps to parse config.json)  git (used to clone / update apps installed)  singularity (user level container execution engine)   For IU HPC resource, please feel free to use following ~/bin directory which contains jq  $ ~/.bashrc export   PATH = $PATH :/N/u/brlife/Carbonate/bin  For singularity, you can either install it on the system, or for most HPC systems you can simply add following in your  ~/.modules  file.  module   load   singularity   By default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc  export SINGULARITY_CACHEDIR=/N/dc2/scratch/ username /singularity-cachedir   Please replace   with your username, and make sure specified directories exists.", 
            "title": "Common Binaries"
        }, 
        {
            "location": "/resources/register/#other-env-parameters", 
            "text": "Depending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via  FREESURFER_LICENSE .  export FREESURFER_LICENSE= hayashis@iu.edu 29511 *xxxxxxxxxxx xxxxxxxxxxx", 
            "title": "Other ENV parameters"
        }, 
        {
            "location": "/resources/register/#enabling-apps", 
            "text": "Once you have registered and tested your resource, you can now enable apps to run on your resource.  Go back to the Brainlife's  resource settings page , and click the resource you have created. Under the services section, enter the git org/repo name (such as like  brain-life/app-life ) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK.  You can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife.  example", 
            "title": "Enabling Apps"
        }, 
        {
            "location": "/technical/arthitecture/", 
            "text": "Brainlife Architecture\n\n\n\n\nTODO..", 
            "title": "Architecture"
        }, 
        {
            "location": "/technical/arthitecture/#brainlife-architecture", 
            "text": "TODO..", 
            "title": "Brainlife Architecture"
        }, 
        {
            "location": "/technical/api/", 
            "text": "Brainlife API\n\n\nThis document describes some of Brainlife's microservices in case you might be interested in directly interfacing with them through APIs.\n\n\nWarehouse\n\n\nWarehouse is the main application responsible for bulk of Brainlife platform UI.\n\n\n\n\nWarehouse Github\n\n\nWarehouse API Doc\n\n\n\n\n\n\nNote\n\n\nBrainlife CLI interacts with Warehouse API to import / export datasets, query task status, and among other things. At the moment, we have a very limited CLI support, but please try using our CLI tool if you just want to interface with Brainlife Warehouse via commandline \nCLI Github\n\n\n\n\nAmaretti\n\n\nAmaretti\n is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information.\n\n\n\n\nAmaretti Doc\n\n\nAMaretti Github\n\n\nAmaretti API Doc\n\n\n\n\nAuthentication Service\n\n\n\n\nAuth Github\n\n\nAuth API Doc\n\n\n\n\nEvent Service\n\n\n\n\nEvent Github\n\n\nEvent API Doc\n\n\n\n\nProfile Service\n\n\n\n\nProfile Github\n\n\nProfile API Doc", 
            "title": "APIs"
        }, 
        {
            "location": "/technical/api/#brainlife-api", 
            "text": "This document describes some of Brainlife's microservices in case you might be interested in directly interfacing with them through APIs.", 
            "title": "Brainlife API"
        }, 
        {
            "location": "/technical/api/#warehouse", 
            "text": "Warehouse is the main application responsible for bulk of Brainlife platform UI.   Warehouse Github  Warehouse API Doc    Note  Brainlife CLI interacts with Warehouse API to import / export datasets, query task status, and among other things. At the moment, we have a very limited CLI support, but please try using our CLI tool if you just want to interface with Brainlife Warehouse via commandline  CLI Github", 
            "title": "Warehouse"
        }, 
        {
            "location": "/technical/api/#amaretti", 
            "text": "Amaretti  is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information.   Amaretti Doc  AMaretti Github  Amaretti API Doc", 
            "title": "Amaretti"
        }, 
        {
            "location": "/technical/api/#authentication-service", 
            "text": "Auth Github  Auth API Doc", 
            "title": "Authentication Service"
        }, 
        {
            "location": "/technical/api/#event-service", 
            "text": "Event Github  Event API Doc", 
            "title": "Event Service"
        }, 
        {
            "location": "/technical/api/#profile-service", 
            "text": "Profile Github  Profile API Doc", 
            "title": "Profile Service"
        }, 
        {
            "location": "/contact/", 
            "text": "Contact\n\n\nEmail\n\n\nBranlife \nbrlife@iu.edu\n\n\nFranco Pestilli \nfrakkopesto@gmail.com\n\n\nNewsletter Subscription\n\n\nBrainlife Users Newsletter\n\n\nBrainlife App Developers Newsletter\n\n\nSlack\n\n\nJoin brainlife.slack.com\n \n\n\nBug Report / Feature Request\n\n\nBrainlife UI Issues\n\n\nBrainlife Authentication Issues\n\n\n\n\nNote\n\n\nFor Brainlife App specific issue, please contact individual developers for each App via the github issues.", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#contact", 
            "text": "", 
            "title": "Contact"
        }, 
        {
            "location": "/contact/#email", 
            "text": "Branlife  brlife@iu.edu  Franco Pestilli  frakkopesto@gmail.com", 
            "title": "Email"
        }, 
        {
            "location": "/contact/#newsletter-subscription", 
            "text": "Brainlife Users Newsletter  Brainlife App Developers Newsletter", 
            "title": "Newsletter Subscription"
        }, 
        {
            "location": "/contact/#slack", 
            "text": "Join brainlife.slack.com", 
            "title": "Slack"
        }, 
        {
            "location": "/contact/#bug-report-feature-request", 
            "text": "Brainlife UI Issues  Brainlife Authentication Issues   Note  For Brainlife App specific issue, please contact individual developers for each App via the github issues.", 
            "title": "Bug Report / Feature Request"
        }, 
        {
            "location": "/privacy/", 
            "text": "Effective: 2018-05-01\n\n\n\nApplicability\n\n\n\nThis privacy notice applies only to the brainlife.io (https://brainlife.io) and explains our practices concerning the collection, use, and disclosure of visitor information.  Visitor information collected by brainlife.io will be used only as outlined in this privacy notice.\n\n\n\nOther units at the university may collect and use visitor information in different ways.  Therefore, visitors to other university websites should review the privacy notices for the particular sites they visit.  brainlife.io is not responsible for the content of other websites or for the privacy practices of websites outside the scope of this notice.\n\n\n\nChanges\n\n\n\nBecause Internet technologies continue to evolve rapidly, brainlife.io may make appropriate changes to this notice in the future.  Any such changes will be consistent with our commitment to respecting visitor privacy, and will be clearly posted in a revised privacy notice.\n\n\n\nCollection and Use\n\n\n\nPassive/Automatic Collection\n\n\n\nWhen you view pages on our site, the web server automatically collects certain technical information from your computer and about your connection including: \n\n\n\nyour IP address\nthe domain name from which you visit our site\nuser-specific information on which pages are visited\naggregate information on pages visited\nthe referring website\nthe date and time of visit\nthe duration of visit\nyour browser type\nyour screen resolution\nAsset accessed on the platform\n\n\n\nContinued use of our website indicates consent to the collection, use, and disclosure of this information as described in this notice.\n\n\n\nThis technical information is retained in detail for up to 180 days days.\n\nActive/Manual/Voluntary Collection\nOther than automatically collected technical information about your visit (described above, or cookies, described below), we may ask you to provide information voluntarily, such as through forms or other manual input\u2014in order to make products and services available to you, to maintain and manage our relationship with you, including providing associated services or to better understand and serve your needs. This information is generally retained as long as you continue to maintain a relationship with us. Your providing this information is wholly voluntary. However, not providing the requested information (or subsequently asking that the data be removed) may affect our ability to deliver the products or service for which the information is needed. Providing the requested information indicates your consent to the collection, use, and disclosure of this information as described in this notice. Information we may actively collect could include:\nthe email addresses of those who communicate with us via email\nthe email addresses of those who make postings to our chat areas\nname\nInformation Usage\n\n\nThis information is:\n\n\nused to customize the content of our site\nused to notify visitors about updates to our site\n\n\nAdditional Information Related to Information Collection\n\n\nn/a\n\n\nInformation Used For Contact\nIf you supply us with your postal/mailing address:\nYou will only receive the information for which you provided us your address.\nInformation Sharing\nWe do not share any aggregate, non-personally identifiable information with other entities or organizations.\nWe do not share any personally identifiable information with other entities or organizations, except when legally required to do so, at the request of governmental authorities conducting an investigation, to verify or enforce compliance with the policies governing our website and applicable laws, or to protect against misuse or unauthorized use of our website.\nExcept as described above, we will not share any information with any party for any reason.\nExcept as provided in the Disclosure of Information section below, we do not attempt to use the technical information discussed in this section to identify individual visitors.\n\n\nAdditional Information Related to Information Use\n\n\nn/a\n\n\n\nCookies\n\n\n\nA \ncookie\n is a small data file that is written to your hard drive that contains information about your visit to a web page. If you prefer not to receive cookies, you may configure your browser not to accept them at all, or to notify and require approval before accepting new cookies. Some web pages/sites may not function properly if the cookies are turned off, or you may have to provide the same information each time you visit those pages.\n\n\n\nOur site does not use cookies to store information about your actions or choices on pages associated with our site.\n\n\n\nChildren\n\n\n\nThis site is not directed to children under 13 years of age, does not sell products or services intended for purchase by children, and does not knowingly collect or store any personal information, even in aggregate, about children under the age of 13.  We encourage parents and teachers to be involved in children\u2019s Internet explorations.  It is particularly important for parents to guide their children when they are asked to provide personal information online.\n\n\n\nUse of Third Party Services\n\n\n\nThis website uses Google Analytics, a web analytics service provided by Google, Inc. (\"Google\").  Google Analytics uses cookies (described above) to help the website analyze how users use the site. The information generated by the cookie about your use of the website (including possibly your IP address) will be transmitted to and stored by Google.\n\n\n\nFor more information, please visit \nGoogle\u2019s Privacy Policy\n.\n\n\n\nAdditional Information Related to the Use of Third Party Services\n\n\n\nhttps://disqus.comrn\n\n\n\nUpdating Inaccurate Information\n\n\n\nIn some cases, we will grant visitors the ability to update or correct inaccuracies in the information that we maintain.\n\n\n\nVisitors may correct inaccuracies in:\n\n\n\ncontact information that we have on file\n\n\n\nVisitors can have this information corrected by:\n\n\n\nsending us email at the listed address\nvisiting us at the following URL: \nbrainlife.io\nbrlife@iu.edu\n\n\n\nDisclosure of Information\n\n\n\nOther than sharing your information with other appropriate university personnel and units to ensure the quality, functionality, and security of our website, or manage your relationship with us, we will not disclose personally identifiable information about your use of the site except under the following circumstances:\n\n\n\n\n\nWith your prior written (including email) consent\n\n\nWhen we have given you clear notice that we will disclose information that you voluntarily provide\n\n\nWith appropriate external parties, such as law enforcement agencies, in order to investigate and respond to suspected violations of law or university policy.  Any such disclosures shall comply with all applicable laws and university policies.\n\n\n\n\n\nSecurity\n\n\n\nDue to the rapidly evolving nature of information technologies, no transmission of information over the Internet can be guaranteed to be completely secure. While Indiana University is committed to protecting user privacy, IU cannot guarantee the security of any information users transmit to university sites, and users do so at their own risk.\n\n\n\nWe have appropriate security measures in place in our physical facilities to protect against the loss, misuse, or alteration of information that we have collected from you at our site.\n\n\n\nOnce we receive user information, we will use reasonable safeguards consistent with prevailing industry standards and commensurate with the sensitivity of the data being stored to maintain the security of that information on our systems.\n\n\n\nWe will comply with all applicable federal, state and local laws regarding the privacy and security of user information.\n\n\n\nLinks to non-university sites\n\n\n\nIndiana University is not responsible for the availability, content, or privacy practices of non-university sites. Non-university sites are not bound by this site privacy notice policy and may or may not have their own privacy policies.\n\n\n\nPrivacy Notice Changes\n\n\n\nFrom time to time, we may use visitor information for new, unanticipated uses not previously disclosed in our privacy notice.\n\n\n\nWe will post the policy changes to our website to notify you of these changes and provide you with the ability to opt out of these new uses. If you are concerned about how your information is used, you should check back at our website periodically.\n\n\n\nVisitors may prevent their information from being used for purposes other than those for which it was originally collected by:\n\n\n\nsending us an email at the listed address\nbrlife@iu.edu\n\n\n\nSupplemental Information\n\n\n\nn/a\n\n\n\nContact Information\n\n\n\nIf you have questions or concerns about this policy, please contact us.\n\n\n\nbrainlife.io\nATTN: Franco Pestilli\n1101 East 10th Street, Department of Psychological and Brain\nDepartment of Psychological and Brain Sciences\nBloomington, Indiana 47405\nbrlife@iu.edu\n8128569967\n\n\n\nIf you feel as though this site\u2019s privacy practices differ from the information stated, you may contact us at the listed address or phone number. \nIf you feel that this site is not following its stated policy and communicating with the owner of this site does not resolve the matter, or if you have general questions or concerns about privacy or information technology policy at Indiana University, please contact the chief privacy officer through the University Information Policy Office, 812-855-UIPO, \nprivacy@iu.edu\n.", 
            "title": "Privacy Policy"
        }, 
        {
            "location": "/releases/", 
            "text": "Upcoming\n\n\n First Production Release\n\n\n September 2018?\n\n\nTBD..\n\n\n Beta 2\n\n\n August 2018?\n\n\nTBD...\n\n\n Beta Release\n\n\n April 8th, 2018\n\n\n(warehouse v1.1.2) \n\n\nOur first official \nrelease\n! We now have most of our features functional and ready to be used by users outside our labs. We've developed an initial set of documentations and defined or identified standard operating protocols needed for sustainable operation of this platform.\n\n\nPast Releases", 
            "title": "Releases"
        }, 
        {
            "location": "/releases/#upcoming", 
            "text": "", 
            "title": "Upcoming"
        }, 
        {
            "location": "/releases/#first-production-release", 
            "text": "", 
            "title": "First Production Release"
        }, 
        {
            "location": "/releases/#september-2018", 
            "text": "TBD..", 
            "title": "September 2018?"
        }, 
        {
            "location": "/releases/#beta-2", 
            "text": "", 
            "title": "Beta 2"
        }, 
        {
            "location": "/releases/#august-2018", 
            "text": "TBD...", 
            "title": "August 2018?"
        }, 
        {
            "location": "/releases/#beta-release", 
            "text": "", 
            "title": "Beta Release"
        }, 
        {
            "location": "/releases/#april-8th-2018", 
            "text": "(warehouse v1.1.2)   Our first official  release ! We now have most of our features functional and ready to be used by users outside our labs. We've developed an initial set of documentations and defined or identified standard operating protocols needed for sustainable operation of this platform.", 
            "title": "April 8th, 2018"
        }, 
        {
            "location": "/releases/#past-releases", 
            "text": "", 
            "title": "Past Releases"
        }
    ]
}