{
    "docs": [
        {
            "location": "/", 
            "text": "Brainlife Documentation\n\n\nWelcome to Brainlife documentation! This site contains up-to-date information about the Brainlife platform. The information is organized based on\nour target audiences. We classify our users into following groups.\n\n\n\n\nEnd Users: Users who use Brainlife to run apps on their data to generate data derivatives, publish results.\n\n\nApp Developers: Users who develops apps for Briainlife.\n\n\nResource Provider: Users who provides computing resource for Brainlife. \n\n\nData Publisher: Users who use Brainlife to publish their datasets for others to use\n\n\nLearner: Users who are not familiar with neuroscience and wants to use Brainlife as an educational tool.\n\n\n\n\nContact\n\n\nFor General Inquery \n \nbrlife@iu.edu\n.\n\n\nSlack (online chat support) \n \nbrainlife.slack.com\n \n\n\n\n\nWarning\n\n\nSlack auto-inviter not ready yet.. please \nsend us an email\n to invite you in!\n\n\n\n\nContact PI \n \nFranco Pestilli\n\n\nBug report / feature request \n \nGithub Issues", 
            "title": "Home"
        }, 
        {
            "location": "/#brainlife-documentation", 
            "text": "Welcome to Brainlife documentation! This site contains up-to-date information about the Brainlife platform. The information is organized based on\nour target audiences. We classify our users into following groups.   End Users: Users who use Brainlife to run apps on their data to generate data derivatives, publish results.  App Developers: Users who develops apps for Briainlife.  Resource Provider: Users who provides computing resource for Brainlife.   Data Publisher: Users who use Brainlife to publish their datasets for others to use  Learner: Users who are not familiar with neuroscience and wants to use Brainlife as an educational tool.", 
            "title": "Brainlife Documentation"
        }, 
        {
            "location": "/#contact", 
            "text": "For General Inquery    brlife@iu.edu .  Slack (online chat support)    brainlife.slack.com     Warning  Slack auto-inviter not ready yet.. please  send us an email  to invite you in!   Contact PI    Franco Pestilli  Bug report / feature request    Github Issues", 
            "title": "Contact"
        }, 
        {
            "location": "/users/project/", 
            "text": "Projects\n\n\nProject contains your datasets, executed processes, publication records, and pipeline rules.\n\n\nEach project can be set to use either \npublic\n or \nprivate\n access policy, and you can define a group of users to be either \nadmins\n, \nmembers\n or both.  Only the project \nadministrators\n are allowed to update the project access policy, and edit \nadmins\n and \nmembers\n of the project.\n\n\nPrivate\n project allows only the project members to download, or use the datasets to process data. Only the admins and project members can find the project metadata.\n\n\nPublic\n project allows anyone registered on Brainlife to download and use the datasets to process data. \n\n\nFollowing table show who can perform which action under Public and Private project.\n\n\n\n\n\n\n\n\nAction\n\n\nPublic Project\n\n\nPrivate Project\n\n\n\n\n\n\n\n\n\n\nUpdate project detail\n\n\nAdmin\n\n\nAdmin\n\n\n\n\n\n\nSee project detail\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nList datasets\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nDownload datasets\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nUpdate dataset detail\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nCreate publication record\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nUpdate publication record\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nUpload datasets\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nList processes\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nSubmit new process\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nAccess process output\n\n\nAdmin / Members\n\n\nAdmin / Members\n\n\n\n\n\n\nList published datasets\n\n\nAdmin / Members / Guest\n\n\nAdmin / Members / Guest\n\n\n\n\n\n\nList publication records\n\n\nAdmin / Members / Guest\n\n\nAdmin / Members / Guest\n\n\n\n\n\n\nDownload published datasets\n\n\nAdmin / Members / Guest\n\n\nAdmin / Members / Guest\n\n\n\n\n\n\n\n\nDataset Archive\n\n\nOnce you store your dataset on project dataset page, they will be stored in Brainlife \narchive\n and be stored until you remove them. Once you store your dataset, the content of the data (the raw files) can not be modified, although you can still update the description, metadata, and tags. When you submit a process, the datasets from the archive will be automatically staged out of the archive to the remote computing resources that you have access to.\n\n\n\n\nNote\n\n\nWhen you create a publications record, datasets that are part of the publication will be copied to a secure tape archive (IU SDA/HPSS system)\nfor added data security.", 
            "title": "Project"
        }, 
        {
            "location": "/users/project/#projects", 
            "text": "Project contains your datasets, executed processes, publication records, and pipeline rules.  Each project can be set to use either  public  or  private  access policy, and you can define a group of users to be either  admins ,  members  or both.  Only the project  administrators  are allowed to update the project access policy, and edit  admins  and  members  of the project.  Private  project allows only the project members to download, or use the datasets to process data. Only the admins and project members can find the project metadata.  Public  project allows anyone registered on Brainlife to download and use the datasets to process data.   Following table show who can perform which action under Public and Private project.     Action  Public Project  Private Project      Update project detail  Admin  Admin    See project detail  Admin / Members  Admin / Members    List datasets  Admin / Members  Admin / Members    Download datasets  Admin / Members  Admin / Members    Update dataset detail  Admin / Members  Admin / Members    Create publication record  Admin / Members  Admin / Members    Update publication record  Admin / Members  Admin / Members    Upload datasets  Admin / Members  Admin / Members    List processes  Admin / Members  Admin / Members    Submit new process  Admin / Members  Admin / Members    Access process output  Admin / Members  Admin / Members    List published datasets  Admin / Members / Guest  Admin / Members / Guest    List publication records  Admin / Members / Guest  Admin / Members / Guest    Download published datasets  Admin / Members / Guest  Admin / Members / Guest", 
            "title": "Projects"
        }, 
        {
            "location": "/users/project/#dataset-archive", 
            "text": "Once you store your dataset on project dataset page, they will be stored in Brainlife  archive  and be stored until you remove them. Once you store your dataset, the content of the data (the raw files) can not be modified, although you can still update the description, metadata, and tags. When you submit a process, the datasets from the archive will be automatically staged out of the archive to the remote computing resources that you have access to.   Note  When you create a publications record, datasets that are part of the publication will be copied to a secure tape archive (IU SDA/HPSS system)\nfor added data security.", 
            "title": "Dataset Archive"
        }, 
        {
            "location": "/apps/introduction/", 
            "text": "What is App?\n\n\nBrainlife \nApps\n are any code published on github that conforms to \nABCD specification\n and registered on Brain-Life.org. \nApp\n receives sets of input datasets where the format and structure of datatype conforms to registered datatypes on Brain-Life.org. \n\n\nAt minimum, it should meet following requirements..\n\n\n\n\nHosted on a public github repo.\n\n\nProvides an executable with a name \nmain\n at the root of the repo which runs the rest of the app.\n\n\nApp should receive all input parameters and paths to input files through \nconfig.json\n\n\nApp should write all output files under the current directory set by Brainlife. You don't have to know where exactly the current directory be, as it will be different everytime your application is run.\n\n\n\n\nBrain-Life's workflow service looks for a executable file called \nmain\n which can be a PBS submit script, or any script written in bash, python, etc. The \nApp\n then loads all input parameters (including paths to various input files) from \nconfig.json\n created by Brain-Life on the current directory. \nApp\n then runs the algorithm and create any output files inside the current directory. \nApps\n should exit with return code of 0 if successfully completed. Any non-0 exit code will be regarded as task being failed.\n\n\nApp\n should follow the principle of \nDo One Thing and Do It Well\n where a complex workflow should be split into several apps (but no more than necessary nor practical to do so). \n\n\nApp\n interfaces through Brain-Life \ndatatypes\n which are pre-defined datatypes commonly used by various apps such as..\n\n\n\n\nanat/t1 (t1.nii.gz)\n\n\nanat/t2 \n\n\ndwi (diffusion data and bvecs/bvals)\n\n\nfreesurfer (entire freesurfer output)\n\n\ntract (.tck file containing track data)\n\n\n\n\netc..\n\n\nYour app should read from one or more of these common datatypes, and write to another set of common datatypes. This may mean you have to do an extra importing / exporting of your data, but it allows other apps to interoperate with your app and increases the chance of code-reuse. \n\n\nBefore writing your apps, please browse \nalready registered apps\n and datatypes under Brain-Life.org to make sure that you are not reinventing parts of your application. If you find an app that is similar but does not exactly do what you need, please conctact the developer of the app and discuss if the feature you need can be added to the already existing app. \n\n\nTODO\n\n\nconfig.json\n\n\nYour app should read configuration from config.json in a local (working) directory. Here is an example of a sample config.json (https://github.com/brain-life/app-dipy-tracking/blob/master/config.json.template)\n\n\nIn Matlab, you can do something like following to read json\n\n\naddpath(genpath(\n/N/u/hayashis/BigRed2/git/jsonlab\n))\nconfig = loadjson(\nconfig.json\n);\ndisp(config.some.param);\n\n\n\n\n\nIn Python, you can do..\n\n\nimport\n \njson\n\n\nwith\n \nopen\n(\nconfig.json\n)\n \nas\n \nconfig_json\n:\n\n    \nconfig\n \n=\n \njson\n.\nload\n(\nconfig_json\n)\n\n\n\n# Load the data\n\n\nprint\n(\nconfig\n[\ndata_file\n])\n\n\n\n\n\n\njsonlab comes from \nhttps://github.com/fangq/jsonlab.git\n\n\nInput parameter should include paths to various input files, or command line parameters passed to various sub-processes. By using config.json, it makes it easier to run it via Docker. (It also prepares your application to be more \nABCD Specification\n compatible.)\n\n\nAll input parameters are assumed to be text (char). You need to write your functions that are going to be MATLAB compiled with all the arguments as text. Arguments passing a number need to be given as text and within the function converted to integers values (str2num(), etc.).", 
            "title": "Introduction"
        }, 
        {
            "location": "/apps/introduction/#what-is-app", 
            "text": "Brainlife  Apps  are any code published on github that conforms to  ABCD specification  and registered on Brain-Life.org.  App  receives sets of input datasets where the format and structure of datatype conforms to registered datatypes on Brain-Life.org.   At minimum, it should meet following requirements..   Hosted on a public github repo.  Provides an executable with a name  main  at the root of the repo which runs the rest of the app.  App should receive all input parameters and paths to input files through  config.json  App should write all output files under the current directory set by Brainlife. You don't have to know where exactly the current directory be, as it will be different everytime your application is run.   Brain-Life's workflow service looks for a executable file called  main  which can be a PBS submit script, or any script written in bash, python, etc. The  App  then loads all input parameters (including paths to various input files) from  config.json  created by Brain-Life on the current directory.  App  then runs the algorithm and create any output files inside the current directory.  Apps  should exit with return code of 0 if successfully completed. Any non-0 exit code will be regarded as task being failed.  App  should follow the principle of  Do One Thing and Do It Well  where a complex workflow should be split into several apps (but no more than necessary nor practical to do so).   App  interfaces through Brain-Life  datatypes  which are pre-defined datatypes commonly used by various apps such as..   anat/t1 (t1.nii.gz)  anat/t2   dwi (diffusion data and bvecs/bvals)  freesurfer (entire freesurfer output)  tract (.tck file containing track data)   etc..  Your app should read from one or more of these common datatypes, and write to another set of common datatypes. This may mean you have to do an extra importing / exporting of your data, but it allows other apps to interoperate with your app and increases the chance of code-reuse.   Before writing your apps, please browse  already registered apps  and datatypes under Brain-Life.org to make sure that you are not reinventing parts of your application. If you find an app that is similar but does not exactly do what you need, please conctact the developer of the app and discuss if the feature you need can be added to the already existing app.", 
            "title": "What is App?"
        }, 
        {
            "location": "/apps/introduction/#todo", 
            "text": "", 
            "title": "TODO"
        }, 
        {
            "location": "/apps/introduction/#configjson", 
            "text": "Your app should read configuration from config.json in a local (working) directory. Here is an example of a sample config.json (https://github.com/brain-life/app-dipy-tracking/blob/master/config.json.template)  In Matlab, you can do something like following to read json  addpath(genpath( /N/u/hayashis/BigRed2/git/jsonlab ))\nconfig = loadjson( config.json );\ndisp(config.some.param);  In Python, you can do..  import   json  with   open ( config.json )   as   config_json : \n     config   =   json . load ( config_json )  # Load the data  print ( config [ data_file ])   jsonlab comes from  https://github.com/fangq/jsonlab.git  Input parameter should include paths to various input files, or command line parameters passed to various sub-processes. By using config.json, it makes it easier to run it via Docker. (It also prepares your application to be more  ABCD Specification  compatible.)  All input parameters are assumed to be text (char). You need to write your functions that are going to be MATLAB compiled with all the arguments as text. Arguments passing a number need to be given as text and within the function converted to integers values (str2num(), etc.).", 
            "title": "config.json"
        }, 
        {
            "location": "/apps/helloworld/", 
            "text": "Let's begin by creating a brand \nnew github repository\n. Please be sure to make the repository public so that Brain-Life can download your app.\n\n\nGit clone your new repository to where you will be developing and running your app to test.\n\n\ngit clone git@github.com:soichih/app-test.git\n\n\n\n\n\nNow, create a file named \nmain\n which is used to run your app by Brain-Life. You can also run this file to test your application locally.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n#!/bin/bash\n\n\n\n#PBS -l nodes=1:ppn=1\n\n\n#PBS -l walltime=00:05:00\n\n\n\n#parse config.json for input parameters\n\n\nt1\n=\n$(\njq -r .t1 config.json\n)\n\n./main.py \n$t1\n\n\n\n\n\n\n\nPlease be sure to set the executable bit.\n\n\nchmod +x main\n\n\n\n\n\n\n\njq\n is a common command line tool used to parse a small JSON file and pull values out of it. You can install it by \napt-get install jq\n.\n\n\n\n\nFollowing part in \nmain\n instructs PBS (or slurm job manager) to request certain number of nodes / process-per-node. It's not necessary if you are not planning to run your app via PBS scheduler.\n\n\n#PBS -l nodes=1:ppn=1\n#PBS -l walltime=00:05:00\n\n\n\n\n\nYou can receive input parameters from Brain-Life through a JSON file called \nconfig.json\n which is created by Brain-Life when the app is executed through Brain-Life. \n\n\nFollowing lines parses the \nconfig.json\n using \njq\n and pass it to the main part of the application \nmain.py\n.\n\n\n#parse config.json for input parameters\nt1=$(jq -r .t1 config.json)\n./main.py $t1\n\n\n\n\n\nTo be able to test your application, let's create a test \nconfig.json\n.\n\n\n{\n   \nt1\n: \n/somewhere/t1.nii.gz\n\n}\n\n\n\n\n\nYou should add this file to .gitignore since it is only used locally on your machine to test your app.\n\n\n\n\nInstead of using \njq\n command to parse \nconfig.json\n in \nmain\n, you can use python's json parsing library inside \nmain.py\n if you prefer.\n\n\n\n\nFinally, the \nmain\n runs a python script called \nmain.py\n. Let's create it.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n#!/usr/bin/env python\n\n\n\nimport\n \nsys\n\n\nimport\n \ndipy.tracking\n \nas\n \ndipytracking\n\n\ndipytracking\n.\nbench\n()\n\n\n\nt1\n=\nsys\n.\nargv\n[\n1\n]\n\n\nprint\n(\nt1\n)\n\n\n\nf\n \n=\n \nopen\n(\noutput.txt\n,\n \nw\n)\n\n\nf\n.\nwrite\n(\nhello world\n)\n\n\nf\n.\nclose\n()\n\n\n\n\n\n\n\nAny output files from your app should be written to the current directory, so that Brain-Life can pick them up. Please be sure to add any output files from your app to .gitignore so that it won't be part of your git repo.\n\n\n.gitignore\n\n\noutput.txt\n\n\n\n\n\nNow, you can test run your app simply by executing \nmain\n\n\n./main\n...\n\n\n\n\n\n\n\nIf you are testing on HPC clusters, be sure to enter the interactive shell session before running your \nmain\n by executing something like \nqsub -I\n\n\n\n\nIf your app outputs correct output file on the local directgory, your application is ready to be pushed to the github. \n\n\ngit add .\ngit commit -m\ninitial import\n\ngit push\n\n\n\n\n\nNow you are ready to register your application on the Brian-Life so that your user can start submitting your app!\n\n\nTo summarize, Brain-Life app has following characteristics.\n\n\n\n\nPublished in Github.\n\n\nIt has \nmain\n which starts the app.\n\n\nIt reads input parameters from \nconfig.json\n created by Brain-Life on the current directory.\n\n\nIt writes output files on the current directory.\n\n\n\n\n\n\nFor more examples on Brain-Life apps, please see \nBrain-Life hosted apps\n.", 
            "title": "Hello World"
        }, 
        {
            "location": "/apps/register/", 
            "text": "Once you have developed your app, you can register it on Brain-Life so that other users can discover and execute your app. \n\n\nFirst, head to \nBrain-Life's Apps page\n. Click on the \nPlus Button\n on the bottom right corner of the page. \n![plus]({{ \"/assets/s/plus.png\" | absolute_url }}) \n\n\nBasic Info\n\n\nOn \"New App\" form, fill out name / description, and classification tags. Classification tag is used to organize apps into various groups to help user discover your app more easily.\n\n\nYou can leave \navatar\n URL empty and Brain-Life will auto-generate an avatar for you.\n\n\nProject\n field is optional, but if your app is written specifically for a particular project, you can select it here. If the select private project, your app will only be visible to the member of the project.\n\n\nSource Code\n is where you enter the Github repository name. Please enter only the organization / repository name (like \nbrain-life/app-life\n) not the full github repo URL.\n\n\nYou should leave the \nMax Retry\n field empty.\n\n\nBranch\n\n\nIf you don't specify the github repo's branch name, it uses \nmaster\n branch by default. As with any other project, you will most likely making changes to your \nmaster\n branch after you register your app, which means user won't be able to reproduce the output with exactly the same version of the code. Once you finish developing your app, you should consider creating a release branch (like \nrelease_1.0\n) and freeze the code which will be executed by the Brain-Life by specifying the branch name.\n\n\n\n\nOnce you create a new branch, any bug fixes you are making on \nmaster\n branch should be back-ported to other branches. You should, however, not make any changes to branches unless absolutely necessary as it would break the data reproducibility. \n\n\nIf you make a backward incompatible changes and/or any changes that requires different set of configuration parameters / inputs, you should register entirely new app on Brain-Life with that new version.\n\n\n\n\nConfiguration Parameters\n\n\nConfiguration parameters allows users to enter any number, boolean(true/false) or string parameters. You can also define enum parameter which lets users select one of from multiple string labels. \n\n\n\n\nPlaceholder\n For each input parameter, you can set a placeholder; a string displayed inside the form element if no value is specified. You can use placeholder to let user know the behavior of the app if you don't specify any value for that parameter. \n\n\nDescription\n Some parameter types let you specify a description which will be displayed next to the input parameter to show some detailed explanation about the input parameter.\n\n\n\n\nFor example, with following configuration parameter definition ..\n![input parameters definition]({{ \"/assets/s/input_param_def.png\" | absolute_url }})\n\n\nWhen users submit your app, they will be presented with following UI.\n![input parameter]({{ \"/assets/s/input_param.png\" | absolute_url }})\n\n\nYour app will receive something like following in \nconfig.json\n generated on the local directory of your app. You have to parse this in your app.\n\n\n{\n\n    \nparam1\n:\n \n100\n\n\n}\n\n\n\n\n\n\nInput Datasets\n\n\nMost app requires one or more input datasets to operate on. Similarly to configuration parameters, you can define input datasets with specific datatypes where users will be required to select when they submit your app.\n\n\nSometimes, you want to be more specific about the type of data within a particular datatype. For example, anat/t1w could be ACPC aligned, or not, dwi could be single-shell-ed, or not, etc.. Brain-Life adds context to each datatype through \ndatatype tags\n. \"ACPC Aligned\" anat/t1w can have a datatype tag of \"acpc_aligned\". If you know a specific datatype tags to limit the type of datasets that user should be selecting, you can specify those tags under the \nDatatype Tags\n section.\n\n\nOnce you select the datatype / tags, you also need to map the \nkey\n used in \nconfig.json\n to particular file / directory inside the datatype. You can do this by adding \nFile Mapping\n.\n\n\n![datasets]({{ \"/assets/s/input_datasets.png\" | absolute_url }})\n\n\nAbove configuration allows user to select anat/t1w datatype and your app will receive the path to the t1.nii.gz via a \nconfig.json\n key of \nt1\n\n\nOuput Datasets\n\n\nMost app produces output datasets which are then fed to another app as their input datasets. You can specify the datatypes of your output datasets here. It's up to your app to produce output in the correct file structure / file names to be compatible with specified datatype. Please consult with Brain-Life staff if you are not sure. \n\n\nraw\n datatype\n\n\nMany app creates datatypes that are not meant to be used by any other app (\"terminal apps\"). If you want your output to be archived to the Brain-Life warehouse, you can define the output datatype as \nraw\n with a good datatype tags to distinguish with other raw datatypes. \n\n\nFinally, click submit. Visit the apps page to make sure everything looks OK.\n\n\nEnabling app on resource\n\n\nOnce you registered your app on Brain-Life, you then need to have resources where you can run your app through Brain-Life. Resource could be any VM, HPC clusters, or public / private cloud resources. Only the resource owners can enable your app to run on their resource. Please contact the resource administrator for the resource where you'd like to submit your app . \n\n\nFor all Brain-Life's shared resources, please contact \nSoichi Hayashi\n.\n\n\nOnce your app is enabled on various resources, you should see a table of resources where the app can run under the app detail page.\n\n\n![resources]({{ \"/assets/s/resources.png\" | absolute_url }})", 
            "title": "Register App"
        }, 
        {
            "location": "/apps/register/#basic-info", 
            "text": "On \"New App\" form, fill out name / description, and classification tags. Classification tag is used to organize apps into various groups to help user discover your app more easily.  You can leave  avatar  URL empty and Brain-Life will auto-generate an avatar for you.  Project  field is optional, but if your app is written specifically for a particular project, you can select it here. If the select private project, your app will only be visible to the member of the project.  Source Code  is where you enter the Github repository name. Please enter only the organization / repository name (like  brain-life/app-life ) not the full github repo URL.  You should leave the  Max Retry  field empty.", 
            "title": "Basic Info"
        }, 
        {
            "location": "/apps/register/#branch", 
            "text": "If you don't specify the github repo's branch name, it uses  master  branch by default. As with any other project, you will most likely making changes to your  master  branch after you register your app, which means user won't be able to reproduce the output with exactly the same version of the code. Once you finish developing your app, you should consider creating a release branch (like  release_1.0 ) and freeze the code which will be executed by the Brain-Life by specifying the branch name.   Once you create a new branch, any bug fixes you are making on  master  branch should be back-ported to other branches. You should, however, not make any changes to branches unless absolutely necessary as it would break the data reproducibility.   If you make a backward incompatible changes and/or any changes that requires different set of configuration parameters / inputs, you should register entirely new app on Brain-Life with that new version.", 
            "title": "Branch"
        }, 
        {
            "location": "/apps/register/#configuration-parameters", 
            "text": "Configuration parameters allows users to enter any number, boolean(true/false) or string parameters. You can also define enum parameter which lets users select one of from multiple string labels.    Placeholder  For each input parameter, you can set a placeholder; a string displayed inside the form element if no value is specified. You can use placeholder to let user know the behavior of the app if you don't specify any value for that parameter.   Description  Some parameter types let you specify a description which will be displayed next to the input parameter to show some detailed explanation about the input parameter.   For example, with following configuration parameter definition ..\n![input parameters definition]({{ \"/assets/s/input_param_def.png\" | absolute_url }})  When users submit your app, they will be presented with following UI.\n![input parameter]({{ \"/assets/s/input_param.png\" | absolute_url }})  Your app will receive something like following in  config.json  generated on the local directory of your app. You have to parse this in your app.  { \n     param1 :   100  }", 
            "title": "Configuration Parameters"
        }, 
        {
            "location": "/apps/register/#input-datasets", 
            "text": "Most app requires one or more input datasets to operate on. Similarly to configuration parameters, you can define input datasets with specific datatypes where users will be required to select when they submit your app.  Sometimes, you want to be more specific about the type of data within a particular datatype. For example, anat/t1w could be ACPC aligned, or not, dwi could be single-shell-ed, or not, etc.. Brain-Life adds context to each datatype through  datatype tags . \"ACPC Aligned\" anat/t1w can have a datatype tag of \"acpc_aligned\". If you know a specific datatype tags to limit the type of datasets that user should be selecting, you can specify those tags under the  Datatype Tags  section.  Once you select the datatype / tags, you also need to map the  key  used in  config.json  to particular file / directory inside the datatype. You can do this by adding  File Mapping .  ![datasets]({{ \"/assets/s/input_datasets.png\" | absolute_url }})  Above configuration allows user to select anat/t1w datatype and your app will receive the path to the t1.nii.gz via a  config.json  key of  t1", 
            "title": "Input Datasets"
        }, 
        {
            "location": "/apps/register/#ouput-datasets", 
            "text": "Most app produces output datasets which are then fed to another app as their input datasets. You can specify the datatypes of your output datasets here. It's up to your app to produce output in the correct file structure / file names to be compatible with specified datatype. Please consult with Brain-Life staff if you are not sure.", 
            "title": "Ouput Datasets"
        }, 
        {
            "location": "/apps/register/#raw-datatype", 
            "text": "Many app creates datatypes that are not meant to be used by any other app (\"terminal apps\"). If you want your output to be archived to the Brain-Life warehouse, you can define the output datatype as  raw  with a good datatype tags to distinguish with other raw datatypes.   Finally, click submit. Visit the apps page to make sure everything looks OK.", 
            "title": "raw datatype"
        }, 
        {
            "location": "/apps/register/#enabling-app-on-resource", 
            "text": "Once you registered your app on Brain-Life, you then need to have resources where you can run your app through Brain-Life. Resource could be any VM, HPC clusters, or public / private cloud resources. Only the resource owners can enable your app to run on their resource. Please contact the resource administrator for the resource where you'd like to submit your app .   For all Brain-Life's shared resources, please contact  Soichi Hayashi .  Once your app is enabled on various resources, you should see a table of resources where the app can run under the app detail page.  ![resources]({{ \"/assets/s/resources.png\" | absolute_url }})", 
            "title": "Enabling app on resource"
        }, 
        {
            "location": "/apps/container/", 
            "text": "Docker is a software containerization tool that allow you to package your app and its dependencies into a portable \"container\". Once built, you can run it on any machine as long as it supports singularity regardless of the type of host OS, or host software installed.\n\n\nAlthough you can create a fully functional standalone docker container for your app, normally we only need to containerize the application \ndependendencies\n. For Brainlife, we recommend not to include the main part of your app (your python or bash scripts that drives your application) on your container. Once you create container for you application \nenvironment\n, with singularity, your can use it to execute the most ideosyncratic parts of your app through it. \n\n\n\n\nWarning\n\n\nIf you do share your \nenvironment\n container, be sure to specify which version(tag) of the container you are using in your app. Like..\n\nsingularity exec -e docker://brainlife/mrtrix_on_mcr:1.0 ./ensembletracking.sh\n\n\n\n\nTo build a docker container, you need to \ninstall Docker engine on your laptop\n or find a server that has docker engine installed that you can use. (Contact \nSoichi\n if you need a help.)\n\n\nWe assume you already have your Brainlife app hosted on Github, and you are making changes inside a cloned git repo on a machine with Docker engine.\n\n\nCompile Matlab\n\n\nTo run your application through a container, all Matlab scripts need to be compiled to a binary format using the \nmcc\n MatLab command\n. You can create a script that runs something like following.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n#!/bin/bash\n\nmodule load matlab/2017a\n\nmkdir -p compiled\n\ncat \n build.m \nEND\n\n\naddpath(genpath(\n/N/u/brlife/git/vistasoft\n))\n\n\naddpath(genpath(\n/N/u/brlife/git/jsonlab\n))\n\n\naddpath(genpath(\n/N/soft/mason/SPM/spm8\n))\n\n\nmcc -m -R -nodisplay -d compiled myapp\n\n\n%# function sptensor\n\n\nexit\n\n\nEND\n\nmatlab -nodisplay -nosplash -r build\n\n\n\n\n\n\nThis script generates a Matlab script called \nbuild.m\n and immediately executes it. \nbuild.m\n will setup the path and run Matlab command called \nmcc\n which does the actual compilation of your Matlab code and generates a binary that can be run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR. You can freely download MCR library from Matlab website, or use MCR container such as brainlife/mcr to execute your binary with.\n\n\nYou will need to adjust addpath() to include all of your dependencies that your application requires. \"myapp\" is the name of the matlab script that you use to execute your application. mcc commad will create a binary with the same name \"myapp\" inside the ./compiled directory.\n\n\n\n\n-m ...\n tells the name of the main entry function (in this case it's \nmain\n) of your application (it reads your config.json and runs the whole application)\n\n\n-R -nodisplay\n sets the command line options you'd normally pass when you run MatLab on the command line.\n\n\n-d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized.\n\n\n\n\nIf your app uses any MEX files (.c code) you will need to add an extra option (-a) to specify where those MEX files are stored. For example..\n\n\nmcc -m -R -nodisplay -a /N/u/hayashis/BigRed2/git/encode/mexfiles -d compiled myapp\n\n\n\n\n\n\n\nYou can use OpenMP to parallelize your application by using a container that has libgomp1 installed.\n\n\nmcc compiled application can't run certain Matlab statements; like addpath(). You might need to create a stripped down version of the main function that does not include those statements (or wrap them inside \nif not isdeployed\n statement that gets executed only when you run it directly on Matlab) . \n\n\n\n\nIf you don't have MatLab installed on your local machine, then you can do the compliation on the machine that has Matlab installed, the build the docker container on your own machine.\n\n\n\n\nNOTE. If you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround might be temporarly edit your startup.m to not include any addpath (or rename startup.m to startup.m.disabled) then run your compile script. \n\n\n\n\nLoading an ENCODE fe structure from an mcc application\n\n\nIf you are writing a function that will load a fe structure containing a sparse tensor array created by ENCODE, you need to add the following decorator above the load function:\n\n\n%# function sptensor\n\n\nThis lets the compiler know it should include the sptensor class, which \nload()\n does not use by default. If you do not include this, a warning will occur when you run the function stating the sptensor class was not found and it will load as an empty field. \n\n\nCreate a bash script to run your app\n\n\nUnless your application sorely consists of matlab, you may want to create another bash script which will do any pre(/post)processing of the data. For example:\n\n\n[docker.sh]\n\n\n#!/bin/bash\n\n./preprocess.sh \n#run some things (need to parse config.json using command line tool like jq)\n\n./mca/main \n#then execute matlab binary\n\n\n\n\n\n\nCreate Dockerfile (MatLab)\n\n\nBefore you start working on Dockerfile, make sure you are already familiar with the basic concepts of Docker and \nhow to build docker container\n. \n\n\nAll Docker containers have a base-container. This base is used to derive all other containers. If your application uses Matlab, then I recommend using\nthe Brain-Life Docker Container\n. Alternatively, you could also use a more general OS container such as a Ubuntu, CentOS. The \nneurodebian container\n is a good alternative.\n\n\nOn this example, I am going to use \nthe LiFE Docker Container\n as a template (this was compiled using MCR:2016a on Ubuntu Linux compatible with NeuroDebian), but we are going to \napp-dtiinit\n as an example to build a Docker container. To start:\n\n\n\n\ncd into your local app-dtiinit folder (this folder should have the main file). \n\n\ncopy the Dockerfile from \nonline\n into the current directory.\n\n\nOpen the \nDockerfile\n and add the following lines, which will add a series of Docker commands: \n\n\nSet the Docker base image and maintainer.\n   \nFROM brainlife/mcr:R2016a\n   MAINTAINER Your Name \nyouremail@iu.edu\n\n\nAdd dependencies. \n   This app requires the external, package FSL  to add this dependency we will the following lines to the Dockerfile.\n\n\n\n\nRUN sudo apt-get install fsl-complete\n\n   3. Add the app-dtiinit to the Docker. \n   We want to put the entire content of you git repository (in our example because we have some MatLab code, the repository has been previously made into a standalone executable and compiled and the compiled version is under app-dtiinit/msa) on to this container somewhere. I am going to put it under /app.\n   \nADD . /app\n\n\n\n\n\n\nLastly, need to specify the output directory to use\n   \nRUN mkdir /output\n   WORKDIR /output\n\n\n\n\n\n\nThen set the entry point of your application\n   \nENTRYPOINT [\"/app/docker.sh\"]\n\n\n\n\n\n\n3b. Prepare for your container to be run under singularity. Singularity allows users to run your container where they don't have root access (or docker engine access). One issue with running Docker container under singularity is that, often dynamically linked libraries creates symlinks under /usr/lib64 directory when they are first executed. (TODO - explain why this fails under singularity). To setup all necessary symlinks before singularity executes the container, run ldconfig at the end of your Dockerfile\n\n\nRUN ldconfig\n\n\nFor more information about singularity, please see http://singularity.lbl.gov/docs-docker#best-practices\n\n\n\n\nNow, create a sample config.json on your current directory to be used..\n\n\n\n\n[config.json]\n\n\n{\n   \nsome\n: \nparam\n,\n   \nanother\n: 123,\n   \ninput\n: \n/input/somefile.nii.gz\n\n}\n\n\n\n\n\n\n\nBuild the container (make sure to name it something like \n-t brainlife/someapp\n), \n\n\n\n\ndocker build -t brainlife/check.\n\n\n\n\n\n\n\nThen run it to test it\n\n\n\n\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp\n\n\n\n\n\n7 (optional). You can also save the above two commands into separate .sh files for ease of access. \n\n\nvim run_docker.sh\n(copy and paste into run_docker.sh\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp)\n./run_docker.sh\n\n\n\n\n\nDo something similar for building the container into its own build.sh file.\n\n\nCreate DockerFile (Python)\n\n\nThere are a few differences between the Matlab and Python versions of DockerFile. Namely the version and dependencies. While the Matlab one will show how to create one from an existing DockerFile, this part will show how to create a DockerFile from scratch. Please remember to install Docker before continuing.\n\n\n\n\nIn the folder of your Brain-Life application. Create a file called DockerFile.\n\n\n\n\nvim Dockerfile\n\n\n\n\n\n\n\nIn the DockerFile, set the OS and version of Python you would like to use.\n\n\n\n\nFROM ubuntu:16.04 (or whatever OS fits your fancy)\nFROM python:2\n\n\n\n\n\n\n\nNow install the dependencies your application requires.\n\n\n\n\nTo install pip and git, which you would most likely want to do, add the following to your DockerFile.\n\n\nRUN apt-get update \n apt-get install -y python-pip git\n\n\n\n\n\nTo pip install your dependencies add the following command:\n\n\nRUN pip install numpy Cython your-other-dependencies\n\n\n\n\n\nTo git clone a repository and install from there add the following\n\n\nRUN git clone the git url of the repository /rep-name\nRUN cd /rep-name \n python setup.py build_ext --inplace\n\n\n\n\n\n\n\nNow we need to make a folder for adding our main python files. The DockerFile will run your applications from here.\n\n\n\n\nRUN mkdir /app\nCOPY main.py /app\nCOPY another necessary .py file /app\n\n\n\n\n\nCopy all necessary python files into the app folder.\n\n\n\n\nWe will now create a folder for the output of your application.\n\n\n\n\nRUN mkdir /output\nWORKDIR /output\n\n\n\n\n\nIf you want to test your application locally, make sure you have your \nlocal\n config.json file in the output folder.\n\n\n\n\nIf you did any git clones and install from the setup.py in there make sure you also set the PYTHONPATH like so\n\n\n\n\nIf you installed everything from pip, you may skip this step.\n\n\nENV PYTHONPATH /my-git-repo:$PYTHONPATH\n\n\n\n\n\n\n\nFinally, we add the last line to finish up our DockerFile.\n\n\n\n\nCMD /app/main.py\n\n\n\n\n\nCongratulations! You wrote the DockerFile! You can see a full example here: \"add example here Aman\"\n\n\nBuilding and running your container\n\n\n\n\nBuild the container (make sure to name it something like \n-t brainlife/someapp\n), \n\n\n\n\ndocker build -t brainlife/check .\n\n\n\n\n\n\n\nThen run it to test it\n\n\n\n\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp\n\n\n\n\n\n3 (optional). You can also save the above two commands into separate .sh files for ease of access. \n\n\nrun_docker.sh\n\n\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp\n\n\n\n\n\nDo something similar for building the container into its own build.sh file.\n\n\nvim run_docker.sh\n(copy and paste into build.sh)\ndocker build -t brainlife/check.\n./build.sh (to run)\n\n\n\n\n\nYou only need to run the build once, unless you change something in your DockerFile. Otherwise you can directly run with the command or with run_docker.sh.\n\n\nYou will also probably need to give your run_docker.sh and build.sh permissions.\n\n\nchmod +x build.sh run_docker.sh\n\n\n\n\n\nREADME\n\n\nOnce you know that your container works, you should push your container to docker hub and update the README of your git repo to include instructions on how to run your container.\n\n\nExamples\n\n\nMatlab App Exmple \n\n\n\n\ngithub brain-life/app-life\n\n\ndockerhub brainlife/life\n (Dockerhub Autobuild)\n\n\n\n\nDipy (python) App Example\n\n\n\n\ngithub brain-life/app-dipy-workflows", 
            "title": "Containerizing App"
        }, 
        {
            "location": "/apps/container/#compile-matlab", 
            "text": "To run your application through a container, all Matlab scripts need to be compiled to a binary format using the  mcc  MatLab command . You can create a script that runs something like following.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 #!/bin/bash \nmodule load matlab/2017a\n\nmkdir -p compiled\n\ncat   build.m  END  addpath(genpath( /N/u/brlife/git/vistasoft ))  addpath(genpath( /N/u/brlife/git/jsonlab ))  addpath(genpath( /N/soft/mason/SPM/spm8 ))  mcc -m -R -nodisplay -d compiled myapp  %# function sptensor  exit  END \nmatlab -nodisplay -nosplash -r build   This script generates a Matlab script called  build.m  and immediately executes it.  build.m  will setup the path and run Matlab command called  mcc  which does the actual compilation of your Matlab code and generates a binary that can be run without Matlab license. The generated binary still requires a few Matlab proprietary libraries called MCR. You can freely download MCR library from Matlab website, or use MCR container such as brainlife/mcr to execute your binary with.  You will need to adjust addpath() to include all of your dependencies that your application requires. \"myapp\" is the name of the matlab script that you use to execute your application. mcc commad will create a binary with the same name \"myapp\" inside the ./compiled directory.   -m ...  tells the name of the main entry function (in this case it's  main ) of your application (it reads your config.json and runs the whole application)  -R -nodisplay  sets the command line options you'd normally pass when you run MatLab on the command line.  -d ...` tells where to output the generated binary. You should avoid writing it out to the application root directory; just to keep things organized.   If your app uses any MEX files (.c code) you will need to add an extra option (-a) to specify where those MEX files are stored. For example..  mcc -m -R -nodisplay -a /N/u/hayashis/BigRed2/git/encode/mexfiles -d compiled myapp   You can use OpenMP to parallelize your application by using a container that has libgomp1 installed.  mcc compiled application can't run certain Matlab statements; like addpath(). You might need to create a stripped down version of the main function that does not include those statements (or wrap them inside  if not isdeployed  statement that gets executed only when you run it directly on Matlab) .    If you don't have MatLab installed on your local machine, then you can do the compliation on the machine that has Matlab installed, the build the docker container on your own machine.   NOTE. If you are loading any custom paths via startup.m, those paths may influence how your binary is compiled. At the moment, I don't know a good way to prevent it from loaded when you run build.m. The only workaround might be temporarly edit your startup.m to not include any addpath (or rename startup.m to startup.m.disabled) then run your compile script.", 
            "title": "Compile Matlab"
        }, 
        {
            "location": "/apps/container/#loading-an-encode-fe-structure-from-an-mcc-application", 
            "text": "If you are writing a function that will load a fe structure containing a sparse tensor array created by ENCODE, you need to add the following decorator above the load function:  %# function sptensor  This lets the compiler know it should include the sptensor class, which  load()  does not use by default. If you do not include this, a warning will occur when you run the function stating the sptensor class was not found and it will load as an empty field.", 
            "title": "Loading an ENCODE fe structure from an mcc application"
        }, 
        {
            "location": "/apps/container/#create-a-bash-script-to-run-your-app", 
            "text": "Unless your application sorely consists of matlab, you may want to create another bash script which will do any pre(/post)processing of the data. For example:  [docker.sh]  #!/bin/bash \n./preprocess.sh  #run some things (need to parse config.json using command line tool like jq) \n./mca/main  #then execute matlab binary", 
            "title": "Create a bash script to run your app"
        }, 
        {
            "location": "/apps/container/#create-dockerfile-matlab", 
            "text": "Before you start working on Dockerfile, make sure you are already familiar with the basic concepts of Docker and  how to build docker container .   All Docker containers have a base-container. This base is used to derive all other containers. If your application uses Matlab, then I recommend using the Brain-Life Docker Container . Alternatively, you could also use a more general OS container such as a Ubuntu, CentOS. The  neurodebian container  is a good alternative.  On this example, I am going to use  the LiFE Docker Container  as a template (this was compiled using MCR:2016a on Ubuntu Linux compatible with NeuroDebian), but we are going to  app-dtiinit  as an example to build a Docker container. To start:   cd into your local app-dtiinit folder (this folder should have the main file).   copy the Dockerfile from  online  into the current directory.  Open the  Dockerfile  and add the following lines, which will add a series of Docker commands:   Set the Docker base image and maintainer.\n    FROM brainlife/mcr:R2016a\n   MAINTAINER Your Name  youremail@iu.edu  Add dependencies. \n   This app requires the external, package FSL  to add this dependency we will the following lines to the Dockerfile.   RUN sudo apt-get install fsl-complete \n   3. Add the app-dtiinit to the Docker. \n   We want to put the entire content of you git repository (in our example because we have some MatLab code, the repository has been previously made into a standalone executable and compiled and the compiled version is under app-dtiinit/msa) on to this container somewhere. I am going to put it under /app.\n    ADD . /app    Lastly, need to specify the output directory to use\n    RUN mkdir /output\n   WORKDIR /output    Then set the entry point of your application\n    ENTRYPOINT [\"/app/docker.sh\"]    3b. Prepare for your container to be run under singularity. Singularity allows users to run your container where they don't have root access (or docker engine access). One issue with running Docker container under singularity is that, often dynamically linked libraries creates symlinks under /usr/lib64 directory when they are first executed. (TODO - explain why this fails under singularity). To setup all necessary symlinks before singularity executes the container, run ldconfig at the end of your Dockerfile  RUN ldconfig  For more information about singularity, please see http://singularity.lbl.gov/docs-docker#best-practices   Now, create a sample config.json on your current directory to be used..   [config.json]  {\n    some :  param ,\n    another : 123,\n    input :  /input/somefile.nii.gz \n}   Build the container (make sure to name it something like  -t brainlife/someapp ),    docker build -t brainlife/check.   Then run it to test it   docker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp  7 (optional). You can also save the above two commands into separate .sh files for ease of access.   vim run_docker.sh\n(copy and paste into run_docker.sh\ndocker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp)\n./run_docker.sh  Do something similar for building the container into its own build.sh file.", 
            "title": "Create Dockerfile (MatLab)"
        }, 
        {
            "location": "/apps/container/#create-dockerfile-python", 
            "text": "There are a few differences between the Matlab and Python versions of DockerFile. Namely the version and dependencies. While the Matlab one will show how to create one from an existing DockerFile, this part will show how to create a DockerFile from scratch. Please remember to install Docker before continuing.   In the folder of your Brain-Life application. Create a file called DockerFile.   vim Dockerfile   In the DockerFile, set the OS and version of Python you would like to use.   FROM ubuntu:16.04 (or whatever OS fits your fancy)\nFROM python:2   Now install the dependencies your application requires.   To install pip and git, which you would most likely want to do, add the following to your DockerFile.  RUN apt-get update   apt-get install -y python-pip git  To pip install your dependencies add the following command:  RUN pip install numpy Cython your-other-dependencies  To git clone a repository and install from there add the following  RUN git clone the git url of the repository /rep-name\nRUN cd /rep-name   python setup.py build_ext --inplace   Now we need to make a folder for adding our main python files. The DockerFile will run your applications from here.   RUN mkdir /app\nCOPY main.py /app\nCOPY another necessary .py file /app  Copy all necessary python files into the app folder.   We will now create a folder for the output of your application.   RUN mkdir /output\nWORKDIR /output  If you want to test your application locally, make sure you have your  local  config.json file in the output folder.   If you did any git clones and install from the setup.py in there make sure you also set the PYTHONPATH like so   If you installed everything from pip, you may skip this step.  ENV PYTHONPATH /my-git-repo:$PYTHONPATH   Finally, we add the last line to finish up our DockerFile.   CMD /app/main.py  Congratulations! You wrote the DockerFile! You can see a full example here: \"add example here Aman\"", 
            "title": "Create DockerFile (Python)"
        }, 
        {
            "location": "/apps/container/#building-and-running-your-container", 
            "text": "Build the container (make sure to name it something like  -t brainlife/someapp ),    docker build -t brainlife/check .   Then run it to test it   docker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp  3 (optional). You can also save the above two commands into separate .sh files for ease of access.   run_docker.sh  docker run --rm -it \\\n        -v /host/input:/input \\\n        -v `pwd`:/output \\\n        brainlife/yourapp  Do something similar for building the container into its own build.sh file.  vim run_docker.sh\n(copy and paste into build.sh)\ndocker build -t brainlife/check.\n./build.sh (to run)  You only need to run the build once, unless you change something in your DockerFile. Otherwise you can directly run with the command or with run_docker.sh.  You will also probably need to give your run_docker.sh and build.sh permissions.  chmod +x build.sh run_docker.sh", 
            "title": "Building and running your container"
        }, 
        {
            "location": "/apps/container/#readme", 
            "text": "Once you know that your container works, you should push your container to docker hub and update the README of your git repo to include instructions on how to run your container.", 
            "title": "README"
        }, 
        {
            "location": "/apps/container/#examples", 
            "text": "Matlab App Exmple    github brain-life/app-life  dockerhub brainlife/life  (Dockerhub Autobuild)   Dipy (python) App Example   github brain-life/app-dipy-workflows", 
            "title": "Examples"
        }, 
        {
            "location": "/apps/versioning/", 
            "text": "Although Brainlife does not require you to use any specific git branching schema and you should observe any standrad practice from your own group, here are some guidelines that we suggest.\n\n\n1. git pull often\n\n\nBefore you start editing your app on your local machine each day, be sure to pull from origin. \n\n\ngit pull\n\n\n\n\n\nYou should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master.\n\n\n\n\nBy the way, see \ngit rebase\n if you are not familiar with rebasing. It helps our commit log to be clean.\n\n\n\n\n2. Always work on master branch\n\n\nWhen you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin.\n\n\n3. Create branch for new versions\n\n\nWhen we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. The easiest way to create a new branch is to do this on github UI. Just click on where it says \"Branch: master\" and enter new version number to create a new branch. The branch name should be a version number .. like \"1.0\", \"1.1\", \"1.2\", etc.., it should not be a branch names like the local branches you create on your local repo.\n\n\nOnce you create a branch, you should update the BL app to point users to use that new branch.\n\n\nYou could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also.\n\n\n4. Bug fix on master, then on branch.\n\n\nIf you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I like using command like \ncherry-pick\n to apply specific changes on other branches. Again, we should not add any new features on branches - only bug fixes.\n\n\n5. Semvar\n\n\nIf you don't know what semantic versioning is, please read \nhttps://semver.org/\n.\n\n\nFor branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new BL app with different major version number so that user can continue to submit your app with previous versions.", 
            "title": "Versioning Tips"
        }, 
        {
            "location": "/apps/versioning/#1-git-pull-often", 
            "text": "Before you start editing your app on your local machine each day, be sure to pull from origin.   git pull  You should git pull as often as you can to reduce possible merge issues down the road. When you are done testing your changes, git push to origin master.   By the way, see  git rebase  if you are not familiar with rebasing. It helps our commit log to be clean.", 
            "title": "1. git pull often"
        }, 
        {
            "location": "/apps/versioning/#2-always-work-on-master-branch", 
            "text": "When you are working on your app, you should always make changes on master. You can create as many local branches as you'd like (for each features you are working on) but when you are done, merge it to your own local master, then push the local master to the origin master. Pull from origin master, and push to origin master. Use local branches for you to organize what you are working on. Don't push local branches to origin.", 
            "title": "2. Always work on master branch"
        }, 
        {
            "location": "/apps/versioning/#3-create-branch-for-new-versions", 
            "text": "When we are done with making all changes, tested it, and ready to release it, we should create a new branch from master. The easiest way to create a new branch is to do this on github UI. Just click on where it says \"Branch: master\" and enter new version number to create a new branch. The branch name should be a version number .. like \"1.0\", \"1.1\", \"1.2\", etc.., it should not be a branch names like the local branches you create on your local repo.  Once you create a branch, you should update the BL app to point users to use that new branch.  You could use tags instead of branches, but tags does not allow you make modifications like you can with branches. Tags are great to point to any particular commit point, but you can do that with just a plain commit ID also.", 
            "title": "3. Create branch for new versions"
        }, 
        {
            "location": "/apps/versioning/#4-bug-fix-on-master-then-on-branch", 
            "text": "If you find a bug after you create a branch, you first need to fix the bug on the master, test it, then apply the same fix on all branches that are affected. I like using command like  cherry-pick  to apply specific changes on other branches. Again, we should not add any new features on branches - only bug fixes.", 
            "title": "4. Bug fix on master, then on branch."
        }, 
        {
            "location": "/apps/versioning/#5-semvar", 
            "text": "If you don't know what semantic versioning is, please read  https://semver.org/ .  For branch names, we should use major and minor version (like \"2.3\"), but don't include the patch number, as patch numbers are incremeneted for each bug fixes and you don't need to create a new branch for each bug fixes. If you make non-backward compatible changes, you should consider registering a brand new BL app with different major version number so that user can continue to submit your app with previous versions.", 
            "title": "5. Semvar"
        }, 
        {
            "location": "/apps/customhooks/", 
            "text": "Life cycles hooks are the script used to start / stop / monitor your apps by Brain-Life. By default, it looks for executable installed on each resource in the PATH with named \nstart\n, \nstatus\n, and \nstop\n. Resource owner needs to make sure these scripts are installed and accessible by your apps. \n\n\n\n\nFor most PBS, SLURM, and vanila VM, resource owner can install \nABCD default hooks\n.\n\n\n\n\nBy default, \nstart\n hook should look for a file named \nmain\n to start your app. Therefore, the only file required to make your app runnable by Brain-Life is this \nmain\n executable on the root directory of the app's git repository. \n\n\nUnder most circumstances, app developers shouldn't have to worry about these hook scripts. However, if your app requires some special mechanism to start / stop and monitor your app, you might need to provide your own hook scripts. \n\n\nYou can specify the paths to these hook scripts by creating a file named \npackage.json\n\n\n{\n\n  \nbrainlife\n:\n \n{\n\n    \nstart\n:\n \n./start.sh\n,\n\n    \nstop\n:\n \n./stop.sh\n,\n\n    \nstatus\n:\n \n./status.sh\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThen, you will need to provide those hook scripts as part of your app.\n\n\n\n\nPlease be sure to \nchmod +x *.sh\n so that your hook scripts are executable.\n\n\n\n\nstart.sh\n\n\nFollowing is an example for \nstart\n script. It submits a file named \nmain\n (should be provided by each app) through qsub. It stores \njobid\n so that we can monitor the job status.\n\n\n1\n2\n3\n4\n5\n6\n#!/bin/bash\n\n\n\n#return code 0 = job started successfully.\n\n\n#return code non-0 = job failed to start\n\n\nqsub -d \n$PWD\n -V -o \n\\$\nPBS_JOBID.log -e \n\\$\nPBS_JOBID.err main \n jobid\n\n\n\n\n\n\nstop.sh\n\n\nFollowing is an example for \nstop\n script. This scripts reads the jobid created by \nstart\n script and call qdel to stop it.\n\n\n1\n2\n#!/bin/bash\n\nqdel \n`\ncat jobid\n`\n\n\n\n\n\n\n\nstatus.sh\n\n\nstatus hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the \njobid\n stored by start script to query the job status with \nqstat\n PBS command. \n\n\nAnything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n#!/bin/bash\n\n\n\n#return code 0 = running\n\n\n#return code 1 = finished successfully\n\n\n#return code 2 = failed\n\n\n#return code 3 = unknown (retry later)\n\n\n\nif\n \n[\n ! -f jobid \n]\n;\nthen\n\n    \necho\n \nno jobid - not yet submitted?\n\n    \nexit\n \n1\n\n\nfi\n\n\n\njobid\n=\n`\ncat jobid\n`\n\n\nif\n \n[\n -z \n$jobid\n \n]\n;\n \nthen\n\n    \necho\n \njobid is empty.. failed to submit?\n\n    \nexit\n \n3\n\n\nfi\n\n\n\njobstate\n=\n`\nqstat -f \n$jobid\n \n|\n grep job_state \n|\n cut -b17\n`\n\n\nif\n \n[\n -z \n$jobstate\n \n]\n;\n \nthen\n\n    \necho\n \nJob removed before completing - maybe timed out?\n\n    \nexit\n \n2\n\n\nfi\n\n\n\ncase\n \n$jobstate\n in\nQ\n)\n\n    showstart \n$jobid\n \n|\n grep start\n    \nexit\n \n0\n\n    \n;;\n\nR\n)\n\n    \n#get last line of last log touched\n\n    \nlogfile\n=\n$(\nls -rt *.log \n|\n tail -1\n)\n\n    tail -1 \n$logfile\n\n    \nexit\n \n0\n\n    \n;;\n\nH\n)\n\n    \necho\n \nJob held.. waiting\n\n    \nexit\n \n0\n\n    \n;;\n\nC\n)\n\n    \nexit_status\n=\n`\nqstat -f \n$jobid\n \n|\n grep exit_status \n|\n cut -d\n=\n -f2 \n|\n xargs\n`\n\n    \nif\n \n[\n \n$exit_status\n -eq \n0\n \n]\n;\n \nthen\n\n        \necho\n \nfinished with code 0\n\n        \nexit\n \n1\n\n    \nelse\n\n        \necho\n \nfinished with code \n$exit_status\n\n        \nexit\n \n2\n\n    \nfi\n\n    \n;;\n\n*\n)\n\n    \necho\n \nunknown job status \n$jobstate\n\n    \nexit\n \n2\n\n    \n;;\n\n\n\nesac", 
            "title": "Custom Hooks"
        }, 
        {
            "location": "/apps/customhooks/#startsh", 
            "text": "Following is an example for  start  script. It submits a file named  main  (should be provided by each app) through qsub. It stores  jobid  so that we can monitor the job status.  1\n2\n3\n4\n5\n6 #!/bin/bash  #return code 0 = job started successfully.  #return code non-0 = job failed to start \n\nqsub -d  $PWD  -V -o  \\$ PBS_JOBID.log -e  \\$ PBS_JOBID.err main   jobid", 
            "title": "start.sh"
        }, 
        {
            "location": "/apps/customhooks/#stopsh", 
            "text": "Following is an example for  stop  script. This scripts reads the jobid created by  start  script and call qdel to stop it.  1\n2 #!/bin/bash \nqdel  ` cat jobid `", 
            "title": "stop.sh"
        }, 
        {
            "location": "/apps/customhooks/#statussh", 
            "text": "status hook is a bit more complicated. It needs to return various exit codes based on the status of the app. It uses the  jobid  stored by start script to query the job status with  qstat  PBS command.   Anything you output to stdout will be used to set task's status message. For example, you can output the last line from the log file to relay the last log entry to the users on Brain-Life.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55 #!/bin/bash  #return code 0 = running  #return code 1 = finished successfully  #return code 2 = failed  #return code 3 = unknown (retry later)  if   [  ! -f jobid  ] ; then \n     echo   no jobid - not yet submitted? \n     exit   1  fi  jobid = ` cat jobid `  if   [  -z  $jobid   ] ;   then \n     echo   jobid is empty.. failed to submit? \n     exit   3  fi  jobstate = ` qstat -f  $jobid   |  grep job_state  |  cut -b17 `  if   [  -z  $jobstate   ] ;   then \n     echo   Job removed before completing - maybe timed out? \n     exit   2  fi  case   $jobstate  in\nQ ) \n    showstart  $jobid   |  grep start\n     exit   0 \n     ;; \nR ) \n     #get last line of last log touched \n     logfile = $( ls -rt *.log  |  tail -1 ) \n    tail -1  $logfile \n     exit   0 \n     ;; \nH ) \n     echo   Job held.. waiting \n     exit   0 \n     ;; \nC ) \n     exit_status = ` qstat -f  $jobid   |  grep exit_status  |  cut -d =  -f2  |  xargs ` \n     if   [   $exit_status  -eq  0   ] ;   then \n         echo   finished with code 0 \n         exit   1 \n     else \n         echo   finished with code  $exit_status \n         exit   2 \n     fi \n     ;; \n* ) \n     echo   unknown job status  $jobstate \n     exit   2 \n     ;;  esac", 
            "title": "status.sh"
        }, 
        {
            "location": "/resources/register/", 
            "text": "Background\n\n\nBrainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled. \n\n\nAs Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases.\n\n\n\n\nYou have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control.\n\n\nYou are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging.\n\n\nYou are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide.\n\n\n\n\nCurrently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact \nBrainlife Admin\n\n\n\n\nNote\n\n\nResource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact \nBrainlife Admin\n to enable your app on Brainlife default resources.\n\n\n\n\n\n\nWarning\n\n\nAlthough we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security\nespecially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information).\n\n\n\n\nRegistering Resources\n\n\nTo register your resource, go to \nBrainlife Settings\n page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields.\n\n\n\n\nName\n Enter the name of rhe resource\n\n\nHostname\n The hostname of your compute resource (usually a login/submit host)\n\n\nUsername\n Username used to ssh to this resource\n\n\nWorkdir\n Directory used to stage and store generated datasets by apps. \nYou should not share the same directory with other resources\n. Please make sure that the specified directory exits (mkdir if not).\n\n\nSSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read \nauthorized_keys\n for more detail.\n\n\n\n\nYou can leave the rest of the fields empty for now.\n\n\nClick OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good.\n\n\nConfiguring Resources\n\n\nOnce you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps.\n\n\nABCD Default Hooks\n\n\nABCD Hooks\n are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting \n$PATH\n. If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following.\n\n\ncd ~\ngit clone https://github.com/brain-life/abcd-spec\n\n\n\n\n\nThen, add one of following to your ~/.bashrc\n\n\nFor PBS cluster\n\n\nexport PATH=~/abcd-spec/hooks/pbs:$PATH\n\n\n\n\n\nFor Slurm cluster\n\n\nexport PATH=~/abcd-spec/hooks/slurm:$PATH\n\n\n\n\n\nFor direct execution - no batch submission manager\n\n\nexport PATH=~/abcd-spec/hooks/direct:$PATH\n\n\n\n\n\nCommon Binaries\n\n\nBrainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed.\n\n\n\n\njq (command line json parser commonly used by Brainlife apps to parse config.json)\n\n\ngit (used to clone / update apps installed)\n\n\nsingularity (user level container execution engine)\n\n\n\n\nFor IU HPC resource, please feel free to use following ~/bin directory which contains jq\n\n\n$ ~/.bashrc\n\nexport\n \nPATH\n=\n$PATH\n:/N/u/brlife/Carbonate/bin\n\n\n\n\n\nFor singularity, you can either install it on the system, or for most HPC systems you can simply add following in your \n~/.modules\n file.\n\n\nmodule\n \nload\n \nsingularity\n\n\n\n\n\n\nBy default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc\n\n\nexport SINGULARITY_CACHEDIR=/N/dc2/scratch/\nusername\n/singularity-cachedir\n\n\n\n\n\n\n\nPlease replace \n with your username, and make sure specified directories exists.\n\n\n\n\nOther ENV parameters\n\n\nDepending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via \nFREESURFER_LICENSE\n.\n\n\nexport FREESURFER_LICENSE=\nhayashis@iu.edu 29511 *CPmh9xvKQKHE FSg0ijTusqaQc\n\n\n\n\n\n\nEnabling Apps\n\n\nOnce you have registered and tested your resource, you can now enable apps to run on your resource.\n\n\nGo back to the Brainlife's \nresource settings page\n, and click the resource you have created. Under the services section, enter the git org/repo name (such as like \nbrain-life/app-life\n) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK.\n\n\nYou can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife. \nexample", 
            "title": "Registering Resource"
        }, 
        {
            "location": "/resources/register/#background", 
            "text": "Brainlife provides a mechanism to make a computing resource, such as a Cloud compute system or a high-performance computing cluster, available to specific user, projects or user defined groups. By default, shared compute resources are available to all users where most of Brainlife apps are enabled.   As Brainlife's default resources are available by all users, often users must wait on the queue for requested tasks to get executed. You can register your own compute resources for the following use cases.   You have access to your own HPC resource, and you'd like to use it to run Brainlife apps, for better performance, or better access control.  You are an app developer and you'd like to use your own resource to troubleshoot your apps on your own resource, for easier debugging.  You are an app developer and your app can only run on specialized resources (like Hadoop, Spark, etc..) that Brainlife's shared resources do not provide.   Currently, only Brainlife admin can share personal resources with other members. If you wish to share your resources, please contact  Brainlife Admin   Note  Resource owner decides which apps are allowed to run on their resource. If you register a resource and enable apps on it, only you can run those apps on that resource. If you are publishing your app, and you want all users to be able to execute your app, please contact  Brainlife Admin  to enable your app on Brainlife default resources.    Warning  Although we do our best to limit access to your dataset on shared resources, we recommend registering your own resource for added security\nespecially if you are planning to process sensitive data. We currently do not allow any datasets with PHI (protected health information).", 
            "title": "Background"
        }, 
        {
            "location": "/resources/register/#registering-resources", 
            "text": "To register your resource, go to  Brainlife Settings  page, and Under \"HPC Systems\" click \"Add New Account\". A resource entry form should appear. Please populate the following fields.   Name  Enter the name of rhe resource  Hostname  The hostname of your compute resource (usually a login/submit host)  Username  Username used to ssh to this resource  Workdir  Directory used to stage and store generated datasets by apps.  You should not share the same directory with other resources . Please make sure that the specified directory exits (mkdir if not).  SSH Public Key: Copy the content of this key to your resource's ~/.ssh/authorized_keys. Please read  authorized_keys  for more detail.   You can leave the rest of the fields empty for now.  Click OK. Once you are finished with copying ssh key and make sure the workdir exists, click \"Test\" button to see if Brainlife can access your resource. You should see a green checkbox if everything is good.", 
            "title": "Registering Resources"
        }, 
        {
            "location": "/resources/register/#configuring-resources", 
            "text": "Once you register your resource, you will need to perform a few things to prepare your resource so that Brainlife can successfully execute Brainlife apps.", 
            "title": "Configuring Resources"
        }, 
        {
            "location": "/resources/register/#abcd-default-hooks", 
            "text": "ABCD Hooks  are used to start, stop and monitor apps on remote resources. Some app provides its own hooks, but many of them relies on default hooks that are installed on each resource. As a resource provider, you need to provide these default hooks and make them available by setting  $PATH . If you are not sure how to write these scripts, you can install and use Brainlife's default ABCD hooks by doing following.  cd ~\ngit clone https://github.com/brain-life/abcd-spec  Then, add one of following to your ~/.bashrc", 
            "title": "ABCD Default Hooks"
        }, 
        {
            "location": "/resources/register/#for-pbs-cluster", 
            "text": "export PATH=~/abcd-spec/hooks/pbs:$PATH", 
            "title": "For PBS cluster"
        }, 
        {
            "location": "/resources/register/#for-slurm-cluster", 
            "text": "export PATH=~/abcd-spec/hooks/slurm:$PATH", 
            "title": "For Slurm cluster"
        }, 
        {
            "location": "/resources/register/#for-direct-execution-no-batch-submission-manager", 
            "text": "export PATH=~/abcd-spec/hooks/direct:$PATH", 
            "title": "For direct execution - no batch submission manager"
        }, 
        {
            "location": "/resources/register/#common-binaries", 
            "text": "Brainlife expects certain binaries to be installed on all resources. Please make sure following commands are installed.   jq (command line json parser commonly used by Brainlife apps to parse config.json)  git (used to clone / update apps installed)  singularity (user level container execution engine)   For IU HPC resource, please feel free to use following ~/bin directory which contains jq  $ ~/.bashrc export   PATH = $PATH :/N/u/brlife/Carbonate/bin  For singularity, you can either install it on the system, or for most HPC systems you can simply add following in your  ~/.modules  file.  module   load   singularity   By default, singularity uses user's home directory to cache docker images (and /tmp to create a merged container image to run). If you have limited amount of home directory space, you should override these directories by adding the following in your .bashrc  export SINGULARITY_CACHEDIR=/N/dc2/scratch/ username /singularity-cachedir   Please replace   with your username, and make sure specified directories exists.", 
            "title": "Common Binaries"
        }, 
        {
            "location": "/resources/register/#other-env-parameters", 
            "text": "Depending on the app you are trying to run, some app may require additional ENV parameters. For example, brain-life/app-freesurfer requires you to provide your freesurfer license via  FREESURFER_LICENSE .  export FREESURFER_LICENSE= hayashis@iu.edu 29511 *CPmh9xvKQKHE FSg0ijTusqaQc", 
            "title": "Other ENV parameters"
        }, 
        {
            "location": "/resources/register/#enabling-apps", 
            "text": "Once you have registered and tested your resource, you can now enable apps to run on your resource.  Go back to the Brainlife's  resource settings page , and click the resource you have created. Under the services section, enter the git org/repo name (such as like  brain-life/app-life ) for the app that you'd like to enable, and the score for each service. The higher the score is, the more likely the resource will be chosen to run your app (if there are multiple resources available). Brainlife gives higher score for resources that you own (not shared ones), you should leave it the default of 10 unless it's competing with other resource that you have access to. Click OK.  You can see which resource an app is configured to run, and which resource will be chosen when you submit it under App detail / Computing Resources section on Brainlife.  example", 
            "title": "Enabling Apps"
        }, 
        {
            "location": "/todo/", 
            "text": "Coming soon..", 
            "title": "Todo"
        }, 
        {
            "location": "/todo/", 
            "text": "Coming soon..", 
            "title": "Todo"
        }, 
        {
            "location": "/technical/api/", 
            "text": "Brainlife platform consists of various microservices which enables various functionalities available to our end-users through our web UI. This document describes some of these services in case you might be interested in knowing how Brainlife works or directly interfacing with these services through its APIs.\n\n\nWarehouse\n\n\nWarehouse is the main application responsible for bulk of Brainlife platform UI.\n\n\nWarehouse API Doc\n\n\nWarehouse Github\n\n\nCLI\n\n\nYou can use Brainlife CLI to import / export datasets, among other things. At the moment, we have a very limited CLI support. CLI is simply a wrapper around Brainlife APIs for various services listed in this page. You can use other APIs to perform necessary action if necessary.\n\n\nCLI Github\n\n\nAmaretti\n\n\nAmaretti\n is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information.\n\n\nAmaretti Technical Doc\n\n\nAMaretti Github\n\n\nAmaretti API Doc\n\n\nOther APIs\n\n\nAuthentication Service\n\n\nAuth Github\n\n\nAuth API Doc\n\n\nEvent Service\n\n\nEvent Github\n\n\nEvent API Doc\n\n\nProfile Service\n\n\nProfile Github\n\n\nProfile API Doc", 
            "title": "APIs"
        }, 
        {
            "location": "/technical/api/#warehouse", 
            "text": "Warehouse is the main application responsible for bulk of Brainlife platform UI.  Warehouse API Doc  Warehouse Github", 
            "title": "Warehouse"
        }, 
        {
            "location": "/technical/api/#cli", 
            "text": "You can use Brainlife CLI to import / export datasets, among other things. At the moment, we have a very limited CLI support. CLI is simply a wrapper around Brainlife APIs for various services listed in this page. You can use other APIs to perform necessary action if necessary.  CLI Github", 
            "title": "CLI"
        }, 
        {
            "location": "/technical/api/#amaretti", 
            "text": "Amaretti  is responsible for submitting, monitoring, and interfacing with apps running on various resources that you have access to. Please see Amaretti technical doc for more information.  Amaretti Technical Doc  AMaretti Github  Amaretti API Doc", 
            "title": "Amaretti"
        }, 
        {
            "location": "/technical/api/#other-apis", 
            "text": "", 
            "title": "Other APIs"
        }, 
        {
            "location": "/technical/api/#authentication-service", 
            "text": "Auth Github  Auth API Doc", 
            "title": "Authentication Service"
        }, 
        {
            "location": "/technical/api/#event-service", 
            "text": "Event Github  Event API Doc", 
            "title": "Event Service"
        }, 
        {
            "location": "/technical/api/#profile-service", 
            "text": "Profile Github  Profile API Doc", 
            "title": "Profile Service"
        }
    ]
}